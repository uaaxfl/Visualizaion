[
    [
        {
            "id": "2006",
            "citation_count": 60,
            "name": 60,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2008",
            "citation_count": 147,
            "name": 147,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P06-2100",
            "name": "Morphological Richness Offsets Resource Demand {--} Experiences in Constructing a {POS} Tagger for {H}indi",
            "publication_data": 2006,
            "citation": 60,
            "abstract": "In this paper we report our work on building a POS tagger for a morphologically rich language- Hindi. The theme of the research is to vindicate the stand that- if morphology is strong and harnessable, then lack of training corpora is not debilitating. We establish a methodology of POS tagging which the resource disadvantaged (lacking annotated corpora) languages can make use of. The methodology makes use of locally annotated modestly-sized corpora (15,562 words), exhaustive morpohological analysis backed by high-coverage lexicon and a decision tree based learning algorithm (CN2). The evaluation of the system was done with 4-fold cross validation of the corpora in the news domain (www.bbc.co.uk/hindi). The current accuracy of POS tagging is 93.45% and can be further improved.",
            "cx": 3700.6,
            "cy": -1283.23,
            "rx": 110.118,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "I08-1067",
            "name": "Simple Syntactic and Morphological Processing Can Help {E}nglish-{H}indi Statistical Machine Translation",
            "publication_data": 2008,
            "citation": 61,
            "abstract": "In this paper, we report our work on incorporating syntactic and morphological information for English to Hindi statistical machine translation. Two simple and computationally inexpensive ideas have proven to be surprisingly effective: (i) reordering the English source sentence as per Hindi syntax, and (ii) using the suffixes of Hindi words. The former is done by applying simple transformation rules on the English parse tree. The latter, by using a simple suffix separation program. With only a small amount of bilingual training data and limited tools for Hindi, we achieve reasonable performance and substantial improvements over the baseline phrase-based system. Our approach eschews the use of parsing or other sophisticated linguistic tools for the target language (Hindi) making it a useful framework for statistical machine translation from English to Indian languages in general, since such tools are not widely available for Indian languages currently.",
            "cx": 2231.6,
            "cy": -1193.49,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C10-2040",
            "name": "Verbs are where all the action lies: Experiences of Shallow Parsing of a Morphologically Rich Language",
            "publication_data": 2010,
            "citation": 13,
            "abstract": "Verb suffixes and verb complexes of morphologically rich languages carry a lot of information. We show that this information if harnessed for the task of shallow parsing can lead to dramatic improvements in accuracy for a morphologically rich language- Marathi. The crux of the approach is to use a powerful morphological analyzer backed by a high coverage lexicon to generate rich features for a CRF based sequence classifier. Accuracy figures of 94% for Part of Speech Tagging and 97% for Chunking using a modestly sized corpus (20K words) vindicate our claim that for morphologically rich languages linguistic insight can obviate the need for large amount of annotated corpora.",
            "cx": 6984.6,
            "cy": -1014.01,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2009",
            "citation_count": 69,
            "name": 69,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "sankaran-etal-2008-common",
            "name": "A Common Parts-of-Speech Tagset Framework for {I}ndian Languages",
            "publication_data": 2008,
            "citation": 29,
            "abstract": "We present a universal Parts-of-Speech (POS) tagset framework covering most of the Indian languages (ILs) following the hierarchical and decomposable tagset schema. In spite of significant number of speakers, there is no workable POS tagset and tagger for most ILs, which serve as fundamental building blocks for NLP research. Existing IL POS tagsets are often designed for a specific language; the few that have been designed for multiple languages cover only shallow linguistic features ignoring linguistic richness and the idiosyncrasies. The new framework that is proposed here addresses these deficiencies in an efficient and principled manner. We follow a hierarchical schema similar to that of EAGLES and this enables the framework to be flexible enough to capture rich features of a language/ language family, even while capturing the shared linguistic structures in a methodical way. The proposed common framework further facilitates the sharing and reusability of scarce resources in these languages and ensures cross-linguistic compatibility.",
            "cx": 7388.6,
            "cy": -1193.49,
            "rx": 72.6644,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "I08-7013",
            "name": "Designing a Common {POS}-Tagset Framework for {I}ndian Languages",
            "publication_data": 2008,
            "citation": 8,
            "abstract": "Research in Parts-of-Speech (POS) tagset design for European and East Asian languages started with a mere listing of important morphosyntactic features in one language and has matured in later years towards hierarchical tagsets, decomposable tags, common framework for multiple languages (EAGLES) etc. Several tagsets have been developed in these languages along with large amount of annotated data for furthering research. Indian Languages (ILs) present a contrasting picture with very little research in tagset design issues. We present our work in designing a common POS-tagset framework for ILs, which is the result of in-depth analysis of eight languages from two major families, viz. Indo-Aryan and Dravidian. Our framework follows hierarchical tagset layout similar to the EAGLES guidelines, but with significant changes as needed for the ILs.",
            "cx": 7193.6,
            "cy": -1193.49,
            "rx": 103.889,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P09-1090",
            "name": "Case markers and Morphology: Addressing the crux of the fluency problem in {E}nglish-{H}indi {SMT}",
            "publication_data": 2009,
            "citation": 35,
            "abstract": "We report in this paper our work on accurately generating case markers and suffixes in English-to-Hindi SMT. Hindi is a relatively free word-order language, and makes use of a comparatively richer set of case markers and morphological suffixes for correct meaning representation. From our experience of large-scale English-Hindi MT, we are convinced that fluency and fidelity in the Hindi output get an order of magnitude facelift if accurate case markers and suffixes are produced. Now, the moot question is: what entity on the English side encodes the information contained in case markers and suffixes on the Hindi side? Our studies of correspondences in the two languages show that case markers and suffixes in Hindi are predominantly determined by the combination of suffixes and semantic relations on the English side. We, therefore, augment the aligned corpus of the two languages, with the correspondence of English suffixes and semantic relations with Hindi suffixes and case markers. Our results on 400 test sentences, translated using an SMT system trained on around 13000 parallel sentences, show that suffix  semantic relation xe2x86x92 case marker/suffix is a very useful translation factor, in the sense of making a significant difference to output quality as indicated by subjective evaluation as well as BLEU scores.",
            "cx": 2161.6,
            "cy": -1103.75,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W12-5906",
            "name": "Partially modelling word reordering as a sequence labelling problem",
            "publication_data": 2012,
            "citation": 3,
            "abstract": "Source side reordering has been shown to improve the performance of phrase based machine translation systems. In this work, we explore the learning of source side reordering given a training corpus of word aligned data. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space.",
            "cx": 919.597,
            "cy": -834.531,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5105",
            "name": "Supertag Based Pre-ordering in Machine Translation",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "None",
            "cx": 2088.6,
            "cy": -655.051,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5148",
            "name": "Merging Verb Senses of {H}indi {W}ord{N}et using Word Embeddings",
            "publication_data": 2014,
            "citation": 2,
            "abstract": "In this paper, we present an approach for merging fine-grained verb senses of Hindi WordNet. Senses are merged based on gloss similarity score. We explore the use of word embeddings for gloss similarity computation and compare with various WordNet based gloss similarity measures. Our results indicate that word embeddings show significant improvement over WordNet based measures. Consequently, we observe an increase in accuracy on merging fine-grained senses. Gold standard data constructed for our experiments is made available.",
            "cx": 3653.6,
            "cy": -655.051,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-3308",
            "name": "The {IIT} {B}ombay {H}indi-{E}nglish Translation System at {WMT} 2014",
            "publication_data": 2014,
            "citation": 9,
            "abstract": "In this paper, we describe our EnglishHindi and Hindi-English statistical systems submitted to the WMT14 shared task. The core components of our translation systems are phrase based (Hindi-English) and factored (English-Hindi) SMT systems. We show that the use of number, case and Tree Adjoining Grammar information as factors helps to improve English-Hindi translation, primarily by generating morphological inflections correctly. We show improvements to the translation systems using pre-procesing and post-processing components. To overcome the structural divergence between English and Hindi, we preorder the source side sentence to conform to the target language word order. Since parallel corpus is limited, many words are not translated. We translate out-of-vocabulary words and transliterate named entities in a post-processing stage. We also investigate ranking of translations from multiple systems to select the best translation.",
            "cx": 1062.6,
            "cy": -655.051,
            "rx": 72.25,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "kunchukuttan-etal-2014-shata",
            "name": "Shata-Anuvadak: Tackling Multiway Translation of {I}ndian Languages",
            "publication_data": 2014,
            "citation": 15,
            "abstract": "We present a compendium of 110 Statistical Machine Translation systems built from parallel corpora of 11 Indian languages belonging to both Indo-Aryan and Dravidian families. We analyze the relationship between translation accuracy and the language families involved. We feel that insights obtained from this analysis will provide guidelines for creating machine translation systems of specific Indian language pairs. We build phrase based systems and some extensions. Across multiple languages, we show improvements on the baseline phrase based systems using these extensions: (1) source side reordering for English-Indian language translation, and (2) transliteration of untranslated words for Indian language-Indian language translation. These enhancements harness shared characteristics of Indian languages. To stimulate similar innovation widely in the NLP community, we have made the trained models for these language pairs publicly available.",
            "cx": 606.597,
            "cy": -655.051,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W15-5950",
            "name": "Investigating the potential of post-ordering {SMT} output to improve translation quality",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Post-ordering of Statistical Machine Translation (SMT) output to correct word order errors could be a promising area of research to overcome structural divergence between language pairs. This is especially true when it is difficult to incorporate rich linguistic features into the baseline decoder. In this paper, we propose an algorithm for generating oracle reorderings of MT output. We use the oracle reorderings to empirically quantify an upper bound on improvement in translation quality through post-ordering techniques. In our study encompassing multiple language pairs, we show that significant improvement in translation quality can be obtained by applying reordering transformations on the output of the SMT system. This presents a strong case for investing effort in exploring the post-ordering problem.",
            "cx": 3426.6,
            "cy": -565.311,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6303",
            "name": "Can {SMT} and {RBMT} Improve each other{'}s Performance?- An Experiment with {E}nglish-{H}indi Translation",
            "publication_data": 2016,
            "citation": 3,
            "abstract": "None",
            "cx": 2128.6,
            "cy": -475.571,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-4622",
            "name": "{IITP} {E}nglish-{H}indi Machine Translation System at {WAT} 2016",
            "publication_data": 2016,
            "citation": 3,
            "abstract": "In this paper we describe the system that we develop as part of our participation in WAT 2016. We develop a system based on hierarchical phrase-based SMT for English to Hindi language pair. We perform re-ordering and augment bilingual dictionary to improve the performance. As a baseline we use a phrase-based SMT model. The MT models are fine-tuned on the development set, and the best configurations are used to report the evaluation on the test set. Experiments show the BLEU of 13.71 on the benchmark test data. This is better compared to the official baseline BLEU score of 10.79.",
            "cx": 2320.6,
            "cy": -475.571,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1485",
            "name": "Synset Ranking of {H}indi {W}ord{N}et",
            "publication_data": 2016,
            "citation": 1,
            "abstract": "Word Sense Disambiguation (WSD) is one of the open problems in the area of natural language processing. Various supervised, unsupervised and knowledge based approaches have been proposed for automatically determining the sense of a word in a particular context. It has been observed that such approaches often find it difficult to beat the WordNet First Sense (WFS) baseline which assigns the sense irrespective of context. In this paper, we present our work on creating the WFS baseline for Hindi language by manually ranking the synsets of Hindi WordNet. A ranking tool is developed where human experts can see the frequency of the word senses in the sense-tagged corpora and have been asked to rank the senses of a word by using this information and also his/her intuition. The accuracy of WFS baseline is tested on several standard datasets. F-score is found to be 60{\\%}, 65{\\%} and 55{\\%} on Health, Tourism and News datasets respectively. The created rankings can also be used in other NLP applications viz., Machine Translation, Information Retrieval, Text Summarization, etc.",
            "cx": 3887.6,
            "cy": -475.571,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I17-2048",
            "name": "Utilizing Lexical Similarity between Related, Low-resource Languages for Pivot-based {SMT}",
            "publication_data": 2017,
            "citation": 1,
            "abstract": "We investigate pivot-based translation between related languages in a low resource, phrase-based SMT setting. We show that a subword-level pivot-based SMT model using a related pivot language is substantially better than word and morpheme-level pivot models. It is also highly competitive with the best direct translation model, which is encouraging as no direct source-target training corpus is used. We also show that combining multiple related language pivot models can rival a direct translation model. Thus, the use of subwords as translation units coupled with multiple related pivot languages can compensate for the lack of a direct parallel corpus.",
            "cx": 1056.6,
            "cy": -385.831,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1548",
            "name": "The {IIT} {B}ombay {E}nglish-{H}indi Parallel Corpus",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "None",
            "cx": 3151.6,
            "cy": -296.09,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N19-1387",
            "name": "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages",
            "publication_data": 2019,
            "citation": 6,
            "abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality in extremely low-resource scenarios.",
            "cx": 1756.6,
            "cy": -206.35,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C08-2007",
            "name": "{H}indi Compound Verbs and their Automatic Extraction",
            "publication_data": 2008,
            "citation": 19,
            "abstract": "We analyse Hindi complex predicates and propose linguistic tests for their detection. This analysis enables us to identify a category of VV complex predicates called lexical compound verbs (LCpdVs) which need to be stored in the dictionary. Based on the linguistic analysis, a simple automatic method has been devised for extracting LCpdVs from corpora. We achieve an accuracy of around 98% in this task. The LCpdVs thus extracted may be used to automatically augment lexical resources like wordnets, an otherwise time consuming and labourintensive process",
            "cx": 9459.6,
            "cy": -1193.49,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L16-1369",
            "name": "Multiword Expressions Dataset for {I}ndian Languages",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "Multiword Expressions (MWEs) are used frequently in natural languages, but understanding the diversity in MWEs is one of the open problem in the area of Natural Language Processing. In the context of Indian languages, MWEs play an important role. In this paper, we present MWEs annotation dataset created for Indian languages viz., Hindi and Marathi. We extract possible MWE candidates using two repositories: 1) the POS-tagged corpus and 2) the IndoWordNet synsets. Annotation is done for two types of MWEs: compound nouns and light verb constructions. In the process of annotation, human annotators tag valid MWEs from these candidates based on the standard guidelines provided to them. We obtained 3178 compound nouns and 2556 light verb constructions in Hindi and 1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems.",
            "cx": 9459.6,
            "cy": -475.571,
            "rx": 108.789,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C08-1068",
            "name": "{H}indi {U}rdu Machine Transliteration using Finite-State Transducers",
            "publication_data": 2008,
            "citation": 30,
            "abstract": "Finite-state Transducers (FST) can be very efficient to implement inter-dialectal transliteration. We illustrate this on the Hindi and Urdu language pair. FSTs can also be used for translation between surface-close languages. We introduce UIT (universal intermediate transcription) for the same pair on the basis of their common phonetic repository in such a way that it can be extended to other languages like Arabic, Chinese, English, French, etc. We describe a transliteration model based on FST and UIT, and evaluate it on Hindi and Urdu corpora.",
            "cx": 9772.6,
            "cy": -1193.49,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W09-3536",
            "name": "A Hybrid Model for {U}rdu {H}indi Transliteration",
            "publication_data": 2009,
            "citation": 20,
            "abstract": "We report in this paper a novel hybrid approach for Urdu to Hindi transliteration that combines finite-state machine (FSM) based techniques with statistical word language model based approach. The output from the FSM is filtered with the word language model to produce the correct Hindi output. The main problem handled is the case of omission of diacritical marks from the input Urdu text. Our system produces the correct Hindi output even when the crucial information in the form of diacritic marks is absent. The approach improves the accuracy of the transducer-only approach from 50.7% to 79.1%. The results reported show that performance can be improved using a word language model to disambiguate the output produced by the transducer-only approach, especially when diacritic marks are not present in the Urdu input.",
            "cx": 9718.6,
            "cy": -1103.75,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C10-2091",
            "name": "Finite-state Scriptural Translation",
            "publication_data": 2010,
            "citation": 2,
            "abstract": "We use robust and fast Finite-State Machines (FSMs) to solve scriptural translation problems. We describe a phonetico-morphotactic pivot UIT (universal intermediate transcription), based on the common phonetic repository of Indo-Pak languages. It is also extendable to other language groups. We describe a finite-state scriptural translation model based on finite-state transducers and UIT. We report its performance on Hindi, Urdu, Punjabi and Seraiki corpora. For evaluation, we design two classification scales based on the word and sentence accuracies for translation system classifications. We also show that subjective evaluations are vital for real life usage of a translation system in addition to objective evaluations.",
            "cx": 9584.6,
            "cy": -1014.01,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2010.jeptalnrecital-court.7",
            "name": "Weak Translation Problems {--} a case study of Scriptural Translation",
            "publication_data": 2010,
            "citation": 0,
            "abstract": "General purpose, high quality and fully automatic MT is believed to be impossible. We are interested in scriptural translation problems, which are weak sub-problems of the general problem of translation. We introduce the characteristics of the weak problems of translation and of the scriptural translation problems, describe different computational approaches (finite-state, statistical and hybrid) to solve these problems, and report our results on several combinations of Indo-Pak languages and writing systems.",
            "cx": 9836.6,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W13-4706",
            "name": "{U}rdu {H}indi Machine Transliteration using {SMT}",
            "publication_data": 2013,
            "citation": 3,
            "abstract": "Transliteration is a process of transcribing a word of the source language into the target language such that when the native speaker of the target language pronounces it, it sounds as the native pronunciation of the source word. Statistical techniques have brought significant advances and have made real progress in various fields of Natural Language Processing (NLP). In this paper, we have analysed the application of Statistical Machine Translation (SMT) for solving the problem of Urdu Hindi transliteration using a parallel lexicon. We have designed total 24 Statistical Transliteration (ST) systems by combining different types of alignments, translation models and target language models. We have performed total 576 experiments and have reported significant results. From Hindixe2x80x93toxe2x80x93Urdu transliteration, we have achieved the maximum word-level accuracy of 71.5%. From Urduxe2x80x93toxe2x80x93Hindi transliteration, the maximum word-level accuracy is 77.8% when the input Urdu text contains all necessary diacritical marks and 77% when the input Urdu text does not contain all necessary diacritical marks. At character-level, transliteration accuracy is more than 90% in both directions.",
            "cx": 9836.6,
            "cy": -744.791,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 98,
            "name": 98,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5914",
            "name": "Solving Data Sparsity by Morphology Injection in Factored {SMT}",
            "publication_data": 2015,
            "citation": 3,
            "abstract": "None",
            "cx": 2139.6,
            "cy": -565.311,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D09-1048",
            "name": "Projecting Parameters for Multilingual Word Sense Disambiguation",
            "publication_data": 2009,
            "citation": 14,
            "abstract": "We report in this paper a way of doing Word Sense Disambiguation (WSD) that has its origin in multilingual MT and that is cognizant of the fact that parallel corpora, wordnets and sense annotated corpora are scarce resources. With respect to these resources, languages show different levels of readiness; however a more resource fortunate language can help a less resource fortunate language. Our WSD method can be applied to a language even when no sense tagged corpora for that language is available. This is achieved by projecting wordnet and corpus parameters from another language to the language in question. The approach is centered around a novel synset based multilingual dictionary and the empirical observation that within a domain the distribution of senses remains more or less invariant across languages. The effectiveness of our approach is verified by doing parameter projection and then running two different WSD algorithms. The accuracy values of approximately 75% (F1-score) for three languages in two different domains establish the fact that within a domain it is possible to circumvent the problem of scarcity of resources by projecting parameters like sense distributions, corpus-co-occurrences, conceptual distance, etc. from one language to another.",
            "cx": 4075.6,
            "cy": -1103.75,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C10-1063",
            "name": "Value for Money: Balancing Annotation Effort, Lexicon Building and Accuracy for Multilingual {WSD}",
            "publication_data": 2010,
            "citation": 7,
            "abstract": "Sense annotation and lexicon building are costly affairs demanding prudent investment of resources. Recent work on multilingual WSD has shown that it is possible to leverage the annotation work done for WSD of one language (SL) for another (TL), by projecting Wordnet and sense marked corpus parameters of SL to TL. However, this work does not take into account the cost of manually cross-linking the words within aligned synsets. Further, it does not answer the question of Can better accuracy be achieved if a user is willing to pay additional money? We propose a measure for cost-benefit analysis which measures the value for money earned in terms of accuracy by investing in annotation effort and lexicon building. Two key ideas explored in this paper are (i) the use of probabilistic cross-linking model to reduce manual cross-linking effort and (ii) the use of selective sampling to inject a few training examples for hard-to-disambiguate words from the target language to boost the accuracy.",
            "cx": 4022.6,
            "cy": -1014.01,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1057",
            "name": "Together We Can: Bilingual Bootstrapping for {WSD}",
            "publication_data": 2011,
            "citation": 9,
            "abstract": "Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can benefit from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.",
            "cx": 4056.6,
            "cy": -924.271,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I11-1078",
            "name": "It Takes Two to Tango: A Bilingual Unsupervised Approach for Estimating Sense Distributions using Expectation Maximization",
            "publication_data": 2011,
            "citation": 4,
            "abstract": "Several bilingual WSD algorithms which exploit translation correspondences between parallel corpora have been proposed. However, the availability of such parallel corpora itself is a tall task for some of the resource constrained languages of the world. We propose an unsupervised bilingual EM based algorithm which relies on the counts of translations to estimate sense distributions. No parallel or sense annotated corpora are needed. The algorithm relies on a synset-aligned bilingual dictionary and in-domain corpora from the two languages. A symmetric generalized Expectation Maximization formulation is used wherein the sense distributions of words in one language are estimated based on the raw counts of the words in the aligned synset in the target language. The overall performance of our algorithm when tested on 4 language-domain pairs is better than current state-of-the-art knowledge based and bilingual unsupervised ap",
            "cx": 3884.6,
            "cy": -924.271,
            "rx": 62.4516,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.57",
            "name": "Mapping it differently: A solution to the linking challenges",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "This paper reports the work of creating bilingual mappings in English for certain synsets of Hindi wordnet, the need for doing this, the methods adopted and the tools created for the task. Hindi wordnet, which forms the foundation for other Indian language wordnets, has been linked to the English WordNet. To maximize linkages, an important strategy of using direct and hypernymy linkages has been followed. However, the hypernymy linkages were found to be inadequate in certain cases and posed a challenge due to sense granularity of language. Thus, the idea of creating bilingual mappings was adopted as a solution. A bilingual mapping means a linkage between a concept in two different languages, with the help of translation and/or transliteration. Such mappings retain meaningful representations, while capturing semantic similarity at the same time. This has also proven to be a great enhancement of Hindi wordnet and can be a crucial resource for multilingual applications in natural language processing, including machine translation and cross language information retrieval.",
            "cx": 4358.6,
            "cy": -475.571,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2011",
            "citation_count": 88,
            "name": 88,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-4011",
            "name": "More Languages, More {MAP}?: A Study of Multiple Assisting Languages in Multilingual {PRF}",
            "publication_data": 2010,
            "citation": 0,
            "abstract": "Multilingual Pseudo-Relevance Feedback (MultiPRF) is a framework to improve the PRF of a source language by taking the help of another language called assisting language. In this paper, we extend the MultiPRF framework to include multiple assisting languages. We consider three different configurations to incorporate multiple assisting languages - a) Parallel - all assisting languages combined simultaneously b) Serial - assisting languages combined in sequence one after another and c) Selective - dynamically selecting the best feedback model for each query. We study their effect on MultiPRF performance. Results using multiple assisting languages are mixed and it helps in boosting MultiPRF accuracy only in some cases. We also observe that MultiPRF becomes more robust with increase in number of assisting languages.",
            "cx": 1392.6,
            "cy": -1014.01,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-3604",
            "name": "A Paradigm-Based Finite State Morphological Analyzer for {M}arathi",
            "publication_data": 2010,
            "citation": 12,
            "abstract": "A morphological analyzer forms the foundation for many NLP applications of Indian Languages. In this paper, we propose and evaluate the morphological analyzer for Marathi, an inflectional language. The morphological analyzer exploits the efficiency and flexibility offered by finite state machines in modeling the morphotactics while using the well devised system of paradigms to handle the stem alternations intelligently by exploiting the regularity in inflectional forms. We plug the morphological analyzer with statistical pos tagger and chunker to see its impact on their performance so as to confirm its usability as a foundation for NLP applications.",
            "cx": 10270.6,
            "cy": -1014.01,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W12-5020",
            "name": "Error tracking in search engine development",
            "publication_data": 2012,
            "citation": 1,
            "abstract": "In this paper, we describe a tool that allows one to track the output of every module of a search engine. The tool provides the ability to perform pseudo error-correction by allowing the user to modify these outputs or tune parameters of the modules to check for improvement of results. Often it is important to see if certain surface level changes can help in the improvement of the result quality. This is crucial since it saves the immediate need to make changes in the system in terms of resource updation or development efforts. We describe query processing pipeline in sufficient detail and then show the efficacy of our tool for an example in Marathi along with giving a thorough error analysis for the example considered. We believe this paper will establish that such a tool is of significant importance for instant detection and correction of errors along with giving the readers an idea on how to develop the same.",
            "cx": 10072.6,
            "cy": -834.531,
            "rx": 81.135,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-2023",
            "name": "Morphological Analyzer for Affix Stacking Languages: A Case Study of {M}arathi",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "In this paper we describe and evaluate a Finite State Machine (FSM) based Morphological Analyzer (MA) for Marathi, a highly inflectional language with agglutinative su ffixes. Marathi belongs to the Indo-European family and is considerably influenced by Dravidian languages. Adroit handling of participial constructions and other derived forms ( Krudantas and Taddhitas) in addition to inflected forms is crucial to NLP and MT of Marathi. We firs t describe Marathi morphological phenomena, detailing the complexities of inflectional and derivational morphology, and then go into the construction and working of the MA. The MA produces the root word and the features. A thorough evaluation against gold standard data establish es the efficacy of this MA. To the best of our knowledge, this work is t he first of its kind on a systematic and exhaustive study of the Morphotactics of a suffix-stac king language, leading to high quality morph analyzer. The system forms part of a Marathi -Hindi transfer based machine translation system. The methodology delineated in the paper can be replicated fo r other languages showing similar suffix stacking behaviour as Marathi.",
            "cx": 10270.6,
            "cy": -834.531,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5103",
            "name": "Tackling Close Cousins: Experiences In Developing Statistical Machine Translation Systems For {M}arathi And {H}indi",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "None",
            "cx": 10396.6,
            "cy": -655.051,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-3607",
            "name": "Hybrid Stemmer for {G}ujarati",
            "publication_data": 2010,
            "citation": 20,
            "abstract": "In this paper we present a lightweight stemmer for Gujarati using a hybrid approach. Instead of using a completely unsupervised approach, we have harnessed linguistic knowledge in the form of a hand-crafted Gujarati suffix list in order to improve the quality of the stems and suffixes learnt during the training phase. We used the EMILLE corpus for training and evaluating the stemmerxe2x80x99s performance. The use of hand-crafted suffixes boosted the accuracy of our stemmer by about 17% and helped us achieve an accuracy of 67.86 %.",
            "cx": 10504.6,
            "cy": -1014.01,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W11-3001",
            "name": "Hybrid Inflectional Stemmer and Rule-based Derivational Stemmer for {G}ujarati",
            "publication_data": 2011,
            "citation": 24,
            "abstract": "In this paper we present two stemmers for Gujaratia lightweight inflectional stemmer based on a hybrid approach and a heavyweight derivational stemmer based on a rule-based approach. Besides using a module for unsupervised learning of stems and suffixes for lightweight stemming, we have also included a module performing POS (Part Of Speech) based stemming and a module using a set of substitution rules, in order to improve the quality of these stems and suffixes. The inclusion of these modules boosted the accuracy of the inflectional stemmer by 9.6% and 12.7% respectively, helping us achieve an accuracy of 90.7%. The maximum index compression obtained for the inflectional stemmer is about 95%. On the other hand, the derivational stemmer is completely rule-based, for which, we attained an accuracy of 70.7% with the help of suffix-stripping, substitution and orthographic rules. Both these systems were developed to be useful in applications such as Information Retrieval, corpus compression, dictionary search and as pre-processing modules in other NLP problems such as WSD.",
            "cx": 10504.6,
            "cy": -924.271,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1137",
            "name": "Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages",
            "publication_data": 2010,
            "citation": 8,
            "abstract": "In a previous work of ours Chinnakotla et al. (2010) we introduced a novel framework for Pseudo-Relevance Feedback (PRF) called MultiPRF. Given a query in one language called Source, we used English as the Assisting Language to improve the performance of PRF for the source language. MulitiPRF showed remarkable improvement over plain Model Based Feedback (MBF) uniformly for 4 languages, viz., French, German, Hungarian and Finnish with English as the assisting language. This fact inspired us to study the effect of any source-assistant pair on MultiPRF performance from out of a set of languages with widely different characteristics, viz., Dutch, English, Finnish, French, German and Spanish. Carrying this further, we looked into the effect of using two assisting languages together on PRF.n n The present paper is a report of these investigations, their results and conclusions drawn therefrom. While performance improvement on MultiPRF is observed whatever the assisting language and whatever the source, observations are mixed when two assisting languages are used simultaneously. Interestingly, the performance improvement is more pronounced when the source and assisting languages are closely related, e.g., French and Spanish.",
            "cx": 1162.6,
            "cy": -1014.01,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N15-1125",
            "name": "Leveraging Small Multilingual Corpora for {SMT} Using Many Pivot Languages",
            "publication_data": 2015,
            "citation": 10,
            "abstract": "We present our work on leveraging multilingual parallel corpora of small sizes for Statistical Machine Translation between Japanese and Hindi using multiple pivot languages. In our setting, the source and target part of the corpus remains the same, but we show that using several different pivot to extract phrase pairs from these source and target parts lead to large BLEU improvements. We focus on a variety of ways to exploit phrase tables generated using multiple pivots to support a direct source-target phrase table. Our main method uses the Multiple Decoding Paths (MDP) feature of Moses, which we empirically verify as the best compared to the other methods we used. We compare and contrast our various results to show that one can overcome the limitations of small corpora by using as many pivot languages as possible in a multilingual setting. Most importantly, we show that such pivoting aids in learning of additional phrase pairs which are not learned when the direct sourcetarget corpus is small. We obtained improvements of up to 3 BLEU points using multiple pivots for Japanese to Hindi translation compared to when only one pivot is used. To the best of our knowledge, this work is also the first of its kind to attempt the simultaneous utilization of 7 pivot languages at decoding time.",
            "cx": 1137.6,
            "cy": -565.311,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-1155",
            "name": "All Words Domain Adapted {WSD}: Finding a Middle Ground between Supervision and Unsupervision",
            "publication_data": 2010,
            "citation": 19,
            "abstract": "In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creating the training corpus - annotated sense marked corpora - has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora. However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy. Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain. We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus. Accuracy figures close to self domain training lend credence to the viability of our approach. Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.",
            "cx": 2690.6,
            "cy": -1014.01,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-2096",
            "name": "Neighbors Help: Bilingual Unsupervised {WSD} Using Context",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modification to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.",
            "cx": 3411.6,
            "cy": -744.791,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-0124",
            "name": "Graph Based Algorithm for Automatic Domain Segmentation of {W}ord{N}et",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "We present a graph based algorithm for automatic domain segmentation of Wordnet. We pose the problem as a Markov Random Field Classification problem and show how existing graph based algorithms for Image Processing can be used to solve the problem. Our approach is unsupervised and can be easily adopted for any language. We conduct our experiments for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology ex-",
            "cx": 2561.6,
            "cy": -655.051,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5908",
            "name": "Using Word Embeddings for Bilingual Unsupervised {WSD}",
            "publication_data": 2015,
            "citation": 1,
            "abstract": "None",
            "cx": 3624.6,
            "cy": -565.311,
            "rx": 77.1494,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5945",
            "name": "Using Multilingual Topic Models for Improved Alignment in {E}nglish-{H}indi {MT}",
            "publication_data": 2015,
            "citation": 1,
            "abstract": "None",
            "cx": 2451.6,
            "cy": -565.311,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1349",
            "name": "That{'}ll Do Fine!: A Coarse Lexical Resource for {E}nglish-{H}indi {MT}, Using Polylingual Topic Models",
            "publication_data": 2016,
            "citation": 1,
            "abstract": "Parallel corpora are often injected with bilingual lexical resources for improved Indian language machine translation (MT). In absence of such lexical resources, multilingual topic models have been used to create coarse lexical resources in the past, using a Cartesian product approach. Our results show that for morphologically rich languages like Hindi, the Cartesian product approach is detrimental for MT. We then present a novel {`}sentential{'} approach to use this coarse lexical resource from a multilingual topic model. Our coarse lexical resource when injected with a parallel corpus outperforms a system trained using parallel corpus and a good quality lexical resource. As demonstrated by the quality of our coarse lexical resource and its benefit to MT, we believe that our sentential approach to create such a resource will help MT for resource-constrained languages.",
            "cx": 2520.6,
            "cy": -475.571,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.54",
            "name": "High, Medium or Low? Detecting Intensity Variation Among polar synonyms in {W}ord{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "For fine-grained sentiment analysis, we need to go beyond zero-one polarity and find a way to compare adjectives (synonyms) that share the same sense. Choice of a word from a set of synonyms, provides a way to select the exact polarity-intensity. For example, choosing to describe a person as benevolent rather than kind1 changes the intensity of the expression. In this paper, we present a sense based lexical resource, where synonyms are assigned intensity levels, viz., high, medium and low. We show that the measure P (s|w) (probability of a sense s given the word w) can derive the intensity of a word within the sense. We observe a statistically significant positive correlation between P(s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi.",
            "cx": 2709.6,
            "cy": -475.571,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N10-1065",
            "name": "Everybody loves a rich cousin: An empirical study of transliteration through bridge languages",
            "publication_data": 2010,
            "citation": 17,
            "abstract": "Most state of the art approaches for machine transliteration are data driven and require significant parallel names corpora between languages. As a result, developing transliteration functionality among n languages could be a resource intensive task requiring parallel names corpora in the order of nC2. In this paper, we explore ways of reducing this high resource requirement by leveraging the available parallel data between subsets of the n languages, transitively. We propose, and show empirically, that reasonable quality transliteration engines may be developed between two languages, X and Y, even when no direct parallel names data exists between them, but only transitively through language Z. Such systems alleviate the need for O(nC2) corpora, significantly. In addition we show that the performance of such transitive transliteration systems is in par with direct transliteration systems, in practical applications, such as CLIR systems.",
            "cx": 1892.6,
            "cy": -1014.01,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "khapra-etal-2014-transliteration",
            "name": "When Transliteration Met Crowdsourcing : An Empirical Study of Transliteration via Crowdsourcing using Efficient, Non-redundant and Fair Quality Control",
            "publication_data": 2014,
            "citation": 4,
            "abstract": "Sufficient parallel transliteration pairs are needed for training state of the art transliteration engines. Given the cost involved, it is often infeasible to collect such data using experts. Crowdsourcing could be a cheaper alternative, provided that a good quality control (QC) mechanism can be devised for this task. Most QC mechanisms employed in crowdsourcing are aggressive (unfair to workers) and expensive (unfair to requesters). In contrast, we propose a low-cost QC mechanism which is fair to both workers and requesters. At the heart of our approach, lies a rule based Transliteration Equivalence approach which takes as input a list of vowels in the two languages and a mapping of the consonants in the two languages. We empirically show that our approach outperforms other popular QC mechanisms ({\\textbackslash}textit{viz.}, consensus and sampling) on two vital parameters : (i) fairness to requesters (lower cost per correct transliteration) and (ii) fairness to workers (lower rate of rejecting correct answers). Further, as an extrinsic evaluation we use the standard NEWS 2010 test set and show that such quality controlled crowdsourced data compares well to expert data when used for training a transliteration engine.",
            "cx": 1830.6,
            "cy": -655.051,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "K16-1027",
            "name": "Substring-based unsupervised transliteration with phonetic and contextual knowledge",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 1828.6,
            "cy": -475.571,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "Q18-1022",
            "name": "Leveraging Orthographic Similarity for Multilingual Neural Transliteration",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "We address the task of joint training of transliteration models for multiple language pairs (multilingual transliteration). This is an instance of multitask learning, where individual tasks (language pairs) benefit from sharing knowledge with related tasks. We focus on transliteration involving related tasks i.e., languages sharing writing systems and phonetic properties (orthographically similar languages). We propose a modified neural encoder-decoder model that maximizes parameter sharing across language pairs in order to effectively leverage orthographic similarity. We show that multilingual transliteration significantly outperforms bilingual transliteration in different scenarios (average increase of 58{\\%} across a variety of languages we experimented with). We also show that multilingual transliteration models can generalize well to languages/language pairs not encountered during training and hence perform well on the zeroshot transliteration task. We show that further improvements can be achieved by using phonetic feature input.",
            "cx": 1727.6,
            "cy": -296.09,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "bhattacharyya-2010-indowordnet",
            "name": "{I}ndo{W}ord{N}et",
            "publication_data": 2010,
            "citation": "???",
            "abstract": "India is a multilingual country where machine translation and cross lingual search are highly relevant problems. These problems require large resources- like wordnets and lexicons- of high quality and coverage. Wordnets are lexical structures composed of synsets and semantic relations. Synsets are sets of synonyms. They are linked by semantic relations like hypernymy (is-a), meronymy (part-of), troponymy (manner-of) etc. IndoWordnet is a linked structure of wordnets of major Indian languages from Indo-Aryan, Dravidian and Sino-Tibetan families. These wordnets have been created by following the expansion approach from Hindi wordnet which was made available free for research in 2006. Since then a number of Indian languages have been creating their wordnets. In this paper we discuss the methodology, coverage, important considerations and multifarious benefits of IndoWordnet. Case studies are provided for Marathi, Sanskrit, Bodo and Telugu, to bring out the basic methodology of and challenges involved in the expansion approach. The guidelines the lexicographers follow for wordnet construction are enumerated. The difference between IndoWordnet and EuroWordnet also is discussed.",
            "cx": 4839.6,
            "cy": -1014.01,
            "rx": 148.485,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W12-5209",
            "name": "Domain Specific Ontology Extractor For {I}ndian Languages",
            "publication_data": 2012,
            "citation": 8,
            "abstract": "We present a k-partite graph learning algorithm for ontology extraction from unstructured text. The algorithm divides the initial set of terms into different partitions based on information content of the terms and then constructs ontology by detecting subsumption relation between terms in different partitions. This approach not only reduces the amount of computation required for ontology construction but also provides an additional level of term filtering. The experiments are conducted for Hindi and English and the performance is evaluated by comparing resulting ontology with manually constructed ontology for Health domain. We observe that our approach significantly improves the precision. The proposed approach does not require sophisticated NLP tools such as NER and parser and can be easily adopted for any language.",
            "cx": 4330.6,
            "cy": -834.531,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-2008",
            "name": "Cross-Lingual Sentiment Analysis for {I}ndian Languages using Linked {W}ord{N}ets",
            "publication_data": 2012,
            "citation": 38,
            "abstract": "Cross-Lingual Sentiment Analysis (CLSA) is the task of predicting the polarity of the opinion expressed in a text in a language Ltest using a classifier trained on the corpus of another language Lt rain. Popular approaches use Machine Translation (MT) to convert the test document in Ltest to Lt rain and use the classifier of Lt rain. However, MT systems do not exist for most pairs of languages and even if they do, their translation accuracy is low. So we present an alternative approach to CLSA using WordNet senses as features for supervised sentiment classification. A document in Ltest is tested for polarity through a classifier trained on sense marked and polarity labeled corpora of Lt rain. The crux of the idea is to use the linked WordNets of two languages to bridge the language gap. We report our results on two widely spoken Indian languages, Hindi (450 million speakers) and Marathi (72 million speakers), which do not have an MT system between them. The sense-based approach gives a CLSA accuracy of 72% and 84% for Hindi and Marathi sentiment classification respectively. This is an improvement of 14%-15% over an approach that uses a bilingual dictionary.",
            "cx": 4994.6,
            "cy": -834.531,
            "rx": 126.644,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L18-1728",
            "name": "{I}ndian Language Wordnets and their Linkages with {P}rinceton {W}ord{N}et",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "None",
            "cx": 4334.6,
            "cy": -296.09,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018.gwc-1.31",
            "name": "Semi-automatic {W}ord{N}et Linking using Word Embeddings",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of wordnets, which link similar concepts in wordnets of different languages. Such resources are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these resources are considered as gold standard/oracle. Thus, it is crucial that these resources hold correct information. Thereby, they are created by human experts. However, manual maintenance of such resources is a tedious and costly affair. Thus techniques that can aid the experts are desirable. In this paper, we propose an approach to link wordnets. Given a synset of the source language, the approach returns a ranked list of potential candidate synsets in the target language from which the human expert can choose the correct one(s). Our technique is able to retrieve a winner synset in the top 10 ranked list for 60{\\%} of all synsets and 70{\\%} of noun synsets.",
            "cx": 4558.6,
            "cy": -296.09,
            "rx": 112.36,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W19-7509",
            "name": "Introduction to {S}anskrit Shabdamitra: An Educational Application of {S}anskrit {W}ordnet",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "None",
            "cx": 8057.6,
            "cy": -206.35,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W15-5905",
            "name": "Noun Phrase Chunking for {M}arathi using Distant Supervision",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Information Extraction from Indian languages requires effective shallow parsing, especially identification of xe2x80x9cmeaningfulxe2x80x9d noun phrases. Particularly, for an agglutinative and free word order language like Marathi, this problem is quite challenging. We model this task of extracting noun phrases as a sequence labelling problem. A Distant Supervision framework is used to automatically create a large labelled data for training the sequence labelling model. The framework exploits a set of heuristic rules based on corpus statistics for the automatic labelling. Our approach puts together the benefits of heuristic rules, a large unlabelled corpus as well as supervised learning to model complex underlying characteristics of noun phrase occurrences. In comparison to a simple English-like chunking baseline and a publicly available Marathi Shallow Parser, our method demonstrates a better performance.",
            "cx": 7891.6,
            "cy": -565.311,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "ar-etal-2012-cost",
            "name": "Cost and Benefit of Using {W}ord{N}et Senses for Sentiment Analysis",
            "publication_data": 2012,
            "citation": 4,
            "abstract": "Typically, accuracy is used to represent the performance of an NLP system. However, accuracy attainment is a function of investment in annotation. Typically, the more the amount and sophistication of annotation, higher is the accuracy. However, a moot question is ''''''``is the accuracy improvement commensurate with the cost incurred in annotation''''''''? We present an economic model to assess the marginal benefit accruing from increase in cost of annotation. In particular, as a case in point we have chosen the sentiment analysis (SA) problem. In SA, documents normally are polarity classified by running them through classifiers trained on document vectors constructed from lexeme features, i.e., words. If, however, instead of words, one uses word senses (synset ids in wordnets) as features, the accuracy improves dramatically. But is this improvement significant enough to justify the cost of annotation? This question, to the best of our knowledge, has not been investigated with the seriousness it deserves. We perform a cost benefit study based on a vendor-machine model. By setting up a cost price, selling price and profit scenario, we show that although extra cost is incurred in sense annotation, the profit margin is high, justifying the cost.",
            "cx": 4525.6,
            "cy": -834.531,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 132,
            "name": 132,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-4022",
            "name": "{C}-Feel-It: A Sentiment Analyzer for Micro-blogs",
            "publication_data": 2011,
            "citation": 25,
            "abstract": "Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.",
            "cx": 5851.6,
            "cy": -924.271,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C12-1113",
            "name": "Sentiment Analysis in {T}witter with Lightweight Discourse Analysis",
            "publication_data": 2012,
            "citation": 51,
            "abstract": "We propose a lightweight method for using discourse relations for polarity detection of tweets . This method is targeted towards the web-based appli cations that deal with noisy, unstructured text, like the tweets, and cannot afford to use heavy linguistic resource s like parsing due to frequent failure of the parsers to handle noisy dat a. Most of the works in micro-blogs, like Twitter, use a bag-of-words model that ignores the discours e particles like but, since, although etc. In this work, we show how the discourse relations like the connectives and conditionals can be used to incorporate discourse information in any bag-of-words model, to improve sentiment classification accuracy. We also probe the influenc e of the semantic operators like modals and negations on the discourse relations that affect the sentime nt of a sentence. Discourse relations and corresponding rules are identified with minimal processing - just a list look up. We first give a linguistic description of the various discourse r elations which leads to conditions in rules and features in SVM. We show that our discourse-based bag-of-words model performs well in a noisy medium ( Twitter ), where it performs better than an existing Twitte r-based application. Furthermore, we show that our approach is beneficia l to structured reviews as well, where we achieve a better accuracy than a state-of-the-art s ystem in the travel review domain. Our system compares favorably with the state-of-the-art system s and has the additional attractiveness of being less resource intensive.",
            "cx": 5921.6,
            "cy": -834.531,
            "rx": 112.36,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "S13-2082",
            "name": "{IITB}-Sentiment-Analysts: Participation in Sentiment Analysis in {T}witter {S}em{E}val 2013 Task",
            "publication_data": 2013,
            "citation": 13,
            "abstract": "We propose a method for using discourse relations for polarity detection of tweets. We have focused on unstructured and noisy text like tweets on which linguistic tools like parsers and POS-taggers donxe2x80x99t work properly. We have showed how conjunctions, connectives, modals and conditionals affect the sentiments in tweets. We have also handled the commonly used abbreviations, slangs and collocations which are usually used in short text messages like tweets. This work focuses on a Web based application which produces results in real time. This approach is an extension of the previous work (Mukherjee et al. 2012).",
            "cx": 5813.6,
            "cy": -744.791,
            "rx": 161.855,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N13-1088",
            "name": "More than meets the eye: Study of Human Cognition in Sense Annotation",
            "publication_data": 2013,
            "citation": 12,
            "abstract": "Word Sense Disambiguation (WSD) approaches have reported good accuracies in recent years. However, these approaches can be classified as weak AI systems. According to the classical definition, a strong AI based WSD system should perform the task of sense disambiguation in the same manner and with similar accuracy as human beings. In order to accomplish this, a detailed understanding of the human techniques employed for sense disambiguation is necessary. Instead of building yet another WSD system that uses contextual evidence for sense disambiguation, as has been done before, we have taken a step back - we have endeavored to discover the cognitive faculties that lie at the very core of the human sense disambiguation technique. In this paper, we present a hypothesis regarding the cognitive sub-processes involved in the task of WSD. We support our hypothesis using the experiments conducted through the means of an eye-tracking device. We also strive to find the levels of difficulties in annotating various classes of words, with senses. We believe, once such an in-depth analysis is performed, numerous insights can be gained to develop a robust WSD system that conforms to the principle of strong AI.",
            "cx": 5490.6,
            "cy": -744.791,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1100",
            "name": "Harnessing {W}ord{N}et Senses for Supervised Sentiment Classification",
            "publication_data": 2011,
            "citation": 26,
            "abstract": "Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-of-speech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline.",
            "cx": 5187.6,
            "cy": -924.271,
            "rx": 113.689,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1041",
            "name": "The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis",
            "publication_data": 2013,
            "citation": 20,
            "abstract": "Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA.",
            "cx": 4376.6,
            "cy": -744.791,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W14-2619",
            "name": "Dive deeper: Deep Semantics for Sentiment Analysis",
            "publication_data": 2014,
            "citation": 4,
            "abstract": "This paper illustrates the use of deep semantic processing for sentiment analysis. Existing methods for sentiment analysis use supervised approaches which take into account all the subjective words and or phrases. Due to this, the fact that not all of these words and phrases actually contribute to the overall sentiment of the text is ignored. We propose an unsupervised rule-based approach using deep semantic processing to identify only relevant subjective terms. We generate a UNL (Universal Networking Language) graph for the input text. Rules are applied on the graph to extract relevant terms. The sentiment expressed in these terms is used to figure out the overall sentiment of the text. Results on binary sentiment classification have shown promising results.",
            "cx": 5761.6,
            "cy": -655.051,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5920",
            "name": "Domain Sentiment Matters: A Two Stage Sentiment Analyzer",
            "publication_data": 2015,
            "citation": 2,
            "abstract": "There are words that change its polarity from domain to domain. For example, the word deadly is of positive polarity in the cricket domain as in xe2x80x9cShane Warne is a xe2x80x98deadlyxe2x80x99 leg spinnerxe2x80x9d. However, xe2x80x98I witnessed a deadly accidentxe2x80x99 carries negative polarity and going by the sentiment in cricket domain will be misleading. In addition to this, there exist domainspecific words, which have the same polarity across domains, but are used very frequently in a particular domain. For example, blockbuster, is specific to the movie domain. We combine such words as Domain Dedicated Polar Words (DDPW). A concise feature set made up of principal polarity clues makes the classifier less expensive in terms of time complexity and enhances the accuracy of classification. In this paper, we show that DDPW make such a concise feature set for sentiment analysis in a domain. Use of domaindedicated polar words as features beats the state of art accuracies achieved independently with unigrams, adjectives or Universal Sentiment Lexicon (USL).",
            "cx": 2938.6,
            "cy": -565.311,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6315",
            "name": "Meaning Matters: Senses of Words are More Informative than Words for Cross-domain Sentiment Analysis",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 5345.6,
            "cy": -475.571,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1429",
            "name": "Aspect based Sentiment Analysis in {H}indi: Resource Creation and Evaluation",
            "publication_data": 2016,
            "citation": 15,
            "abstract": "Due to the phenomenal growth of online product reviews, sentiment analysis (SA) has gained huge attention, for example, by online service providers. A number of benchmark datasets for a wide range of domains have been made available for sentiment analysis, especially in resource-rich languages. In this paper we assess the challenges of SA in Hindi by providing a benchmark setup, where we create an annotated dataset of high quality, build machine learning models for sentiment analysis in order to show the effective usage of the dataset, and finally make the resource available to the community for further advancement of research. The dataset comprises of Hindi product reviews crawled from various online sources. Each sentence of the review is annotated with aspect term and its associated sentiment. As classification algorithms we use Conditional Random Filed (CRF) and Support Vector Machine (SVM) for aspect term extraction and sentiment analysis, respectively. Evaluation results show the average F-measure of 41.07{\\%} for aspect term extraction and accuracy of 54.05{\\%} for sentiment classification.",
            "cx": 4895.6,
            "cy": -475.571,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "K16-1016",
            "name": "Leveraging Cognitive Features for Sentiment Analysis",
            "publication_data": 2016,
            "citation": 9,
            "abstract": "None",
            "cx": 5602.6,
            "cy": -475.571,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1035",
            "name": "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
            "publication_data": 2017,
            "citation": 14,
            "abstract": "Cognitive NLP systems- i.e., NLP systems that make use of behavioral data - augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like Sentiment Analysis and Sarcasm Detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement/gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",
            "cx": 5662.6,
            "cy": -385.831,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 100,
            "name": 100,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W12-4906",
            "name": "A heuristic-based approach for systematic error correction of gaze data for reading",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "In eye-tracking research, temporally constant deviations between usersxe2x80x99 intended gaze location and location captured by eye-samplers are referred to as systematic error. Systematic errors are frequent and add a lot of noise to the data. It also takes a lot of time and effort to manually correct such disparities. In this paper, we propose and validate a heuristic-based technique to reduce such errors associated with gaze fixations by shifting them to their true locations. This technique is exclusively applicable for reading tasks where the visual objects (characters) are placed on a grid in a sequential manner; which is often the case in psycholinguistic studies.",
            "cx": 6139.6,
            "cy": -834.531,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2062",
            "name": "Automatically Predicting Sentence Translation Difficulty",
            "publication_data": 2013,
            "citation": 14,
            "abstract": "In this paper we introduce Translation Difficulty Index (TDI), a measure of difficulty in text translation. We first define and quantify translation difficulty in terms of TDI. We realize that any measure of TDI based on direct input by translators is fraught with subjectivity and adhocism. We, rather, rely on cognitive evidences from eye tracking. TDI is measured as the sum of fixation (gaze) and saccade (rapid eye movement) times of the eye. We then establish that TDI is correlated with three properties of the input sentence, viz. length (L), degree of polysemy (DP) and structural complexity (SC). We train a Support Vector Regression (SVR) system to predict TDIs for new sentences using these features as input. The prediction done by our framework is well correlated with the empirical gold standard data, which is a repository of and TDI pairs for a set of sentences. The primary use of our work is a way of xe2x80x9cbinningxe2x80x9d sentences (to be translated) in xe2x80x9ceasyxe2x80x9d, xe2x80x9cmediumxe2x80x9d and xe2x80x9chardxe2x80x9d categories as per their predicted TDI. This can decide pricing of any translation task, especially useful in a scenario where parallel corpora for Machine Translation are built through translation crowdsourcing/outsourcing. This can also provide a way of monitoring progress of second language learners.",
            "cx": 6139.6,
            "cy": -744.791,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "kunchukuttan-etal-2012-experiences",
            "name": "Experiences in Resource Generation for Machine Translation through Crowdsourcing",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "The logistics of collecting resources for Machine Translation (MT) has always been a cause of concern for some of the resource deprived languages of the world. The recent advent of crowdsourcing platforms provides an opportunity to explore the large scale generation of resources for MT. However, before venturing into this mode of resource collection, it is important to understand the various factors such as, task design, crowd motivation, quality control, etc. which can influence the success of such a crowd sourcing venture. In this paper, we present our experiences based on a series of experiments performed. This is an attempt to provide a holistic view of the different facets of translation crowd sourcing and identifying key challenges which need to be addressed for building a practical crowdsourcing solution for MT.",
            "cx": 3722.6,
            "cy": -834.531,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-4030",
            "name": "{T}rans{D}oop: A Map-Reduce based Crowdsourced Translation for Complex Domain",
            "publication_data": 2013,
            "citation": 2,
            "abstract": "Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation of constituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowdxe2x80x99s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work.",
            "cx": 3722.6,
            "cy": -744.791,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-3013",
            "name": "Automated Paradigm Selection for {FSA} based {K}onkani Verb Morphological Analyzer",
            "publication_data": 2012,
            "citation": 5,
            "abstract": "A Morphological Analyzer is a crucial tool for any language. In popular tools used to build morphological analyzers like XFST, HFST and Apertiumxe2x80x99s lttoolbox, the finite state approach is used to sequence input characters. We have used the finite state approach to sequence morphemes instead of characters. In this paper we present the architecture and implementation details of a Corpus assisted FSA approach for build ing a Verb Morphological Analyzer. Our main contribution in this paper is the paradigm def inition methodology used for the verbs in a morphologically rich Indian Language Konkani. The mapping of citation form of the verbs to paradigms was carried out using an untagged corpus for Konkani. Besides a reduction in human effort required an F-Score of 0.95 was obtained whe n the mapping was tested on a tagged corpus.",
            "cx": 10616.6,
            "cy": -834.531,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5136",
            "name": "{A}uto{P}ar{S}e: An Automatic Paradigm Selector For Nouns in {K}onkani",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "None",
            "cx": 10616.6,
            "cy": -655.051,
            "rx": 127.059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-3030",
            "name": "Eating Your Own Cooking: Automatically Linking {W}ordnet Synsets of Two Languages",
            "publication_data": 2012,
            "citation": 1,
            "abstract": "Linked wordnets are invaluable linked lexical resources. Wordnet linking involves matching a particular synset (concept) in one wordnet to a synset in another wordnet. We have developed an automatic wordnet linking system that is divided into a number of stages. Starting with a synset in the first language (also referred to as the source language), our algorithm generates a list of candidate synsets in the second language (also referred to as the target language). In consecutive stages, a heuristic is used to prune and rank this list. The winner synset is then chosen as the linkage for the source synset. The candidate synsets are generated using a bilingual dictionary (BiDict). Further, the earlier heuristics which we developed used BiDict to rank these candidate synsets. However, development of a BiDict is cumbersome and requires human labor. Furthermore, in several cases sparsity of the BiDict handicaps the ranking algorithm to a great extent. We have thus devised heuristics to eliminate the requirement of BiDict during the ranking process by using the already linked synsets. Once sufficient number of linked synsets are available, these heuristics outperform our heuristics which use a BiDict. These heuristics are based on observations made from linking techniques applied by lexicographers. Our wordnet linking system can be used for any pair of languages, given either a BiDict or sufficient number of already linked synsets. The interface of the system is easy to comprehend and use. In this paper, we present this interface along with the developed heuristics.",
            "cx": 4741.6,
            "cy": -834.531,
            "rx": 70.0071,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-3033",
            "name": "Discrimination-Net for {H}indi",
            "publication_data": 2012,
            "citation": 1,
            "abstract": "Current state-of-the-art Word Sense Disambiguation (WSD) algorithms are mostly supervised and use the P (Sense|Word) statistic for annotation. This P (Sense|Word) statistic is obtained after training the model on an annotated corpus. The performance of WSD algorithms do not match the efficiency and quality of human annotation. It is therefore important to know the role of the contextual clues in WSD. Human beings in turn, actuate the task of disambiguating the sense of a word, by gathering hints from the context words in the neighbourhood of the word. Contextual clues thus form the basic building block for the human sense disambiguation task. The need was thus felt for a tool, which could help us get a deeper insight into the human mind, while disambiguating polysemous words. As mentioned earlier, in the human mind, sense disambiguation highly depends on finding clues in corpus text, which finally lead to a winner sense. In order to makeWSD algorithms more efficient, it is highly desirable to assimilate knowledge regarding contextual clues of words. In order to make WSD algorithms more efficient, it is highly desirable to assimilate knowledge regarding contextual clues of words, which aid in finding correct senses of words in that context. Hence, we developed a tool which could help a lexicographer mark the clues for disambiguating a word in a context. In the current phase, this tool lets the lexicographer select the clues from the gloss and example fields in the synset, and adds them to a database.",
            "cx": 5386.6,
            "cy": -834.531,
            "rx": 132.872,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-0126",
            "name": "Do not do processing, when you can look up: Towards a Discrimination Net for {WSD}",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "The task of Word Sense Disambiguation (WSD) incorporates in its definition the role of xe2x80x98contextxe2x80x99. We present our work on the development of a tool which allows for automatic acquisition and ranking of xe2x80x98context cluesxe2x80x99 for WSD. These clue words are extracted from the contexts of words appearing in a large monolingual corpus. These mined collection of contextual clues form a discrimination net in the sense that for targeted WSD, navigation of the net leads to the correct sense of a word given its context. Utilizing this resource we intend to develop efficient and light weight WSD based on look up and navigation of memory-resident knowledge base, thereby avoiding heavy computation which often prevents incorporation of any serious WSD in MT and search. The need for large quantities of sense marked data too can be reduced.",
            "cx": 5324.6,
            "cy": -655.051,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C16-1047",
            "name": "A Hybrid Deep Learning Architecture for Sentiment Analysis",
            "publication_data": 2016,
            "citation": 20,
            "abstract": "In this paper, we propose a novel hybrid deep learning archtecture which is highly efficient for sentiment analysis in resource-poor languages. We learn sentiment embedded vectors from the Convolutional Neural Network (CNN). These are augmented to a set of optimized features selected through a multi-objective optimization (MOO) framework. The sentiment augmented optimized vector obtained at the end is used for the training of SVM for sentiment classification. We evaluate our proposed approach for coarse-grained (i.e. sentence level) as well as fine-grained (i.e. aspect level) sentiment analysis on four Hindi datasets covering varying domains. In order to show that our proposed method is generic in nature we also evaluate it on two benchmark English datasets. Evaluation shows that the results of the proposed method are consistent across all the datasets and often outperforms the state-of-art systems. To the best of our knowledge, this is the very first attempt where such a deep learning model is used for less-resourced languages such as Hindi.",
            "cx": 5035.6,
            "cy": -475.571,
            "rx": 50.41,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C16-1287",
            "name": "Borrow a Little from your Rich Cousin: Using Embeddings and Polarities of {E}nglish Words for Multilingual Sentiment Classification",
            "publication_data": 2016,
            "citation": 5,
            "abstract": "In this paper, we provide a solution to multilingual sentiment classification using deep learning. Given input text in a language, we use word translation into English and then the embeddings of these English words to train a classifier. This projection into the English space plus word embeddings gives a simple and uniform framework for multilingual sentiment analysis. A novel idea is augmentation of the training data with polar words, appearing in these sentences, along with their polarities. This approach leads to a performance gain of 7-10{\\%} over traditional classifiers on many languages, irrespective of text genre, despite the scarcity of resources in most languages.",
            "cx": 4715.6,
            "cy": -475.571,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1053",
            "name": "Solving Data Sparsity for Aspect Based Sentiment Analysis Using Cross-Linguality and Multi-Linguality",
            "publication_data": 2018,
            "citation": 4,
            "abstract": "Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The problem is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus. In this work we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus. We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification. The neural network architecture is further assisted by the hand-crafted features for the prediction. We show the efficacy of the proposed model against state-of-the-art methods in two experimental setups i.e. multi-lingual and cross-lingual.",
            "cx": 5332.6,
            "cy": -296.09,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-0413",
            "name": "Language-Agnostic Model for Aspect-Based Sentiment Analysis",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "In this paper, we propose a language-agnostic deep neural network architecture for aspect-based sentiment analysis. The proposed approach is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network, which is further assisted with extra hand-crafted features. We define three different architectures for the successful combination of word embeddings and hand-crafted features. We evaluate the proposed approach for six languages (i.e. English, Spanish, French, Dutch, German and Hindi) and two problems (i.e. aspect term extraction and aspect sentiment classification). Experiments show that the proposed model attains state-of-the-art performance in most of the settings.",
            "cx": 5232.6,
            "cy": -206.35,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 77,
            "name": 77,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W13-3611",
            "name": "{IITB} System for {C}o{NLL} 2013 Shared Task: A Hybrid Approach to Grammatical Error Correction",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "We describe our grammar correction system for the CoNLL-2013 shared task. Our system corrects three of the five error types specified for the shared task noun-number, determiner and subject-verb agreement errors. For noun-number and determiner correction, we apply a classification approach using rich lexical and syntactic features. For subject-verb agreement correction, we propose a new rulebased system which utilizes dependency parse information and a set of conditional rules to ensure agreement of the verb group with its subject. Our system obtained an F-score of 11.03 on the official test set using the M 2 evaluation method (the official evaluation method).",
            "cx": 10900.6,
            "cy": -744.791,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-1708",
            "name": "Tuning a Grammar Correction System for Increased Precision",
            "publication_data": 2014,
            "citation": 6,
            "abstract": "In this paper, we propose two enhancements to a statistical machine translation based approach to grammar correction for correcting all error categories. First, we propose tuning the SMT systems to optimize a metric more suited to the grammar correction task (F- score) rather than the traditional BLEU metric used for tuning language translation tasks. Since the F- score favours higher precision, tuning to this score can potentially improve precision. While the results do not indicate improvement due to tuning with the new metric, we believe this could be due to the small number of grammatical errors in the tuning corpus and further investigation is required to answer the question conclusively. We also explore the combination of custom-engineered grammar correction techniques, which are targeted to specific error categories, with the SMT based method. Our simple ensemble methods yield improvements in recall but decrease the precision. Tuning the custom-built techniques can help in increasing the overall accuracy also.",
            "cx": 10845.6,
            "cy": -655.051,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5902",
            "name": "Addressing Class Imbalance in Grammatical Error Detection with Evaluation Metric Optimization",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "We address the problem of class imbalance in supervised grammatical error detection (GED) for non-native speaker text, which is the result of the low proportion of erroneous examples compared to a large number of error-free examples. Most learning algorithms maximize accuracy which is not a suitable objective for such imbalanced data. For GED, most systems address this issue by tuning hyperparameters to maximize metrics like Fxcexb2 . Instead, we show that learning classifiers that directly learn model parameters by optimizing evaluation metrics like F1 and F2 score deliver better performance on these metrics as compared to traditional sampling and cost-sensitive learning solutions for addressing class imbalance. Optimizing these metrics is useful in recall-oriented grammar error detection scenarios. We also show that there are inherent difficulties in optimizing precision-oriented evaluation metrics like F0.5. We establish this through a systematic evaluation on multiple datasets and different GED tasks.",
            "cx": 10901.6,
            "cy": -565.311,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-2007",
            "name": "Measuring Sentiment Annotation Complexity of Text",
            "publication_data": 2014,
            "citation": 13,
            "abstract": "The effort required for a human annotator to detect sentiment is not uniform for all texts, irrespective of his/her expertise. We aim to predict a score that quantifies this effort, using linguistic properties of the text. Our proposed metric is called Sentiment Annotation Complexity (SAC). As for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking. The sentences in our dataset are labeled with SAC scores derived from eye-fixation duration. Using linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation. We also study the correlation between a human annotatorxe2x80x99s perception of complexity and a machinexe2x80x99s confidence in polarity determination. The merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction.",
            "cx": 6044.6,
            "cy": -655.051,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P18-1219",
            "name": "Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "Predicting a reader{'}s rating of text quality is a challenging task that involves estimating different subjective aspects of the text, like structure, clarity, etc. Such subjective aspects are better handled using cognitive information. One such source of cognitive information is gaze behaviour. In this paper, we show that gaze behaviour does indeed help in effectively predicting the rating of text quality. To do this, we first we model text quality as a function of three properties - organization, coherence and cohesion. Then, we demonstrate how capturing gaze behaviour helps in predicting each of these properties, and hence the overall quality, by reporting improvements obtained by adding gaze features to traditional textual features for score prediction. We also hypothesize that if a reader has fully understood the text, the corresponding gaze behaviour would give a better indication of the assigned rating, as opposed to partial understanding. Our experiments validate this hypothesis by showing greater agreement between the given rating and the predicted rating when the reader has a full understanding of the text.",
            "cx": 6206.6,
            "cy": -296.09,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.aacl-main.86",
            "name": "Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "The gaze behaviour of a reader is helpful in solving several NLP tasks such as automatic essay grading. However, collecting gaze behaviour from readers is costly in terms of time and money. In this paper, we propose a way to improve automatic essay grading using gaze behaviour, which is learnt at run time using a multi-task learning framework. To demonstrate the efficacy of this multi-task learning based approach to automatic essay grading, we collect gaze behaviour for 48 essays across 4 essay sets, and learn gaze behaviour for the rest of the essays, numbering over 7000 essays. Using the learnt gaze behaviour, we can achieve a statistically significant improvement in performance over the state-of-the-art system for the essay sets where we have gaze data. We also achieve a statistically significant improvement for 4 other essay sets, numbering about 6000 essays, where we have no gaze behaviour data available. Our approach establishes that learning gaze behaviour improves automatic essay grading.",
            "cx": 6182.6,
            "cy": -116.61,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2149",
            "name": "Detecting Turnarounds in Sentiment Analysis: Thwarting",
            "publication_data": 2013,
            "citation": 10,
            "abstract": "Thwarting and sarcasm are two uncharted territories in sentiment analysis, the former because of the lack of training corpora and the latter because of the enormous amount of world knowledge it demands. In this paper, we propose a working definition of thwarting amenable to machine learning and create a system that detects if the document is thwarted or not. We focus on identifying thwarting in product reviews, especially in the camera domain. An ontology of the camera domain is created. Thwarting is looked upon as the phenomenon of polarity reversal at a higher level of ontology compared to the polarity expressed at the lower level. This notion of thwarting defined with respect to an ontology is novel, to the best of our knowledge. A rule based implementation building upon this idea forms our baseline. We show that machine learning with annotated corpora (thwarted/nonthwarted) is more effective than the rule based system. Because of the skewed distribution of thwarting, we adopt the Areaunder-the-Curve measure of performance. To the best of our knowledge, this is the first attempt at the difficult problem of thwarting detection, which we hope will at least provide a baseline system to compare against.",
            "cx": 6351.6,
            "cy": -744.791,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P15-2124",
            "name": "Harnessing Context Incongruity for Sarcasm Detection",
            "publication_data": 2015,
            "citation": 101,
            "abstract": "The relationship between context incongruity and sarcasm has been studied in linguistics. We present a computational system that harnesses context incongruity as a basis for sarcasm detection. Our statistical sarcasm classifiers incorporate two kinds of incongruity features: explicit and implicit. We show the benefit of our incongruity features for two text forms tweets and discussion forum posts. Our system also outperforms two past works (with Fscore improvement of 10-20%). We also show how our features can capture intersentential incongruity.",
            "cx": 6573.6,
            "cy": -565.311,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D15-1300",
            "name": "Adjective Intensity and Sentiment Analysis",
            "publication_data": 2015,
            "citation": 8,
            "abstract": "For fine-grained sentiment analysis, we need to go beyond zero-one polarity and find a way to compare adjectives that share a common semantic property. In this paper, we present a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, where adjectives are compared when they belong to the same semantic category. For example, in the semantic category of EXPERTISE, expert, experienced and familiar are respectively of level high, medium and low. We obtain an overall accuracy of 77% for intensity assignment. We show the significance of considering intensity information of adjectives in predicting star-rating of reviews. Our intensity based prediction system results in an accuracy of 59% for a 5-star rated movie review corpus.",
            "cx": 3174.6,
            "cy": -565.311,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-2623",
            "name": "A cognitive study of subjectivity extraction in sentiment annotation",
            "publication_data": 2014,
            "citation": 8,
            "abstract": "Existing sentiment analysers are weak AI systems: they try to capture the functionality of human sentiment detection faculty, without worrying about how such faculty is realized in the hardware of the human. These analysers are agnostic of the actual cognitive processes involved. This, however, does not deliver when applications demand order of magnitude facelift in accuracy, as well as insight into characteristics of sentiment detection process. In this paper, we present a cognitive study of sentiment detection from the perspective of strong AI. We study the sentiment detection process of a set of human xe2x80x9csentiment readersxe2x80x9d. Using eye-tracking, we show that on the way to sentiment detection, humans first extract subjectivity. They focus attention on a subset of sentences before arriving at the overall sentiment. This they do either through xe2x80x9danticipationxe2x80x9d where sentences are skipped during the first pass of reading, or through xe2x80x9dhomingxe2x80x9d where a subset of the sentences are read over multiple passes, or through both. xe2x80x9dHomingxe2x80x9d behaviour is also observed at the sub-sentence level in complex sentiment phenomena like sarcasm.",
            "cx": 5474.6,
            "cy": -655.051,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-1904",
            "name": "Leveraging Annotators{'} Gaze Behaviour for Coreference Resolution",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "None",
            "cx": 5879.6,
            "cy": -475.571,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-2006",
            "name": "Making Headlines in {H}indi: Automatic {E}nglish to {H}indi News Headline Translation",
            "publication_data": 2013,
            "citation": 1,
            "abstract": "News headlines exhibit stylistic peculiarities. The goal of our translation engine xe2x80x98Making Headlines in Hindixe2x80x99 is to achieve automatic translation of English news headlines to Hindi while retaining the Hindi news headline styles. There are two central modules of our engine: the modified translation unit based on Moses and a co-occurrencebased post-processing unit. The modified translation unit provides two machine translation (MT) models: phrase-based and factor-based (both using in-domain data). In addition, a co-occurrence-based post-processing option may be turned on by a user. Our evaluation shows that this engine handles some linguistic phenomena observed in Hindi news headlines.",
            "cx": 2489.6,
            "cy": -744.791,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-1076",
            "name": "Detecting Domain Dedicated Polar Words",
            "publication_data": 2013,
            "citation": 13,
            "abstract": "There are many examples in which a word changes its polarity from domain to domain. For example, unpredictable is positive in the movie domain, but negative in the product domain. Such words cannot be entered in a xe2x80x9cuniversal sentiment lexiconxe2x80x9d which is supposed to be a repository of words with polarity invariant across domains. Rather, we need to maintain separate domain specific sentiment lexicons. The main contribution of this paper is to present an effective method of generating a domain specific sentiment lexicon. For a word whose domain specific polarity needs to be determined, the approach uses the Chi-Square test to detect if the difference is significant between the counts of the word in positive and negative polarity documents. We extract 274 words that are polar in the movie domain, but are not present in the universal sentiment lexicon. Our overall accuracy is around 60% in detecting movie domain specific polar words.",
            "cx": 3053.6,
            "cy": -744.791,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D17-1058",
            "name": "Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "Identification of intensity ordering among polar (positive or negative) words which have the same semantics can lead to a fine-grained sentiment analysis. For example, {`}master{'}, {`}seasoned{'} and {`}familiar{'} point to different intensity levels, though they all convey the same meaning (semantics), i.e., expertise: having a good knowledge of. In this paper, we propose a semi-supervised technique that uses sentiment bearing word embeddings to produce a continuous ranking among adjectives that share common semantics. Our system demonstrates a strong Spearman{'}s rank correlation of 0.83 with the gold standard ranking. We show that sentiment bearing word embeddings facilitate a more accurate intensity ranking system than other standard word embeddings (word2vec and GloVe). Word2vec is the state-of-the-art for intensity ordering task.",
            "cx": 3057.6,
            "cy": -385.831,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-1089",
            "name": "Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification",
            "publication_data": 2018,
            "citation": 5,
            "abstract": "Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on \u00cf\u00872 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.",
            "cx": 2941.6,
            "cy": -296.09,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.lrec-1.613",
            "name": "Recommendation Chart of Domains for Cross-Domain Sentiment Analysis: Findings of A 20 Domain Study",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "Cross-domain sentiment analysis (CDSA) helps to address the problem of data scarcity in scenarios where labelled data for a domain (known as the target domain) is unavailable or insufficient. However, the decision to choose a domain (known as the source domain) to leverage from is, at best, intuitive. In this paper, we investigate text similarity metrics to facilitate source domain selection for CDSA. We report results on 20 domains (all possible pairs) using 11 similarity metrics. Specifically, we compare CDSA performance with these metrics for different domain-pairs to enable the selection of a suitable source domain, given a target domain. These metrics include two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The goal of our experiments is a recommendation chart that gives the K best source domains for CDSA for a given target domain. We show that the best K source domains returned by our similarity metrics have a precision of over 50{\\%}, for varying values of K.",
            "cx": 2788.6,
            "cy": -116.61,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-1131",
            "name": "Structure Cognizant Pseudo Relevance Feedback",
            "publication_data": 2013,
            "citation": 2,
            "abstract": "We propose a structure cognizant framework for pseudo relevance feedback (PRF). This has an application, for example, in selecting expansion terms for general search from subsets such as Wikipedia, wherein documents typically have a minimally fixed set of fields, viz., Title, Body, Infobox and Categories. In existing approaches to PRF based expansion, weights of expansion terms do not depend on their field(s) of origin. This, we feel, is a weakness of current PRF approaches. We propose a per field EM formulation for finding the importance of the expansion terms, in line with traditional PRF. However, the final weight of an expansion term is found by weighting these importance based on whether the term belongs to the title, the body, the infobox or the category field(s). In our experiments with four languages, viz., English, Spanish, Finnish and Hindi, we find that this structure-aware PRF yields a 2% to 30% improvement in performance (MAP) over the vanilla PRF. We conduct ablation tests to evaluate the importance of various fields. As expected, results from these tests emphasize the importance of fields in the order of title, body, categories and infobox.",
            "cx": 3986.6,
            "cy": -744.791,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 208,
            "name": 208,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5115",
            "name": "Introduction to Synskarta: An Online Interface for Synset Creation with Special Reference to {S}anskrit",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "None",
            "cx": 8424.6,
            "cy": -655.051,
            "rx": 117.26,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.46",
            "name": "Sam{\\\\=a}sa-Kart{\\\\=a}: An Online Tool for Producing Compound Words using {I}ndo{W}ord{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "Sam{\\=a}sa or compounds are a regular feature of Indian Languages. They are also found in other languages like German, Italian, French, Russian, Spanish, etc. Compound word is constructed from two or more words to form a single word. The meaning of this word is derived from each of the individual words of the compound. To develop a system to generate, identify and interpret compounds, is an important task in Natural Language Processing. This paper introduces a web based tool - Sam{\\=a}sa-Kart{\\=a} for producing compound words. Here, the focus is on Sanskrit language due to its richness in usage of compounds; however, this approach can be applied to any Indian language as well as other languages. IndoWordNet is used as a resource for words to be compounded. The motivation behind creating compound words is to create, to improve the vocabulary, to reduce sense ambiguity, etc. in order to enrich the WordNet. The Sam{\\=a}sa-Kart{\\=a} can be used for various applications viz., compound categorization, sandhi creation, morphological analysis, paraphrasing, synset creation, etc.",
            "cx": 8245.6,
            "cy": -475.571,
            "rx": 161.441,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W14-5126",
            "name": "{P}a{CM}an : Parallel Corpus Management Workbench",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "We present a Parallel Corpora Management tool that aides parallel corpora generation for the task of Machine Translation (MT). It takes source and target text of a corpus for any language pair in text file format, or zip archives containing multiple corresponding text files. Then, it provides with a helpful interface to lexicographers for manual translation / validation, and gives out the corrected text files as output. It provides various dictionary references as help within the interface which increase the productivity and efficiency of a lexicographer. It also provides automatic translation of the source sentence using an integrated MT system. The tool interface includes a corpora management system which facilitates maintenance of parallel corpora by assigning roles such as manager, lexicographer etc. We have designed a novel tool that provides aides like references to various dictionary sources such as Wordnets, Shabdkosh, Wikitionary etc. We also provide manual word alignment correction which is visualized in the tool and can lead to its gamification in the future, thus, providing a valuable source of word / phrase alignments.",
            "cx": 859.597,
            "cy": -655.051,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5916",
            "name": "{T}rans{C}hat: Cross-Lingual Instant Messaging for {I}ndian Languages",
            "publication_data": 2015,
            "citation": 1,
            "abstract": "None",
            "cx": 913.597,
            "cy": -565.311,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-0130",
            "name": "Facilitating Multi-Lingual Sense Annotation: Human Mediated Lemmatizer",
            "publication_data": 2014,
            "citation": 9,
            "abstract": "Sense marked corpora is essential for supervised word sense disambiguation (WSD). The marked sense ids come from wordnets. However, words in corpora appear in morphed forms, while wordnets store lemma. This situation calls for accurate lemmatizers. The lemma is the gateway to the wordnet. However, the problem is that for many languages, lemmatizers do not exist, and this problem is not easy to solve, since rule based lemmatizers take time and require highly skilled linguists.Satistical stemmers on the other hand do not return legitimate lemma. We present here a novel scheme for creating accurate lemmatizers quickly. These lemmatizers are human mediated. The key idea is that a trie is created out of the vocabulary of the language. The lemmatizing process consists in navigating the trie, trying to find a match between the input word and an entry in the trie. At the point of first mismatch, the yield of the subtree rooted at the partially matched node is output as the list of possible lemma. If the correct lemma does not appear in the listas noted by a human lexicographerbacktracking is initiated. This can output more possibilities. A ranking function filters and orders the output list of lemma. We have evaluated the performance of this human mediated lemmatizer for eighteen Indian Languages and five European languages. We have compared accuracy figures against well known lemmatizers/stemmers like Morpha, Morfessor and Snowball stemmers, and observed superior performance in all cases. Our work shows a way of speedily creating human assisted accurate lemmatizers, thereby removing a difficult roadblock in many NLP tasks, e.g., sense annotation.",
            "cx": 8726.6,
            "cy": -655.051,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5910",
            "name": "{I}ndo{W}ord{N}et Dictionary: An Online Multilingual Dictionary using {I}ndo{W}ord{N}et",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "None",
            "cx": 9141.6,
            "cy": -565.311,
            "rx": 152.97,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.22",
            "name": "Sophisticated Lexical Databases - Simplified Usage: Mobile Applications and Browser Plugins For Wordnets",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "India is a country with 22 officially recognized languages and 17 of these have WordNets, a crucial resource. Web browser based interfaces are available for these WordNets, but are not suited for mobile devices which deters people from effectively using this resource. We present our initial work on developing mobile applications and browser extensions to access WordNets for Indian Languages. Our contribution is two fold: (1) We develop mobile applications for the Android, iOS and Windows Phone OS platforms for Hindi, Marathi and Sanskrit WordNets which allow users to search for words and obtain more information along with their translations in English and other Indian languages. (2) We also develop browser extensions for English, Hindi, Marathi, and Sanskrit WordNets, for both Mozilla Firefox, and Google Chrome. We believe that such applications can be quite helpful in a classroom scenario, where students would be able to access the WordNets as dictionaries as well as lexical knowledge bases. This can help in overcoming the language barrier along with furthering language understanding.",
            "cx": 8726.6,
            "cy": -475.571,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.naacl-main.322",
            "name": "How low is too low? A monolingual take on lemmatisation in {I}ndian languages",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Most prior work on ML based lemmatization has focused on high resource languages, where data sets (word forms) are readily available. For languages which have no linguistic work available, especially on morphology or in languages where the computational realization of linguistic rules is complex and cumbersome, machine learning based lemmatizers are the way togo. In this paper, we devote our attention to lemmatisation for low resource, morphologically rich scheduled Indian languages using neural methods. Here, low resource means only a small number of word forms are available. We perform tests to analyse the variance in monolingual models{'} performance on varying the corpus size and contextual morphological tag data for training. We show that monolingual approaches with data augmentation can give competitive accuracy even in the low resource setting, which augurs well for NLP in low resource setting.",
            "cx": 9322.6,
            "cy": -26.8701,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W14-0145",
            "name": "Semi-Automatic Extension of {S}anskrit {W}ordnet using Bilingual Dictionary",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "In this paper, we report our methods and results of using, for the first time, semi-automatic approach to enhance an Indian language Wordnet. We apply our methods to enhancing an already existing Sanskrit Wordnet created from Hindi Wordnet (which is created from Princeton Wordnet) using expansion approach. We base our experiment on an existing bilingual Sanskrit English Dictionary and show how lemma in this dictionary can be mapped to Princeton Wordnet through which corresponding Sanskrit synsets can be populated by Sanskrit lexemes. This our method will also show how absence of resources of a pair of languages need not be an obstacle, if another resource of one of them is available. Sanskrit being historically related to languages of Indo-European family, we believe that this semi-automatic approach will help enhance Wordnets of other Indian languages of the same family.",
            "cx": 8056.6,
            "cy": -655.051,
            "rx": 118.174,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-0147",
            "name": "{I}ndo{W}ordnet Visualizer: A Graphical User Interface for Browsing and Exploring Wordnets of {I}ndian Languages",
            "publication_data": 2014,
            "citation": 3,
            "abstract": "In this paper, we are presenting a graphical user interface to browse and explore the IndoWordnet lexical database for various Indian languages. IndoWordnet visualizer extracts the related concepts for a given word and displays a sub graph containing those concepts. The interface is enhanced with different features in order to provide flexibility to the user. IndoWordnet visualizer is made publically available. Though it was initially constructed for making the wordnet validation process easier, it is proving to be very useful in analyzing various Natural Language Processing tasks, viz., Semantic relatedness, Word Sense Disambiguation, Information Retrieval, Textual Entailment, etc.",
            "cx": 11123.6,
            "cy": -655.051,
            "rx": 139.1,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-7512",
            "name": "An Introduction to the Textual History Tool",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "None",
            "cx": 11123.6,
            "cy": -206.35,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N15-3017",
            "name": "Brahmi-Net: A transliteration and script conversion system for languages of the {I}ndian subcontinent",
            "publication_data": 2015,
            "citation": 11,
            "abstract": "We present Brahmi-Net - an online system for transliteration and script conversion for all major Indian language pairs (306 pairs). The system covers 13 Indo-Aryan languages, 4 Dravidian languages and English. For training the transliteration systems, we mined parallel transliteration corpora from parallel translation corpora using an unsupervised method and trained statistical transliteration systems using the mined corpora. Languages which do not have parallel corpora are supported by transliteration through a bridge language. Our script conversion system supports conversion between all Brahmi-derived scripts as well as ITRANS romanization scheme. For this, we leverage co-ordinated Unicode ranges between Indic scripts and use an extended ITRANS encoding for transliterating between English and Indic scripts. The system also provides top-k transliterations and simultaneous transliteration into multiple output languages. We provide a Python as well as REST API to access these services. The API and the mined transliteration corpus are made available for research use under an open source license.",
            "cx": 632.597,
            "cy": -565.311,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L16-1098",
            "name": "Lexical Resources to Enrich {E}nglish {M}alayalam Machine Translation",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "In this paper we present our work on the usage of lexical resources for the Machine Translation English and Malayalam. We describe a comparative performance between different Statistical Machine Translation (SMT) systems on top of phrase based SMT system as baseline. We explore different ways of utilizing lexical resources to improve the quality of English Malayalam statistical machine translation. In order to enrich the training corpus we have augmented the lexical resources in two ways (a) additional vocabulary and (b) inflected verbal forms. Lexical resources include IndoWordnet semantic relation set, lexical words and verb phrases etc. We have described case studies, evaluations and have given detailed error analysis for both Malayalam to English and English to Malayalam machine translation systems. We observed significant improvement in evaluations of translation quality. Lexical resources do help uplift performance when parallel corpora are scanty.",
            "cx": 263.597,
            "cy": -475.571,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W18-1207",
            "name": "Meaningless yet meaningful: Morphology grounded subword-level {NMT}",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "We explore the use of two independent subsystems Byte Pair Encoding (BPE) and Morfessor as basic units for subword-level neural machine translation (NMT). We show that, for linguistically distant language-pairs Morfessor-based segmentation algorithm produces significantly better quality translation than BPE. However, for close language-pairs BPE-based subword-NMT may translate better than Morfessor-based subword-NMT. We propose a combined approach of these two segmentation algorithms Morfessor-BPE (M-BPE) which outperforms these two baseline systems in terms of BLEU score. Our results are supported by experiments on three language-pairs: English-Hindi, Bengali-Hindi and English-Bengali.",
            "cx": 516.597,
            "cy": -296.09,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1413",
            "name": "Morphology Injection for {E}nglish-{M}alayalam Statistical Machine Translation",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "None",
            "cx": 191.597,
            "cy": -296.09,
            "rx": 115.931,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 172,
            "name": 172,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5944",
            "name": "Augmenting Pivot based {SMT} with word segmentation",
            "publication_data": 2015,
            "citation": 2,
            "abstract": "None",
            "cx": 1610.6,
            "cy": -565.311,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5946",
            "name": "Triangulation of Reordering Tables: An Advancement Over Phrase Table Triangulation in Pivot-Based {SMT}",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Triangulation in Pivot-Based Statistical Machine Translation(SMT) is a very effective method for building Machine Translation(MT) systems in case of scarcity of the parallel corpus. Phrase Table Triangulation helps in such a resource constrained setting by inducing new phrase pairs with the help of a pivot. However, it does not explore the possibility of extracting reordering information through the use of pivot. This paper presents a novel method for triangulation of reordering tables in Pivot Based SMT. We show that the use of a pivot can help in extracting better reordering information and can assist in improving the quality of the translation. With a detailed example, we show that triangulation of reordering tables also improves the lexical choices a system makes during translation. We observe a BLEU score improvement of 1.06 for Marathi to English MT system with Hindi as a pivot, and also significant improvements in 8 other translation systems by using this method.",
            "cx": 1386.6,
            "cy": -565.311,
            "rx": 110.118,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-3912",
            "name": "Data representation methods and use of mined corpora for {I}ndian language transliteration",
            "publication_data": 2015,
            "citation": 7,
            "abstract": "Our NEWS 2015 shared task submission is a PBSMT based transliteration system with the following corpus preprocessing enhancements: (i) addition of wordboundary markers, and (ii) languageindependent, overlapping character segmentation. We show that the addition of word-boundary markers improves transliteration accuracy substantially, whereas our overlapping segmentation shows promise in our preliminary analysis. We also compare transliteration systems trained using manually created corpora with the ones mined from parallel translation corpus for English to Indian language pairs. We identify the major errors in English to Indian language transliterations by analyzing heat maps of confusion matrices.",
            "cx": 1836.6,
            "cy": -565.311,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-2905",
            "name": "Your Sentiment Precedes You: Using an author{'}s historical tweets to predict sarcasm",
            "publication_data": 2015,
            "citation": 41,
            "abstract": "Sarcasm understanding may require information beyond the text itself, as in the case of xe2x80x98I absolutely love this restaurant!xe2x80x99 which may be sarcastic, depending on the contextual situation. We present the first quantitative evidence to show that historical tweets by an author can provide additional context for sarcasm detection. Our sarcasm detection approach uses two components: a contrast-based predictor (that identifies if there is a sentiment contrast within a target tweet), and a historical tweet-based predictor (that identifies if the sentiment expressed towards an entity in the target tweet agrees with sentiment expressed by the author towards that entity in the past).",
            "cx": 7237.6,
            "cy": -565.311,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W16-5001",
            "name": "{`}Who would have thought of that!{'}: A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports the first topic model for sarcasm detection, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as {`}work{'}, {`}gun laws{'}, {`}weather{'} are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25{\\%}.",
            "cx": 7439.6,
            "cy": -475.571,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "K16-1015",
            "name": "Harnessing Sequence Labeling for Sarcasm Detection in Dialogue from {TV} Series {`}{F}riends{'}",
            "publication_data": 2016,
            "citation": 18,
            "abstract": "This paper is a novel study that views sarcasm detection in dialogue as a sequence labeling task, where a dialogue is made up of a sequence of utterances. We create a manuallylabeled dataset of dialogue from TV series xe2x80x98Friendsxe2x80x99 annotated with sarcasm. Our goal is to predict sarcasm in each utterance, using sequential nature of a scene. We show performance gain using sequence labeling as compared to classification-based approaches. Our experiments are based on three sets of features, one is derived from information in our dataset, the other two are from past works. Two sequence labeling algorithms (SVM-HMM and SEARN) outperform three classification algorithms (SVM, Naive Bayes) for all these feature sets, with an increase in F-score of around 4%. Our observations highlight the viability of sequence labeling techniques for sarcasm detection of dialogue.",
            "cx": 7237.6,
            "cy": -475.571,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D16-1104",
            "name": "Are Word Embedding-based Features Useful for Sarcasm Detection?",
            "publication_data": 2016,
            "citation": 27,
            "abstract": "None",
            "cx": 6621.6,
            "cy": -475.571,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L18-1424",
            "name": "Sarcasm Target Identification: Dataset and An Introductory Approach",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "None",
            "cx": 7111.6,
            "cy": -296.09,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-2111",
            "name": "How Do Cultural Differences Impact the Quality of Sarcasm Annotation?: A Case Study of {I}ndian Annotators and {A}merican Text",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "None",
            "cx": 6422.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "U16-1013",
            "name": "How Challenging is Sarcasm versus Irony Classification?: A Study With a Dataset from {E}nglish Literature",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "None",
            "cx": 6781.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P16-1104",
            "name": "Harnessing Cognitive Features for Sarcasm Detection",
            "publication_data": 2016,
            "citation": 13,
            "abstract": "In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by this observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of Fscore), over the performance of the best reported system.",
            "cx": 6080.6,
            "cy": -475.571,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W17-7518",
            "name": "Is your Statement Purposeless? Predicting Computer Science Graduation Admission Acceptance based on Statement Of Purpose",
            "publication_data": 2017,
            "citation": 0,
            "abstract": "None",
            "cx": 6413.6,
            "cy": -385.831,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-1309",
            "name": "{``}When Numbers Matter!{''}: Detecting Sarcasm in Numerical Portions of Text",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "Research in sarcasm detection spans almost a decade. However a particular form of sarcasm remains unexplored: sarcasm expressed through numbers, which we estimate, forms about 11{\\%} of the sarcastic tweets in our dataset. The sentence {`}Love waking up at 3 am{'} is sarcastic because of the number. In this paper, we focus on detecting sarcasm in tweets arising out of numbers. Initially, to get an insight into the problem, we implement a rule-based and a statistical machine learning-based (ML) classifier. The rule-based classifier conveys the crux of the numerical sarcasm problem, namely, incongruity arising out of numbers. The statistical ML classifier uncovers the indicators i.e., features of such sarcasm. The actual system in place, however, are two deep learning (DL) models, CNN and attention network that obtains an F-score of 0.93 and 0.91 on our dataset of tweets containing numbers. To the best of our knowledge, this is the first line of research investigating the phenomenon of sarcasm arising out of numbers, culminating in a detector thereof.",
            "cx": 6894.6,
            "cy": -206.35,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.aacl-main.31",
            "name": "All-in-One: A Deep Attentive Multi-task Learning Framework for Humour, Sarcasm, Offensive, Motivation, and Sentiment on Memes",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and sentiment analysis on a somewhat complicated form of information, i.e., memes. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For multi-tasking, we propose two attention-like mechanisms viz., Inter-task Relationship Module (iTRM) and Inter-class Relationship Module (iCRM). The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, iCRM develops relations between the different classes of tasks. Finally, representations from both the attentions are concatenated and shared across the five tasks (i.e., humour, sarcasm, offensive, motivational, and sentiment) for multi-tasking. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of memes annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-the-art systems (Baseline and SemEval 2020 winner). The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.",
            "cx": 5965.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D16-1196",
            "name": "Orthographic Syllable as basic unit for {SMT} between Related Languages",
            "publication_data": 2016,
            "citation": 8,
            "abstract": "None",
            "cx": 858.597,
            "cy": -475.571,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-4102",
            "name": "Learning variable length units for {SMT} between related languages via Byte Pair Encoding",
            "publication_data": 2017,
            "citation": 7,
            "abstract": "We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11{\\%} increase in BLEU score. While orthographic syllables can be used only for languages whose writing systems use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and writing systems.",
            "cx": 560.597,
            "cy": -385.831,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-2064",
            "name": "Judicious Selection of Training Data in Assisting Language for Multilingual Neural {NER}",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages. Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages. We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning. To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data.",
            "cx": 858.597,
            "cy": -296.09,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.emnlp-main.675",
            "name": "Role of {L}anguage {R}elatedness in {M}ultilingual {F}ine-tuning of {L}anguage {M}odels: {A} {C}ase {S}tudy in {I}ndo-{A}ryan {L}anguages",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications, compared to models fine-tuned on individual languages. A first of its kind detailed study is presented to track performance change as languages are added to a base language in a graded and greedy (in the sense of best boost of performance) manner; which reveals that careful selection of subset of related languages can significantly improve performance than utilizing all related languages. The Indo-Aryan (IA) language family is chosen for the study, the exact languages being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script barrier is crossed by simple rule-based transliteration of the text of all languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource languages, such as Oriya and Punjabi, are found to be the largest beneficiaries of multilingual fine-tuning. Textual Entailment, Entity Classification, Section Title Prediction, tasks of IndicGLUE and POS tagging form our test bed. Compared to monolingual fine tuning we get relative performance improvement of up to 150{\\%} in the downstream tasks. The surprise take-away is that for any language there is a particular combination of other languages which yields the best performance, and any additional language is in fact detrimental.",
            "cx": 631.597,
            "cy": -26.8701,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N15-1132",
            "name": "Unsupervised Most Frequent Sense Detection using Word Embeddings",
            "publication_data": 2015,
            "citation": 20,
            "abstract": "An acid test for any new Word Sense Disambiguation (WSD) algorithm is its performance against the Most Frequent Sense (MFS). The field of WSD has found the MFS baseline very hard to beat. Clearly, if WSD researchers had access to MFS values, their striving to better this heuristic will push the WSD frontier. However, getting MFS values requires sense annotated corpus in enormous amounts, which is out of bounds for most languages, even if their WordNets are available. In this paper, we propose an unsupervised method for MFS detection from the untagged corpora, which exploits word embeddings. We compare the word embedding of a word with all its sense embeddings and obtain the predominant sense with the highest similarity. We observe significant performance gain for Hindi WSD over the WordNet First Sense (WFS) baseline. As for English, the SemCor baseline is bettered for those words whose frequency is greater than 2. Our approach is language and domain independent.",
            "cx": 8862.6,
            "cy": -565.311,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L18-1049",
            "name": "Sentence Level Temporality Detection using an Implicit Time-sensed Resource",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "None",
            "cx": 8867.6,
            "cy": -296.09,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018.gwc-1.34",
            "name": "An Iterative Approach for Unsupervised Most Frequent Sense Detection using {W}ord{N}et and Word Embeddings",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "Given a word, what is the most frequent sense in which it occurs in a given corpus? Most Frequent Sense (MFS) is a strong baseline for unsupervised word sense disambiguation. If we have large amounts of sense-annotated corpora, MFS can be trivially created. However, sense-annotated corpora are a rarity. In this paper, we propose a method which can compute MFS from raw corpora. Our approach iteratively exploits the semantic congruity among related words in corpus. Our method performs better compared to another similar work.",
            "cx": 9058.6,
            "cy": -296.09,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W18-3705",
            "name": "Thank {``}Goodness{''}! A Way to Measure Style in Student Essays",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "Essays have two major components for scoring - content and style. In this paper, we describe a property of the essay, called goodness, and use it to predict the score given for the style of student essays. We compare our approach to solve this problem with baseline approaches, like language modeling and also a state-of-the-art deep learning system. We show that, despite being quite intuitive, our approach is very powerful in predicting the style of the essays.",
            "cx": 3329.6,
            "cy": -296.09,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 53,
            "name": 53,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6325",
            "name": "A Recurrent Neural Network Architecture for De-identifying Clinical Records",
            "publication_data": 2016,
            "citation": 7,
            "abstract": "None",
            "cx": 11391.6,
            "cy": -475.571,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6331",
            "name": "Opinion Mining in a Code-Mixed Environment: A Case Study with Government Portals",
            "publication_data": 2016,
            "citation": 2,
            "abstract": "None",
            "cx": 4115.6,
            "cy": -475.571,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "K18-1012",
            "name": "Uncovering Code-Mixed Challenges: A Framework for Linguistically Driven Question Generation and Neural Based Question Answering",
            "publication_data": 2018,
            "citation": 4,
            "abstract": "Existing research on question answering (QA) and comprehension reading (RC) are mainly focused on the resource-rich language like English. In recent times, the rapid growth of multi-lingual web content has posed several challenges to the existing QA systems. Code-mixing is one such challenge that makes the task more complex. In this paper, we propose a linguistically motivated technique for code-mixed question generation (CMQG) and a neural network based architecture for code-mixed question answering (CMQA). For evaluation, we manually create the code-mixed questions for Hindi-English language pair. In order to show the effectiveness of our neural network based CMQA technique, we utilize two benchmark datasets, SQuAD and MMQA. Experiments show that our proposed model achieves encouraging performance on CMQG and CMQA.",
            "cx": 4118.6,
            "cy": -296.09,
            "rx": 104.804,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.findings-emnlp.206",
            "name": "A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Code-mixing, the interleaving of two or more languages within a sentence or discourse is ubiquitous in multilingual societies. The lack of code-mixed training data is one of the major concerns for the development of end-to-end neural network-based models to be deployed for a variety of natural language processing (NLP) applications. A potential solution is to either manually create or crowd-source the code-mixed labelled data for the task at hand, but that requires much human efforts and often not feasible because of the language specific diversity in the code-mixed text. To circumvent the data scarcity issue, we propose an effective deep learning approach for automatically generating the code-mixed text from English to multiple languages without any parallel data. In order to train the neural network, we create synthetic code-mixed texts from the available parallel corpus by modelling various linguistic properties of code-mixing. Our codemixed text generator is built upon the encoder-decoder framework, where the encoder is augmented with the linguistic and task-agnostic features obtained from the transformer based language model. We also transfer the knowledge from a neural machine translation (NMT) to warm-start the training of code-mixed generator. Experimental results and in-depth analysis show the effectiveness of our proposed code-mixed text generation on eight diverse language pairs.",
            "cx": 3492.6,
            "cy": -116.61,
            "rx": 65.5227,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W16-6336",
            "name": "On Why Coarse Class Classification is Bottleneck in Noun Compound Interpretation",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 7669.6,
            "cy": -475.571,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1489",
            "name": "Towards a Standardized Dataset for Noun Compound Interpretation",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "None",
            "cx": 7381.6,
            "cy": -296.09,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-4811",
            "name": "Faster Decoding for Subword Level Phrase-based {SMT} between Related Languages",
            "publication_data": 2016,
            "citation": 1,
            "abstract": "A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.",
            "cx": 1064.6,
            "cy": -475.571,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-4206",
            "name": "Deep Learning Architecture for Patient Data De-identification in Clinical Records",
            "publication_data": 2016,
            "citation": 14,
            "abstract": "Rapid growth in Electronic Medical Records (EMR) has emerged to an expansion of data in the clinical domain. The majority of the available health care information is sealed in the form of narrative documents which form the rich source of clinical information. Text mining of such clinical records has gained huge attention in various medical applications like treatment and decision making. However, medical records enclose patient Private Health Information (PHI) which can reveal the identities of the patients. In order to retain the privacy of patients, it is mandatory to remove all the PHI information prior to making it publicly available. The aim is to de-identify or encrypt the PHI from the patient medical records. In this paper, we propose an algorithm based on deep learning architecture to solve this problem. We perform de-identification of seven PHI terms from the clinical records. Experiments on benchmark datasets show that our proposed approach achieves encouraging performance, which is better than the baseline model developed with Conditional Random Field.",
            "cx": 11229.6,
            "cy": -475.571,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W16-0415",
            "name": "Political Issue Extraction Model: A Novel Hierarchical Topic Model That Uses Tweets By Political And Non-Political Authors",
            "publication_data": 2016,
            "citation": 9,
            "abstract": "People often use social media to discuss opinions, including political ones. We refer to relevant topics in these discussions as political issues, and the alternate stands towards these topics as political positions. We present a Political Issue Extraction (PIE) model that is capable of discovering political issues and positions from an unlabeled dataset of tweets. A strength of this model is that it uses twitter timelines of political and non-political authors, and affiliation information of only political authors. The model estimates word-specific distributions (that denote political issues and positions) and hierarchical author/group-specific distributions (that show how these issues divide people). Our experiments using a dataset of 2.4 million tweets from the US show that this model effectively captures the desired properties (with respect to words and groups) of political discussions. We also evaluate the two components of the model by experimenting with: (a) Use to alternate strategies to classify words, and (b) Value addition due to incorporation of group membership information. Estimated distributions are then used to predict political affiliation with 68% accuracy.",
            "cx": 6997.6,
            "cy": -475.571,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N16-4006",
            "name": "Statistical Machine Translation between Related Languages",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "None",
            "cx": 554.597,
            "cy": -475.571,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-2338",
            "name": "Adapting Pre-trained Word Embeddings For Use In Medical Coding",
            "publication_data": 2017,
            "citation": 5,
            "abstract": "Word embeddings are a crucial component in modern NLP. Pre-trained embeddings released by different groups have been a major reason for their popularity. However, they are trained on generic corpora, which limits their direct use for domain specific tasks. In this paper, we propose a method to add task specific information to pre-trained word embeddings. Such information can improve their utility. We add information from medical coding data, as well as the first level from the hierarchy of ICD-10 medical code set to different pre-trained word embeddings. We adapt CBOW algorithm from the word2vec package for our purpose. We evaluated our approach on five different pre-trained word embeddings. Both the original word embeddings, and their modified versions (the ones with added information) were used for automated review of medical coding. The modified word embeddings give an improvement in f-score by 1{\\%} on the 5-fold evaluation on a private medical claims dataset. Our results show that adding extra information is possible and beneficial for the task at hand.",
            "cx": 6586.6,
            "cy": -385.831,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I17-2006",
            "name": "Towards Lower Bounds on Number of Dimensions for Word Embeddings",
            "publication_data": 2017,
            "citation": 1,
            "abstract": "Word embeddings are a relatively new addition to the modern NLP researcher{'}s toolkit. However, unlike other tools, word embeddings are used in a black box manner. There are very few studies regarding various hyperparameters. One such hyperparameter is the dimension of word embeddings. They are rather decided based on a rule of thumb: in the range 50 to 300. In this paper, we show that the dimension should instead be chosen based on corpus statistics. More specifically, we show that the number of pairwise equidistant words of the corpus vocabulary (as defined by some distance/similarity metric) gives a lower bound on the the number of dimensions , and going below this bound results in degradation of quality of learned word embeddings. Through our evaluations on standard word embedding evaluation tasks, we show that for dimensions higher than or equal to the bound, we get better results as compared to the ones below it.",
            "cx": 6779.6,
            "cy": -385.831,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C18-1155",
            "name": "Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds using {LSTM}",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "Interpreting noun compounds is a challenging task. It involves uncovering the underlying predicate which is dropped in the formation of the compound. In most cases, this predicate is of the form VERB+PREP. It has been observed that uncovering the preposition is a significant step towards uncovering the predicate. In this paper, we attempt to paraphrase noun compounds using prepositions. We consider noun compounds and their corresponding prepositional paraphrases as parallelly aligned sequences of words. This enables us to adapt different architectures from cross-lingual embedding literature. We choose the architecture where we create representations of both noun compound (source sequence) and its corresponding prepositional paraphrase (target sequence), such that their sim- ilarity is high. We use LSTMs to learn these representations. We use these representations to decide the correct preposition. Our experiments show that this approach performs considerably well on different datasets of noun compounds that are manually annotated with prepositions.",
            "cx": 7608.6,
            "cy": -296.09,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-5229",
            "name": "{IITP} at {E}mo{I}nt-2017: Measuring Intensity of Emotions using Sentence Embeddings and Optimized Features",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "This paper describes the system that we submitted as part of our participation in the shared task on Emotion Intensity (EmoInt-2017). We propose a Long short term memory (LSTM) based architecture cascaded with Support Vector Regressor (SVR) for intensity prediction. We also employ Particle Swarm Optimization (PSO) based feature selection algorithm for obtaining an optimized feature set for training and evaluation. System evaluation shows interesting results on the four emotion datasets i.e. anger, fear, joy and sadness. In comparison to the other participating teams our system was ranked 5th in the competition.",
            "cx": 5221.6,
            "cy": -385.831,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1057",
            "name": "A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis",
            "publication_data": 2017,
            "citation": 11,
            "abstract": "In this paper, we propose a novel method for combining deep learning and classical feature based models using a Multi-Layer Perceptron (MLP) network for financial sentiment analysis. We develop various deep learning models based on Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). These are trained on top of pre-trained, autoencoder-based, financial word embeddings and lexicon features. An ensemble is constructed by combining these deep learning models and a classical supervised model based on Support Vector Regression (SVR). We evaluate our proposed technique on a benchmark dataset of SemEval-2017 shared task on financial sentiment analysis. The propose model shows impressive results on two datasets, i.e. microblogs and news headlines datasets. Comparisons show that our proposed model performs better than the existing state-of-the-art systems for the above two datasets by 2.0 and 4.1 cosine points, respectively.",
            "cx": 4750.6,
            "cy": -385.831,
            "rx": 50.41,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2020.lrec-1.621",
            "name": "Multi-domain Tweet Corpora for Sentiment Analysis: Resource Creation and Evaluation",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Due to the phenomenal growth of online content in recent time, sentiment analysis has attracted attention of the researchers and developers. A number of benchmark annotated corpora are available for domains like movie reviews, product reviews, hotel reviews, etc.The pervasiveness of social media has also lead to a huge amount of content posted by users who are misusing the power of social media to spread false beliefs and to negatively influence others. This type of content is coming from the domains like terrorism, cybersecurity, technology, social issues, etc. Mining of opinions from these domains is important to create a socially intelligent system to provide security to the public and to maintain the law and order situations. To the best of our knowledge, there is no publicly available tweet corpora for such pervasive domains. Hence, we firstly create a multi-domain tweet sentiment corpora and then establish a deep neural network based baseline framework to address the above mentioned issues. Annotated corpus has Cohen{'}s Kappa measurement for annotation quality of 0.770, which shows that the data is of acceptable quality. We are able to achieve 84.65{\\%} accuracy for sentiment analysis by using an ensemble of Convolutional Neural Network (CNN), Long Short Term Memory (LSTM), and Gated Recurrent Unit(GRU).",
            "cx": 5171.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.lrec-1.201",
            "name": "{CEASE}, a Corpus of Emotion Annotated Suicide notes in {E}nglish",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "A suicide note is usually written shortly before the suicide and it provides a chance to comprehend the self-destructive state of mind of the deceased. From a psychological point of view, suicide notes have been utilized for recognizing the motive behind the suicide. To the best of our knowledge, there is no openly accessible suicide note corpus at present, making it challenging for the researchers and developers to deep dive into the area of mental health assessment and suicide prevention. In this paper, we create a fine-grained emotion annotated corpus (CEASE) of suicide notes in English and develop various deep learning models to perform emotion detection on the curated dataset. The corpus consists of 2393 sentences from around 205 suicide notes collected from various sources. Each sentence is annotated with a particular emotion class from a set of 15 fine-grained emotion labels, namely (forgiveness, happiness{\\_}peacefulness, love, pride, hopefulness, thankfulness, blame, anger, fear, abuse, sorrow, hopelessness, guilt, information, instructions). For the evaluation, we develop an ensemble architecture, where the base models correspond to three supervised deep learning models, namely Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU) and Long Short Term Memory (LSTM). We obtain the highest test accuracy of 60.17{\\%} and cross-validation accuracy of 60.32{\\%}",
            "cx": 4601.6,
            "cy": -116.61,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.icon-main.62",
            "name": "Annotated Corpus of Tweets in {E}nglish from Various Domains for Emotion Detection",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Emotion recognition is a very well-attended problem in Natural Language Processing (NLP). Most of the existing works on emotion recognition focus on the general domain and in some cases to specific domains like fairy tales, blogs, weather, Twitter etc. But emotion analysis systems in the domains of security, social issues, technology, politics, sports, etc. are very rare. In this paper, we create a benchmark setup for emotion recognition in these specialised domains. First, we construct a corpus of 18,921 tweets in English annotated with Paul Ekman{'}s six basic emotions (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and a non-emotive class Others. Thereafter, we propose a deep neural framework to perform emotion recognition in an end-to-end setting. We build various models based on Convolutional Neural Network (CNN), Bi-directional Long Short Term Memory (Bi-LSTM), Bi-directional Gated Recurrent Unit (Bi-GRU). We propose a Hierarchical Attention-based deep neural network for Emotion Detection (HAtED). We also develop multiple systems by considering different sets of emotion classes for each system and report the detailed comparative analysis of the results. Experiments show the hierarchical attention-based model achieves best results among the considered baselines with accuracy of 69{\\%}.",
            "cx": 4833.6,
            "cy": -116.61,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2016.gwc-1.23",
            "name": "A picture is worth a thousand words: Using {O}pen{C}lip{A}rt library for enriching {I}ndo{W}ord{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "WordNet has proved to be immensely useful for Word Sense Disambiguation, and thence Machine translation, Information Retrieval and Question Answering. It can also be used as a dictionary for educational purposes. The semantic nature of concepts in a WordNet motivates one to try to express this meaning in a more visual way. In this paper, we describe our work of enriching IndoWordNet with image acquisitions from the OpenClipArt library. We describe an approach used to enrich WordNets for eighteen Indian languages. Our contribution is three fold: (1) We develop a system, which, given a synset in English, finds an appropriate image for the synset. The system uses the OpenclipArt library (OCAL) to retrieve images and ranks them. (2) After retrieving the images, we map the results along with the linkages between Princeton WordNet and Hindi WordNet, to link several synsets to corresponding images. We choose and sort top three images based on our ranking heuristic per synset. (3) We develop a tool that allows a lexicographer to manually evaluate these images. The top images are shown to a lexicographer by the evaluation tool for the task of choosing the best image representation. The lexicographer also selects the number of relevant images. Using our system, we obtain an Average Precision (P @ 3) score of 0.30.",
            "cx": 8491.6,
            "cy": -475.571,
            "rx": 66.4361,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018.gwc-1.37",
            "name": "{H}indi {W}ordnet for Language Teaching: Experiences and Lessons Learnt",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "This paper reports the work related to making Hindi Wordnet1 available as a digital resource for language learning and teaching, and the experiences and lessons that were learnt during the process. The language data of the Hindi Wordnet has been suitably modified and enhanced to make it into a language learning aid. This aid is based on modern pedagogical axioms and is aligned to the learning objectives of the syllabi of the school education in India. To make it into a comprehensive language tool, grammatical information has also been encoded, as far as these can be marked on the lexical items. The delivery of information is multi-layered, multi-sensory and is available across multiple digital platforms. The front end has been designed to offer an eye-catching user-friendly interface which is suitable for learners starting from age six onward. Preliminary testing of the tool has been done and it has been modified as per the feedbacks that were received. Above all, the entire exercise has offered gainful insights into learning based on associative networks and how knowledge based on such networks can be made available to modern learners.",
            "cx": 8222.6,
            "cy": -296.09,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018.gwc-1.49",
            "name": "Synthesizing Audio for {H}indi {W}ord{N}et",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "In this paper, we describe our work on the creation of a voice model using a speech synthesis system for the Hindi Language. We use pre-existing {``}voices{''}, use publicly available speech corpora to create a {``}voice{''} using the Festival Speech Synthesis System (Black, 1997). Our contribution is two-fold: (1) We scrutinize multiple speech synthesis systems and provide an extensive report on the currently available state-of-the-art systems. We also develop voices using the existing implementations of the aforementioned systems, and (2) We use these voices to generate sample audios for randomly chosen words; manually evaluate the audio generated, and produce audio for all WordNet words using the winner voice model. We also produce audios for the Hindi WordNet Glosses and Example sentences. We describe our efforts to use pre-existing implementations for WaveNet - a model to generate raw audio using neural nets (Oord et al., 2016) and generate speech for Hindi. Our lexicographers perform a manual evaluation of the audio generated using multiple voices. A qualitative and quantitative analysis reveals that the voice model generated by us performs the best with an accuracy of 0.44.",
            "cx": 8448.6,
            "cy": -296.09,
            "rx": 108.789,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018",
            "citation_count": 54,
            "name": 54,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W18-0522",
            "name": "The Whole is Greater than the Sum of its Parts: Towards the Effectiveness of Voting Ensemble Classifiers for Complex Word Identification",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "In this paper, we present an effective system using voting ensemble classifiers to detect contextually complex words for non-native English speakers. To make the final decision, we channel a set of eight calibrated classifiers based on lexical, size and vocabulary features and train our model with annotated datasets collected from a mixture of native and non-native speakers. Thereafter, we test our system on three datasets namely News, WikiNews, and Wikipedia and report competitive results with an F1-Score ranging between 0.777 to 0.855 for each of the datasets. Our system outperforms multiple other models and falls within 0.042 to 0.026 percent of the best-performing model{'}s score in the shared task.",
            "cx": 6413.6,
            "cy": -296.09,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-7531",
            "name": "{H}indi Shabdamitra: A {W}ordnet based {E}-Learning Tool for Language Learning and Teaching",
            "publication_data": 2017,
            "citation": 0,
            "abstract": "None",
            "cx": 7912.6,
            "cy": -385.831,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "E17-1109",
            "name": "Entity Extraction in Biomedical Corpora: An Approach to Evaluate Word Embedding Features with {PSO} based Feature Selection",
            "publication_data": 2017,
            "citation": 7,
            "abstract": "Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for biomedical text mining. In this paper, we propose a novel approach of feature selection for entity extraction that exploits the concept of deep learning and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other features extracted by studying the properties of the datasets. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG, and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86{\\%}, 5.27{\\%} and 7.25{\\%} F-measure points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively.",
            "cx": 4937.6,
            "cy": -385.831,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2019",
            "citation_count": 13,
            "name": 13,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "Y18-3012",
            "name": "{IITP}-{MT} at {WAT}2018: Transformer-based Multilingual Indic-{E}nglish Neural Machine Translation System",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "None",
            "cx": 11267.6,
            "cy": -296.09,
            "rx": 115.931,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.wat-1.29",
            "name": "{IITP}-{MT} at {WAT}2021: Indic-{E}nglish Multilingual Neural Machine Translation using {R}omanized Vocabulary",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "This paper describes the systems submitted to WAT 2021 MultiIndicMT shared task by IITP-MT team. We submit two multilingual Neural Machine Translation (NMT) systems (Indic-to-English and English-to-Indic). We romanize all Indic data and create subword vocabulary which is shared between all Indic languages. We use back-translation approach to generate synthetic data which is appended to parallel corpus and used to train our models. The models are evaluated using BLEU, RIBES and AMFM scores with Indic-to-English model achieving 40.08 BLEU for Hindi-English pair and English-to-Indic model achieving 34.48 BLEU for English-Hindi pair. However, we observe that the shared romanized subword vocabulary is not helping English-to-Indic model at the time of generation, leading it to produce poor quality translations for Tamil, Telugu and Malayalam to English pairs with BLEU score of 8.51, 6.25 and 3.79 respectively.",
            "cx": 11267.6,
            "cy": -26.8701,
            "rx": 133.787,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.bea-1.8",
            "name": "Can Neural Networks Automatically Score Essay Traits?",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include Content, Organization, Language, Sentence Fluency, Word Choice, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring - where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing - which is why trait-scoring is important. In this paper, we show how a deep-learning based system can outperform feature-based machine learning systems, as well as a string kernel system in scoring essay traits.",
            "cx": 3329.6,
            "cy": -116.61,
            "rx": 79.3924,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P18-2011",
            "name": "Identification of Alias Links among Participants in Narratives",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "Identification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This task becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using proper nouns, pronouns or noun phrases with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four diverse history narratives of varying complexity. Our approach performs better than the state-of-the-art approach as well as a combination of standard named entity recognition and coreference resolution techniques.",
            "cx": 11603.6,
            "cy": -296.09,
            "rx": 139.1,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-2404",
            "name": "Extraction of Message Sequence Charts from Narrative History Text",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "In this paper, we advocate the use of Message Sequence Chart (MSC) as a knowledge representation to capture and visualize multi-actor interactions and their temporal ordering. We propose algorithms to automatically extract an MSC from a history narrative. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extraction, we employ a state-of-the art algorithm to temporally re-order these interactions. Our evaluation on multiple publicly available narratives shows improvements over four baselines.",
            "cx": 11464.6,
            "cy": -206.35,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N19-2017",
            "name": "Extraction of Message Sequence Charts from Software Use-Case Descriptions",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Software Requirement Specification documents provide natural language descriptions of the core functional requirements as a set of use-cases. Essentially, each use-case contains a set of actors and sequences of steps describing the interactions among them. Goals of use-case reviews and analyses include their correctness, completeness, detection of ambiguities, prototyping, verification, test case generation and traceability. Message Sequence Chart (MSC) have been proposed as a expressive, rigorous yet intuitive visual representation of use-cases. In this paper, we describe a linguistic knowledge-based approach to extract MSCs from use-cases. Compared to existing techniques, we extract richer constructs of the MSC notation such as timers, conditions and alt-boxes. We apply this tool to extract MSCs from several real-life software use-case descriptions and show that it performs better than the existing techniques. We also discuss the benefits and limitations of the extracted MSCs to meet the above goals.",
            "cx": 11667.6,
            "cy": -206.35,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.icon-main.23",
            "name": "Cognitively Aided Zero-Shot Automatic Essay Grading",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Automatic essay grading (AEG) is a process in which machines assign a grade to an essay written in response to a topic, called the prompt. Zero-shot AEG is when we train a system to grade essays written to a new prompt which was not present in our training data. In this paper, we describe a solution to the problem of zero-shot automatic essay grading, using cognitive information, in the form of gaze behaviour. Our experiments show that using gaze behaviour helps in improving the performance of AEG systems, especially when we provide a new essay written in response to a new prompt for scoring, by an average of almost 5 percentage points of QWK.",
            "cx": 6397.6,
            "cy": -116.61,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "L18-1278",
            "name": "A Deep Neural Network based Approach for Entity Extraction in Code-Mixed {I}ndian Social Media Text",
            "publication_data": 2018,
            "citation": 5,
            "abstract": "None",
            "cx": 3761.6,
            "cy": -296.09,
            "rx": 65.5227,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.aacl-main.90",
            "name": "A Unified Framework for Multilingual and Code-Mixed Visual Question Answering",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we propose an effective deep learning framework for multilingual and code- mixed visual question answering. The pro- posed model is capable of predicting answers from the questions in Hindi, English or Code- mixed (Hinglish: Hindi-English) languages. The majority of the existing techniques on Vi- sual Question Answering (VQA) focus on En- glish questions only. However, many applica- tions such as medical imaging, tourism, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust tech- nique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the dif- ferent languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image fea- tures. We perform extensive evaluation and ablation studies for English, Hindi and Code- mixed VQA. The evaluation shows that the proposed multilingual model achieves state-of- the-art performance in all these settings.",
            "cx": 3956.6,
            "cy": -116.61,
            "rx": 65.5227,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "L18-1440",
            "name": "{MMQA}: A Multi-domain Multi-lingual Question-Answering Framework for {E}nglish and {H}indi",
            "publication_data": 2018,
            "citation": 6,
            "abstract": "None",
            "cx": 3575.6,
            "cy": -296.09,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.coling-main.249",
            "name": "Reinforced Multi-task Approach for Multi-hop Question Generation",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "Question generation (QG) attempts to solve the inverse of question answering (QA) problem by generating a natural language question given a document and an answer. While sequence to sequence neural models surpass rule-based systems for QG, they are limited in their capacity to focus on more than one supporting fact. For QG, we often require multiple supporting facts to generate high-quality questions. Inspired by recent works on multi-hop reasoning in QA, we take up Multi-hop question generation, which aims at generating relevant questions based on supporting facts in the context. We employ multitask learning with the auxiliary task of answer-aware supporting fact prediction to guide the question generator. In addition, we also proposed a question-aware reward function in a Reinforcement Learning (RL) framework to maximize the utilization of the supporting facts. We demonstrate the effectiveness of our approach through experiments on the multi-hop question answering dataset, HotPotQA. Empirical evaluation shows our model to outperform the single-hop neural question generation models on both automatic evaluation metrics such as BLEU, METEOR, and ROUGE and human evaluation metrics for quality and coverage of the generated questions.",
            "cx": 3707.6,
            "cy": -116.61,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1442",
            "name": "Medical Sentiment Analysis using Social Media: Towards building a Patient Assisted System",
            "publication_data": 2018,
            "citation": 5,
            "abstract": "None",
            "cx": 5551.6,
            "cy": -296.09,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.findings-emnlp.386",
            "name": "Looking inside Noun Compounds: Unsupervised Prepositional and Free Paraphrasing",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "A noun compound is a sequence of contiguous nouns that acts as a single noun, although the predicate denoting the semantic relation between its components is dropped. Noun Compound Interpretation is the task of uncovering the relation, in the form of a preposition or a free paraphrase. Prepositional paraphrasing refers to the use of preposition to explain the semantic relation, whereas free paraphrasing refers to invoking an appropriate predicate denoting the semantic relation. In this paper, we propose an unsupervised methodology for these two types of paraphrasing. We use pre-trained contextualized language models to uncover the {`}missing{'} words (preposition or predicate). These language models are usually trained to uncover the missing word/words in a given input sentence. Our approach uses templates to prepare the input sequence for the language model. The template uses a special token to indicate the missing predicate. As the model has already been pre-trained to uncover a missing word (or a sequence of words), we exploit it to predict missing words for the input sequence. Our experiments using four datasets show that our unsupervised approach (a) performs comparably to supervised approaches for prepositional paraphrasing, and (b) outperforms supervised approaches for free paraphrasing. Paraphrasing (prepositional or free) using our unsupervised approach is potentially helpful for NLP tasks like machine translation and information extraction.",
            "cx": 7514.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.findings-acl.256",
            "name": "{F}rame{N}et-assisted Noun Compound Interpretation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "None",
            "cx": 7514.6,
            "cy": -26.8701,
            "rx": 168.997,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W19-5426",
            "name": "Utilizing Monolingual Data in {NMT} for Similar Languages: Submission to Similar Language Translation Task",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi -{\\textgreater} Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.",
            "cx": 3061.6,
            "cy": -206.35,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.mtsummit-research.2",
            "name": "Investigating Active Learning in Interactive Neural Machine Translation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Interactive-predictive translation is a collaborative iterative process and where human translators produce translations with the help of machine translation (MT) systems interactively. Various sampling techniques in active learning (AL) exist to update the neural MT (NMT) model in the interactive-predictive scenario. In this paper and we explore term based (named entity count (NEC)) and quality based (quality estimation (QE) and sentence similarity (Sim)) sampling techniques {--} which are used to find the ideal candidates from the incoming data {--} for human supervision and MT model{'}s weight updation. We carried out experiments with three language pairs and viz. German-English and Spanish-English and Hindi-English. Our proposed sampling technique yields 1.82 and 0.77 and 0.81 BLEU points improvements for German-English and Spanish-English and Hindi-English and respectively and over random sampling based baseline. It also improves the present state-of-the-art by 0.35 and 0.12 BLEU points for German-English and Spanish-English and respectively. Human editing effort in terms of number-of-words-changed also improves by 5 and 4 points for German-English and Spanish-English and respectively and compared to the state-of-the-art.",
            "cx": 3184.6,
            "cy": -26.8701,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.calcs-1.5",
            "name": "{IITP}-{MT} at {CALCS}2021: {E}nglish to {H}inglish Neural Machine Translation using Unsupervised Synthetic Code-Mixed Parallel Corpus",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "This paper describes the system submitted by IITP-MT team to Computational Approaches to Linguistic Code-Switching (CALCS 2021) shared task on MT for English\u00e2\u0086\u0092Hinglish. We submit a neural machine translation (NMT) system which is trained on the synthetic code-mixed (cm) English-Hinglish parallel corpus. We propose an approach to create code-mixed parallel corpus from a clean parallel corpus in an unsupervised manner. It is an alignment based approach and we do not use any linguistic resources for explicitly marking any token for code-switching. We also train NMT model on the gold corpus provided by the workshop organizers augmented with the generated synthetic code-mixed parallel corpus. The model trained over the generated synthetic cm data achieves 10.09 BLEU points over the given test set.",
            "cx": 3474.6,
            "cy": -26.8701,
            "rx": 133.787,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "L18-1559",
            "name": "{TAP}-{DLND} 1.0 : A Corpus for Document Level Novelty Detection",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "Detecting novelty of an entire document is an Artificial Intelligence (AI) frontier problem that has widespread NLP applications, such as extractive document summarization, tracking development of news events, predicting impact of scholarly articles, etc. Important though the problem is, we are unaware of any benchmark document level data that correctly addresses the evaluation of automatic novelty detection techniques in a classification framework. To bridge this gap, we present here a resource for benchmarking the techniques for document level novelty detection. We create the resource via event-specific crawling of news documents across several domains in a periodic manner. We release the annotated corpus with necessary statistics and show its use with a developed system for the problem in concern.",
            "cx": 11895.6,
            "cy": -296.09,
            "rx": 135.115,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C18-1237",
            "name": "Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "The rapid growth of documents across the web has necessitated finding means of discarding redundant documents and retaining novel ones. Capturing redundancy is challenging as it may involve investigating at a deep semantic level. Techniques for detecting such semantic redundancy at the document level are scarce. In this work we propose a deep Convolutional Neural Networks (CNN) based model to classify a document as novel or redundant with respect to a set of relevant documents already seen by the system. The system is simple and do not require any manual feature engineering. Our novel scheme encodes relevant and relative information from both source and target texts to generate an intermediate representation which we coin as the Relative Document Vector (RDV). The proposed method outperforms the existing state-of-the-art on a document-level novelty detection dataset by a margin of \u00e2\u0088\u00bc5{\\%} in terms of accuracy. We further demonstrate the effectiveness of our approach on a standard paraphrase detection dataset where paraphrased passages closely resemble to semantically redundant documents.",
            "cx": 12146.6,
            "cy": -296.09,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.ltedi-1.29",
            "name": "{CFILT} {IIT} {B}ombay@{LT}-{EDI}-{EACL}2021: Hope Speech Detection for Equality, Diversity, and Inclusion using Multilingual Representation from{T}ransformers",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "With the internet becoming part and parcel of our lives, engagement in social media has increased a lot. Identifying and eliminating offensive content from social media has become of utmost priority to prevent any kind of violence. However, detecting encouraging, supportive and positive content is equally important to prevent misuse of censorship targeted to attack freedom of speech. This paper presents our system for the shared task Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI, EACL 2021. The data for this shared task is provided in English, Tamil, and Malayalam which was collected from YouTube comments. It is a multiclass classification problem where each data instance is categorized into one of the three classes: {`}Hope speech{'}, {`}Not hope speech{'}, and {`}Not in intended language{'}. We propose a system that employs multilingual transformer models to obtain the representation of text and classifies it into one of the three classes. We explored the use of multilingual models trained specifically for Indian languages along with generic multilingual models. Our system was ranked 2nd for English, 2nd for Malayalam, and 7th for the Tamil language in the final leader board published by organizers and obtained a weighted F1-score of 0.92, 0.84, 0.55 respectively on the hidden test dataset used for the competition. We have made our system publicly available at GitHub.",
            "cx": 4118.6,
            "cy": -26.8701,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D18-1382",
            "name": "Contextual Inter-modal Attention for Multi-modal Sentiment Analysis",
            "publication_data": 2018,
            "citation": 7,
            "abstract": "Multi-modal sentiment analysis offers various challenges, one being the effective combination of different input modalities, namely text, visual and acoustic. In this paper, we propose a recurrent neural network based multi-modal attention framework that leverages the contextual information for utterance-level sentiment prediction. The proposed approach applies attention on multi-modal multi-utterance representations and tries to learn the contributing features amongst them. We evaluate our proposed approach on two multi-modal sentiment analysis benchmark datasets, viz. CMU Multi-modal Opinion-level Sentiment Intensity (CMU-MOSI) corpus and the recently released CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Evaluation results show the effectiveness of our proposed approach with the accuracies of 82.31{\\%} and 79.80{\\%} for the MOSI and MOSEI datasets, respectively. These are approximately 2 and 1 points performance improvement over the state-of-the-art models for the datasets.",
            "cx": 5775.6,
            "cy": -296.09,
            "rx": 117.26,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N19-1034",
            "name": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis",
            "publication_data": 2019,
            "citation": 4,
            "abstract": "Related tasks often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The multi-modal inputs (i.e. text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance. We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment and emotion analysis. Evaluation results suggest that multi-task learning framework offers improvement over the single-task framework. The proposed approach reports new state-of-the-art performance for both sentiment analysis and emotion analysis.",
            "cx": 5529.6,
            "cy": -206.35,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D19-1566",
            "name": "Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis",
            "publication_data": 2019,
            "citation": 2,
            "abstract": "In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information, (e.g., textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The proposed model learns the inter-modal interaction among the participating modalities through an auto-encoder mechanism. We employ a context-aware attention module to exploit the correspondence among the neighboring utterances. We evaluate our proposed approach for five standard multi-modal affect analysis datasets. Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state-of-the-art systems.",
            "cx": 5753.6,
            "cy": -206.35,
            "rx": 123.988,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.coling-main.393",
            "name": "{MEISD}: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Emotion and sentiment classification in dialogues is a challenging task that has gained popularity in recent times. Humans tend to have multiple emotions with varying intensities while expressing their thoughts and feelings. Emotions in an utterance of dialogue can either be independent or dependent on the previous utterances, thus making the task complex and interesting. Multi-label emotion detection in conversations is a significant task that provides the ability to the system to understand the various emotions of the users interacting. Sentiment analysis in dialogue/conversation, on the other hand, helps in understanding the perspective of the user with respect to the ongoing conversation. Along with text, additional information in the form of audio and video assist in identifying the correct emotions with the appropriate intensity and sentiments in an utterance of a dialogue. Lately, quite a few datasets have been made available for dialogue emotion and sentiment classification, but these datasets are imbalanced in representing different emotions and consist of an only single emotion. Hence, we present at first a large-scale balanced Multimodal Multi-label Emotion, Intensity, and Sentiment Dialogue dataset (MEISD), collected from different TV series that has textual, audio and visual features, and then establish a baseline setup for further research.",
            "cx": 5497.6,
            "cy": -116.61,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.acl-main.401",
            "name": "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit. For multi-tasking, we propose two attention mechanisms, viz. Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention). The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities. In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities. Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking. Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems. The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.",
            "cx": 5732.6,
            "cy": -116.61,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "C18-1042",
            "name": "Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "In this paper, we propose a hybrid technique for semantic question matching. It uses a proposed two-layered taxonomy for English questions by augmenting state-of-the-art deep learning models with question classes obtained from a deep learning based question classifier. Experiments performed on three open-domain datasets demonstrate the effectiveness of our proposed approach. We achieve state-of-the-art results on partial ordering question ranking (POQR) benchmark dataset. Our empirical analysis shows that coupling standard distributional features (provided by the question encoder) with knowledge from taxonomy is more effective than either deep learning or taxonomy-based knowledge alone.",
            "cx": 3920.6,
            "cy": -296.09,
            "rx": 74.9067,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018.gwc-1.47",
            "name": "pyiwn: A Python based {API} to access {I}ndian Language {W}ord{N}ets",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "Indian language WordNets have their individual web-based browsing interfaces along with a common interface for IndoWordNet. These interfaces prove to be useful for language learners and in an educational domain, however, they do not provide the functionality of connecting to them and browsing their data through a lucid application programming interface or an API. In this paper, we present our work on creating such an easy-to-use framework which is bundled with the data for Indian language WordNets and provides NLTK WordNet interface like core functionalities in Python. Additionally, we use a pre-built speech synthesis system for Hindi language and augment Hindi data with audios for words, glosses, and example sentences. We provide a detailed usage of our API and explain the functions for ease of the user. Also, we package the IndoWordNet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development.",
            "cx": 12340.6,
            "cy": -296.09,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.nuse-1.11",
            "name": "Extracting Message Sequence Charts from {H}indi Narrative Text",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we propose the use of Message Sequence Charts (MSC) as a representation for visualizing narrative text in Hindi. An MSC is a formal representation allowing the depiction of actors and interactions among these actors in a scenario, apart from supporting a rich framework for formal inference. We propose an approach to extract MSC actors and interactions from a Hindi narrative. As a part of the approach, we enrich an existing event annotation scheme where we provide guidelines for annotation of the mood of events (realis vs irrealis) and guidelines for annotation of event arguments. We report performance on multiple evaluation criteria by experimenting with Hindi narratives from Indian History. Though Hindi is the fourth most-spoken first language in the world, from the NLP perspective it has comparatively lesser resources than English. Moreover, there is relatively less work in the context of event processing in Hindi. Hence, we believe that this work is among the initial works for Hindi event processing.",
            "cx": 12340.6,
            "cy": -116.61,
            "rx": 100.318,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N19-1091",
            "name": "Courteously Yours: Inducing courteous behavior in Customer Care responses using Reinforced Pointer Generator Network",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "In this paper, we propose an effective deep learning framework for inducing courteous behavior in customer care responses. The interaction between a customer and the customer care representative contributes substantially to the overall customer experience. Thus it is imperative for customer care agents and chatbots engaging with humans to be personal, cordial and emphatic to ensure customer satisfaction and retention. Our system aims at automatically transforming neutral customer care responses into courteous replies. Along with stylistic transfer (of courtesy), our system ensures that responses are coherent with the conversation history, and generates courteous expressions consistent with the emotional state of the customer. Our technique is based on a reinforced pointer-generator model for the sequence to sequence task. The model is also conditioned on a hierarchically encoded and emotionally aware conversational context. We use real interactions on Twitter between customer care professionals and aggrieved customers to create a large conversational dataset having both forms of agent responses: {`}generic{'} and {`}courteous{'}. We perform quantitative and qualitative analyses on established and task-specific metrics, both automatic and human evaluation based. Our evaluation shows that the proposed models can generate emotionally-appropriate courteous expressions while preserving the content. Experimental results also prove that our proposed approach performs better than the baseline models.",
            "cx": 12581.6,
            "cy": -206.35,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.lrec-1.514",
            "name": "Incorporating Politeness across Languages in Customer Care Responses: Towards building a Multi-lingual Empathetic Dialogue Agent",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Customer satisfaction is an essential aspect of customer care systems. It is imperative for such systems to be polite while handling customer requests/demands. In this paper, we present a large multi-lingual conversational dataset for English and Hindi. We choose data from Twitter having both generic and courteous responses between customer care agents and aggrieved users. We also propose strong baselines that can induce courteous behaviour in generic customer care response in a multi-lingual scenario. We build a deep learning framework that can simultaneously handle different languages and incorporate polite behaviour in the customer care agent{'}s responses. Our system is competent in generating responses in different languages (here, English and Hindi) depending on the customer{'}s preference and also is able to converse with humans in an empathetic manner to ensure customer satisfaction and retention. Experimental results show that our proposed models can converse in both the languages and the information shared between the languages helps in improving the performance of the overall system. Qualitative and quantitative analysis shows that the proposed method can converse in an empathetic manner by incorporating courteousness in the responses and hence increasing customer satisfaction.",
            "cx": 12581.6,
            "cy": -116.61,
            "rx": 123.073,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019.gwc-1.51",
            "name": "Utilizing Wordnets for Cognate Detection among {I}ndian Languages",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "Automatic Cognate Detection (ACD) is a challenging task which has been utilized to help NLP applications like Machine Translation, Information Retrieval and Computational Phylogenetics. Unidentified cognate pairs can pose a challenge to these applications and result in a degradation of performance. In this paper, we detect cognate word pairs among ten Indian languages with Hindi and use deep learning methodologies to predict whether a word pair is cognate or not. We identify IndoWordnet as a potential resource to detect cognate word pairs based on orthographic similarity-based methods and train neural network models using the data obtained from it. We identify parallel corpora as another potential resource and perform the same experiments for them. We also validate the contribution of Wordnets through further experimentation and report improved performance of up to 26{\\%}. We discuss the nuances of cognate detection among closely related Indian languages and release the lists of detected cognates as a dataset. We also observe the behaviour of, to an extent, unrelated Indian language pairs and release the lists of detected cognates among them as well.",
            "cx": 12929.6,
            "cy": -206.35,
            "rx": 91.4341,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.lrec-1.378",
            "name": "Challenge Dataset of Cognates and False Friend Pairs from {I}ndian Languages",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Cognates are present in multiple variants of the same text across different languages (e.g., {``}hund{''} in German and {``}hound{''} in the English language mean {``}dog{''}). They pose a challenge to various Natural Language Processing (NLP) applications such as Machine Translation, Cross-lingual Sense Disambiguation, Computational Phylogenetics, and Information Retrieval. A possible solution to address this challenge is to identify cognates across language pairs. In this paper, we describe the creation of two cognate datasets for twelve Indian languages namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. We digitize the cognate data from an Indian language cognate dictionary and utilize linked Indian language Wordnets to generate cognate sets. Additionally, we use the Wordnet data to create a False Friends{'} dataset for eleven language pairs. We also evaluate the efficacy of our dataset using previously available baseline cognate detection approaches. We also perform a manual evaluation with the help of lexicographers and release the curated gold-standard dataset with this paper.",
            "cx": 12820.6,
            "cy": -116.61,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.coling-main.119",
            "name": "Harnessing Cross-lingual Features to Improve Cognate Detection for Low-resource Languages",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Cognates are variants of the same lexical form across different languages; for example {``}fonema{''} in Spanish and {``}phoneme{''} in English are cognates, both of which mean {``}a unit of sound{''}. The task of automatic detection of cognates among any two languages can help downstream NLP tasks such as Cross-lingual Information Retrieval, Computational Phylogenetics, and Machine Translation. In this paper, we demonstrate the use of cross-lingual word embeddings for detecting cognates among fourteen Indian Languages. Our approach introduces the use of context from a knowledge graph to generate improved feature representations for cognate detection. We, then, evaluate the impact of our cognate detection mechanism on neural machine translation (NMT), as a downstream task. We evaluate our methods to detect cognates on a challenging dataset of twelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. Additionally, we create evaluation datasets for two more Indian languages, Konkani and Nepali. We observe an improvement of up to 18{\\%} points, in terms of F-score, for cognate detection. Furthermore, we observe that cognates extracted using our method help improve NMT quality by up to 2.76 BLEU. We also release our code, newly constructed datasets and cross-lingual models publicly.",
            "cx": 13039.6,
            "cy": -116.61,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.sltu-1.49",
            "name": "{``}A Passage to {I}ndia{''}: Pre-trained Word Embeddings for {I}ndian Languages",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Dense word vectors or {`}word embeddings{'} which encode semantic properties of words, have now become integral to NLP tasks like Machine Translation (MT), Question Answering (QA), Word Sense Disambiguation (WSD), and Information Retrieval (IR). In this paper, we use various existing approaches to create multiple word embeddings for 14 Indian languages. We place these embeddings for all these languages, \\textit{viz.}, Assamese, Bengali, Gujarati, Hindi, Kannada, Konkani, Malayalam, Marathi, Nepali, Odiya, Punjabi, Sanskrit, Tamil, and Telugu in a single repository. Relatively newer approaches that emphasize catering to context (BERT, ELMo, \\textit{etc.}) have shown significant improvements, but require a large amount of resources to generate usable models. We release pre-trained embeddings generated using both contextual and non-contextual approaches. We also use MUSE and XLM to train cross-lingual embeddings for all pairs of the aforementioned languages. To show the efficacy of our embeddings, we evaluate our embedding models on XPOS, UPOS and NER tasks for all these languages. We release a total of 436 models using 8 different approaches. We hope they are useful for the resource-constrained Indian language NLP. The title of this paper refers to the famous novel {``}A Passage to India{''} by E.M. Forster, published initially in 1924.",
            "cx": 577.597,
            "cy": -116.61,
            "rx": 79.3924,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.iwslt-1.22",
            "name": "Generating Fluent Translations from Disfluent Text Without Access to Fluent References: {IIT} {B}ombay@{IWSLT}2020",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Machine translation systems perform reasonably well when the input is well-formed speech or text. Conversational speech is spontaneous and inherently consists of many disfluencies. Producing fluent translations of disfluent source text would typically require parallel disfluent to fluent training data. However, fluent translations of spontaneous speech are an additional resource that is tedious to obtain. This work describes the submission of IIT Bombay to the Conversational Speech Translation challenge at IWSLT 2020. We specifically tackle the problem of disfluency removal in disfluent-to-fluent text-to-text translation assuming no access to fluent references during training. Common patterns of disfluency are extracted from disfluent references and a noise induction model is used to simulate them starting from a clean monolingual corpus. This synthetically constructed dataset is then considered as a proxy for labeled data during training. We also make use of additional fluent text in the target language to help generate fluent translations. This work uses no fluent references during training and beats a baseline model by a margin of 4.21 and 3.11 BLEU points where the baseline uses disfluent and fluent references, respectively. Index Terms- disfluency removal, machine translation, noise induction, leveraging monolingual data, denoising for disfluency removal.",
            "cx": 13254.6,
            "cy": -116.61,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.eacl-main.299",
            "name": "Disfluency Correction using Unsupervised and Semi-supervised Learning",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Spoken language is different from the written language in its style and structure. Disfluencies that appear in transcriptions from speech recognition systems generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semi-supervised way. Our unsupervised approach achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semi-supervision. Both are comparable to two competitive fully-supervised models.",
            "cx": 13254.6,
            "cy": -26.8701,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.acl-main.402",
            "name": "Towards Emotion-aided Multi-modal Dialogue Act Classification",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of \\textit{both} multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.",
            "cx": 13453.6,
            "cy": -116.61,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.naacl-main.456",
            "name": "Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in {T}witter",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA ({`}TA{'} means tweet act, i.e., speech act in Twitter) dataset called \\textit{EmoTA} collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants.",
            "cx": 13453.6,
            "cy": -26.8701,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        }
    ],
    [
        {
            "source": "2006",
            "target": "2008",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "P06-2100",
            "target": "I08-1067",
            "d": "M3594.36,-1275.89C3329.56,-1260.07 2641.15,-1218.95 2357.04,-1201.98"
        },
        {
            "source": "P06-2100",
            "target": "C10-2040",
            "d": "M3804.3,-1273.79C4298.5,-1233.58 6410.31,-1061.74 6888.2,-1022.86"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "I08-7013",
            "target": "sankaran-etal-2008-common",
            "d": "M7297.57,-1193.49C7300.22,-1193.49 7302.86,-1193.49 7305.51,-1193.49"
        },
        {
            "source": "I08-1067",
            "target": "P09-1090",
            "d": "M2210.97,-1166.64C2203.92,-1157.81 2195.93,-1147.79 2188.47,-1138.43"
        },
        {
            "source": "I08-1067",
            "target": "W12-5906",
            "d": "M2112.23,-1189.98C1815.23,-1183.26 1063.7,-1163.12 1020.6,-1130.62 938.275,-1068.55 922.538,-935.578 919.878,-871.504"
        },
        {
            "source": "I08-1067",
            "target": "W14-5105",
            "d": "M2126.81,-1180.26C2094.53,-1171.4 2061.71,-1156.3 2039.6,-1130.62 2005.62,-1091.17 2020.6,-1067.08 2020.6,-1015.01 2020.6,-1015.01 2020.6,-1015.01 2020.6,-833.531 2020.6,-780.978 2046.92,-725.121 2066.81,-690.411"
        },
        {
            "source": "I08-1067",
            "target": "W14-5148",
            "d": "M2350.84,-1189.5C2577.55,-1183.08 3062.74,-1165.65 3226.6,-1130.62 3369.58,-1100.06 3534.6,-1161.23 3534.6,-1015.01 3534.6,-1015.01 3534.6,-1015.01 3534.6,-833.531 3534.6,-778.394 3547.43,-762.706 3579.6,-717.921 3588.01,-706.21 3599.13,-695.377 3610.19,-686.158"
        },
        {
            "source": "I08-1067",
            "target": "W14-3308",
            "d": "M2112.4,-1189.48C1892.75,-1183.16 1432.33,-1166.04 1276.6,-1130.62 1160.37,-1104.19 1034.6,-1134.21 1034.6,-1015.01 1034.6,-1015.01 1034.6,-1015.01 1034.6,-833.531 1034.6,-783.989 1045.37,-727.607 1053.56,-691.941"
        },
        {
            "source": "I08-1067",
            "target": "kunchukuttan-etal-2014-shata",
            "d": "M2111.54,-1192.08C1840.04,-1190.27 1185.87,-1180.68 971.597,-1130.62 846.073,-1101.29 705.597,-1143.92 705.597,-1015.01 705.597,-1015.01 705.597,-1015.01 705.597,-833.531 705.597,-777.513 667.517,-722.712 638.591,-689.166"
        },
        {
            "source": "I08-1067",
            "target": "W15-5950",
            "d": "M2351.66,-1190.83C2585.52,-1186.68 3085.05,-1172.91 3150.6,-1130.62 3199.59,-1099.02 3212.6,-1073.32 3212.6,-1015.01 3212.6,-1015.01 3212.6,-1015.01 3212.6,-743.791 3212.6,-667.579 3294.01,-617.863 3356.4,-590.951"
        },
        {
            "source": "I08-1067",
            "target": "W16-6303",
            "d": "M2258.11,-1167.28C2267.59,-1156.82 2277.44,-1144.01 2283.6,-1130.62 2305.35,-1083.31 2302.6,-1067.08 2302.6,-1015.01 2302.6,-1015.01 2302.6,-1015.01 2302.6,-654.051 2302.6,-600.737 2305.55,-578.699 2270.6,-538.441 2253.56,-518.811 2229.14,-505.184 2205.34,-495.826"
        },
        {
            "source": "I08-1067",
            "target": "W16-4622",
            "d": "M2277.24,-1168.35C2291.87,-1158.5 2306.8,-1145.8 2316.6,-1130.62 2345.05,-1086.53 2340.6,-1067.49 2340.6,-1015.01 2340.6,-1015.01 2340.6,-1015.01 2340.6,-654.051 2340.6,-604.804 2332.9,-548.351 2327.06,-512.589"
        },
        {
            "source": "I08-1067",
            "target": "L16-1485",
            "d": "M2352.09,-1192.81C2607.25,-1192.39 3194.38,-1185.28 3385.6,-1130.62 3479.25,-1103.85 3822.99,-878.023 3834.6,-861.401 3910.03,-753.355 3901.19,-586.06 3892.78,-512.9"
        },
        {
            "source": "I08-1067",
            "target": "I17-2048",
            "d": "M2131.95,-1178.46C1983.76,-1154.76 1722.6,-1100.82 1722.6,-1015.01 1722.6,-1015.01 1722.6,-1015.01 1722.6,-743.791 1722.6,-652.471 1773.22,-609.285 1715.6,-538.441 1645.12,-451.797 1315.38,-410.198 1150.69,-394.547"
        },
        {
            "source": "I08-1067",
            "target": "L18-1548",
            "d": "M2341.21,-1182.36C2582.16,-1158.6 3136.6,-1095.22 3136.6,-1015.01 3136.6,-1015.01 3136.6,-1015.01 3136.6,-923.271 3136.6,-854.381 3191.33,-684.08 3231.6,-628.181 3246.57,-607.398 3265.5,-615.265 3276.6,-592.181 3286.94,-570.654 3282.05,-561.695 3276.6,-538.441 3257.83,-458.328 3206.63,-375.374 3175.89,-330.62"
        },
        {
            "source": "I08-1067",
            "target": "N19-1387",
            "d": "M2112.12,-1190.44C1887.61,-1183.88 1415.74,-1156.36 1309.6,-1040.88 1224.24,-948.018 1248.6,-602.704 1248.6,-476.571 1248.6,-476.571 1248.6,-476.571 1248.6,-384.831 1248.6,-298.358 1514.24,-244.14 1661.34,-220.746"
        },
        {
            "source": "C08-2007",
            "target": "L16-1369",
            "d": "M9459.6,-1166.33C9459.6,-1131.71 9459.6,-1068.78 9459.6,-1015.01 9459.6,-1015.01 9459.6,-1015.01 9459.6,-654.051 9459.6,-605.324 9459.6,-549.071 9459.6,-513.185"
        },
        {
            "source": "C08-1068",
            "target": "W09-3536",
            "d": "M9756.69,-1166.64C9751.27,-1157.83 9745.12,-1147.85 9739.37,-1138.51"
        },
        {
            "source": "C08-1068",
            "target": "C10-2091",
            "d": "M9716.88,-1173.21C9693.75,-1163.18 9667.88,-1149.1 9648.6,-1130.62 9625.2,-1108.2 9608.14,-1075.33 9597.54,-1050.36"
        },
        {
            "source": "C08-1068",
            "target": "2010.jeptalnrecital-court.7",
            "d": "M9781.9,-1166.68C9792.95,-1136.06 9811.42,-1084.83 9823.8,-1050.49"
        },
        {
            "source": "C08-1068",
            "target": "W13-4706",
            "d": "M9823.03,-1171.79C9879.62,-1144.82 9963.6,-1091.85 9963.6,-1015.01 9963.6,-1015.01 9963.6,-1015.01 9963.6,-923.271 9963.6,-862.607 9913.92,-808.401 9876.7,-776.305"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "W09-3536",
            "target": "C10-2091",
            "d": "M9685.47,-1081.06C9668.69,-1070.07 9648.06,-1056.56 9629.83,-1044.63"
        },
        {
            "source": "W09-3536",
            "target": "2010.jeptalnrecital-court.7",
            "d": "M9748.68,-1080.38C9762.85,-1069.85 9779.98,-1057.11 9795.34,-1045.69"
        },
        {
            "source": "W09-3536",
            "target": "W13-4706",
            "d": "M9718.39,-1076.62C9718.78,-1052.91 9720.82,-1017.17 9728.6,-987.141 9748.63,-909.815 9791.33,-826.098 9816.63,-780.406"
        },
        {
            "source": "P09-1090",
            "target": "W15-5914",
            "d": "M2171.04,-1076.68C2182.58,-1042.43 2200.6,-980.164 2200.6,-925.271 2200.6,-925.271 2200.6,-925.271 2200.6,-743.791 2200.6,-691.719 2200.89,-676.545 2181.6,-628.181 2177.74,-618.504 2172.17,-608.815 2166.34,-600.071"
        },
        {
            "source": "P09-1090",
            "target": "W16-6303",
            "d": "M2187.86,-1077.48C2218.37,-1045.26 2264.6,-986.369 2264.6,-925.271 2264.6,-925.271 2264.6,-925.271 2264.6,-654.051 2264.6,-601.979 2276.01,-580.711 2245.6,-538.441 2233.43,-521.53 2215.21,-508.837 2196.74,-499.495"
        },
        {
            "source": "D09-1048",
            "target": "C10-1063",
            "d": "M4059.98,-1076.9C4054.71,-1068.17 4048.73,-1058.28 4043.14,-1049.01"
        },
        {
            "source": "D09-1048",
            "target": "P11-1057",
            "d": "M4093.43,-1077.18C4099.86,-1066.48 4106.34,-1053.58 4109.6,-1040.88 4115.54,-1017.75 4117.88,-1009.54 4109.6,-987.141 4105.65,-976.477 4098.96,-966.409 4091.63,-957.615"
        },
        {
            "source": "D09-1048",
            "target": "I11-1078",
            "d": "M4008.24,-1084.44C3982.93,-1074.84 3955.51,-1060.81 3935.6,-1040.88 3913.79,-1019.06 3900.55,-986.327 3893.05,-961.255"
        },
        {
            "source": "D09-1048",
            "target": "2016.gwc-1.57",
            "d": "M4099.28,-1077.56C4108.59,-1066.8 4118.87,-1053.78 4126.6,-1040.88 4184.33,-944.502 4305.68,-620.381 4345.58,-512.119"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "W10-3604",
            "target": "W12-5020",
            "d": "M10243.9,-989.076C10208.6,-957.408 10146.2,-901.531 10107.3,-866.628"
        },
        {
            "source": "W10-3604",
            "target": "C12-2023",
            "d": "M10270.6,-986.824C10270.6,-956.326 10270.6,-905.775 10270.6,-871.583"
        },
        {
            "source": "W10-3604",
            "target": "W14-5103",
            "d": "M10294.1,-988.575C10319.6,-960.427 10359.1,-911.575 10377.6,-861.401 10398.4,-804.888 10400.1,-733.963 10398.8,-691.978"
        },
        {
            "source": "W10-3607",
            "target": "W11-3001",
            "d": "M10504.6,-986.686C10504.6,-978.72 10504.6,-969.829 10504.6,-961.34"
        },
        {
            "source": "P10-1137",
            "target": "W10-4011",
            "d": "M1262.32,-1014.01C1277.63,-1014.01 1292.94,-1014.01 1308.24,-1014.01"
        },
        {
            "source": "P10-1137",
            "target": "N15-1125",
            "d": "M1162.6,-986.849C1162.6,-952.229 1162.6,-889.299 1162.6,-835.531 1162.6,-835.531 1162.6,-835.531 1162.6,-743.791 1162.6,-694.371 1152.98,-637.96 1145.67,-602.254"
        },
        {
            "source": "P10-1155",
            "target": "P13-2096",
            "d": "M2757.67,-1012.45C2842.98,-1009.69 2993.51,-997.838 3112.6,-951.141 3218.83,-909.486 3324.47,-823.92 3377.14,-777.473"
        },
        {
            "source": "P10-1155",
            "target": "W14-0124",
            "d": "M2675.76,-987.618C2669.67,-976.619 2662.84,-963.463 2657.6,-951.141 2656.28,-948.043 2598.42,-769.642 2573.17,-691.767"
        },
        {
            "source": "P10-1155",
            "target": "W15-5908",
            "d": "M2734.27,-993.406C2818.29,-955.029 3007.25,-865.509 3155.6,-771.661 3244.72,-715.279 3248.15,-670.838 3344.6,-628.181 3424.8,-592.711 3453.37,-612.842 3538.6,-592.181 3545.21,-590.577 3552.08,-588.767 3558.91,-586.868"
        },
        {
            "source": "P10-1155",
            "target": "W15-5945",
            "d": "M2626.71,-1005.4C2535.43,-990.574 2378.6,-948.637 2378.6,-835.531 2378.6,-835.531 2378.6,-835.531 2378.6,-743.791 2378.6,-690.728 2406.85,-634.995 2428.21,-600.453"
        },
        {
            "source": "P10-1155",
            "target": "L16-1349",
            "d": "M2686.05,-986.942C2680.43,-952.425 2671.6,-889.606 2671.6,-835.531 2671.6,-835.531 2671.6,-835.531 2671.6,-654.051 2671.6,-589.44 2612.83,-536.371 2568.62,-505.588"
        },
        {
            "source": "P10-1155",
            "target": "2016.gwc-1.54",
            "d": "M2695.15,-986.942C2700.77,-952.425 2709.6,-889.606 2709.6,-835.531 2709.6,-835.531 2709.6,-835.531 2709.6,-654.051 2709.6,-605.324 2709.6,-549.071 2709.6,-513.185"
        },
        {
            "source": "N10-1065",
            "target": "khapra-etal-2014-transliteration",
            "d": "M1888.09,-987.052C1877.17,-924.204 1849.31,-763.763 1836.82,-691.857"
        },
        {
            "source": "N10-1065",
            "target": "K16-1027",
            "d": "M1903.73,-987.096C1917.35,-953.004 1938.6,-890.916 1938.6,-835.531 1938.6,-835.531 1938.6,-835.531 1938.6,-654.051 1938.6,-601.979 1947.62,-582.328 1919.6,-538.441 1911.33,-525.497 1899.28,-514.581 1886.64,-505.696"
        },
        {
            "source": "N10-1065",
            "target": "Q18-1022",
            "d": "M1914.27,-987.752C1922.72,-976.99 1931.94,-963.974 1938.6,-951.141 1963.5,-903.128 1976.6,-889.618 1976.6,-835.531 1976.6,-835.531 1976.6,-835.531 1976.6,-474.571 1976.6,-390.361 1880.99,-342.366 1808.26,-318.039"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "W12-5209",
            "d": "M4773.32,-989.9C4675.61,-955.831 4493.98,-892.501 4396.49,-858.508"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "C12-2008",
            "d": "M4862.14,-987.204C4889.42,-955.964 4935.43,-903.284 4965.4,-868.962"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "L18-1728",
            "d": "M4712.35,-1000.13C4551.62,-980.83 4289.2,-938.213 4233.6,-861.401 4112.04,-693.486 4255.14,-425.26 4312.71,-331.1"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "2018.gwc-1.31",
            "d": "M4841.23,-986.845C4842.83,-946.944 4842.44,-869.053 4820.6,-807.661 4765.82,-653.706 4683,-651.303 4615.6,-502.441 4588.39,-442.346 4595.73,-422.388 4577.6,-358.96 4575.16,-350.419 4572.41,-341.252 4569.78,-332.661"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "W19-7509",
            "d": "M4987.71,-1011.92C5585.11,-1006.57 7785.6,-977.238 7785.6,-835.531 7785.6,-835.531 7785.6,-835.531 7785.6,-384.831 7785.6,-296.151 7888.26,-249.446 7967.44,-226.612"
        },
        {
            "source": "C10-2040",
            "target": "W15-5905",
            "d": "M7073.67,-1010.26C7271.84,-1003.55 7736.71,-984.461 7799.6,-951.141 7857.62,-920.399 7891.6,-901.197 7891.6,-835.531 7891.6,-835.531 7891.6,-835.531 7891.6,-743.791 7891.6,-695.064 7891.6,-638.812 7891.6,-602.925"
        },
        {
            "source": "C10-1063",
            "target": "P11-1057",
            "d": "M4032.61,-987.161C4035.84,-978.846 4039.46,-969.485 4042.9,-960.609"
        },
        {
            "source": "C10-1063",
            "target": "I11-1078",
            "d": "M3986.34,-989.959C3968.2,-978.423 3946.12,-964.387 3927.17,-952.342"
        },
        {
            "source": "C10-1063",
            "target": "ar-etal-2012-cost",
            "d": "M4075.95,-994.186C4168.54,-961.517 4358.69,-894.423 4459.49,-858.856"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "P11-4022",
            "target": "C12-1113",
            "d": "M5871.84,-897.893C5879.04,-888.878 5887.25,-878.588 5894.9,-869"
        },
        {
            "source": "P11-4022",
            "target": "S13-2082",
            "d": "M5824.18,-898.148C5814.82,-887.792 5805.49,-875.027 5800.6,-861.401 5791.38,-835.734 5795.5,-804.982 5801.46,-781.649"
        },
        {
            "source": "P11-1057",
            "target": "L16-1485",
            "d": "M4076.87,-897.917C4101.15,-864.919 4138.6,-804.511 4138.6,-745.791 4138.6,-745.791 4138.6,-745.791 4138.6,-654.051 4138.6,-580.63 4080.97,-580.217 4020.6,-538.441 3998.79,-523.349 3972.71,-510.296 3949.45,-500.122"
        },
        {
            "source": "I11-1078",
            "target": "P13-2096",
            "d": "M3825.87,-915.31C3769.33,-906.424 3682.09,-889.476 3610.6,-861.401 3553.55,-838.998 3493.42,-801.933 3454.38,-775.821"
        },
        {
            "source": "I11-1078",
            "target": "N13-1088",
            "d": "M3927.9,-904.968C3936.91,-901.883 3946.44,-899.125 3955.6,-897.401 4041.5,-881.223 5467.5,-923.914 5528.6,-861.401 5550.22,-839.274 5534.67,-804.63 5517.4,-779.031"
        },
        {
            "source": "I11-1078",
            "target": "W15-5908",
            "d": "M3824.15,-917.464C3687.76,-902.887 3363.61,-859.475 3307.6,-771.661 3294.75,-751.524 3295.55,-738.547 3307.6,-717.921 3334.77,-671.379 3475.45,-616.86 3560,-587.547"
        },
        {
            "source": "D11-1100",
            "target": "ar-etal-2012-cost",
            "d": "M5079.75,-915.604C4966.91,-906.581 4784.91,-889.2 4629.6,-861.401 4620.22,-859.722 4610.43,-857.68 4600.77,-855.487"
        },
        {
            "source": "D11-1100",
            "target": "C12-2008",
            "d": "M5136.64,-900.104C5111.87,-888.844 5081.91,-875.225 5055.9,-863.401"
        },
        {
            "source": "D11-1100",
            "target": "C12-1113",
            "d": "M5287.73,-911.301C5425.08,-894.884 5670.96,-865.491 5812.74,-848.544"
        },
        {
            "source": "D11-1100",
            "target": "S13-2082",
            "d": "M5301,-921.465C5394.09,-916.767 5527.95,-902.536 5637.6,-861.401 5689.5,-841.928 5741.67,-804.957 5775.58,-778.052"
        },
        {
            "source": "D11-1100",
            "target": "P13-1041",
            "d": "M5074.71,-920.599C4941.64,-915.632 4730.05,-901.496 4662.6,-861.401 4637.45,-846.454 4648.89,-823.962 4624.6,-807.661 4565.51,-768.01 4537.53,-789.297 4468.6,-771.661 4461.03,-769.724 4453.12,-767.623 4445.27,-765.486"
        },
        {
            "source": "D11-1100",
            "target": "W14-2619",
            "d": "M5300.01,-919.661C5406.34,-913.954 5555.95,-899.25 5599.6,-861.401 5649.89,-817.789 5600.16,-769.21 5642.6,-717.921 5655.86,-701.887 5674.37,-689.508 5692.84,-680.185"
        },
        {
            "source": "D11-1100",
            "target": "W15-5920",
            "d": "M5073.57,-922.551C4813.59,-920.089 4157.18,-909.228 3610.6,-861.401 3316.13,-835.635 3152.94,-988.076 2951.6,-771.661 2909.3,-726.192 2918.45,-647.775 2928.69,-602.13"
        },
        {
            "source": "D11-1100",
            "target": "W16-6315",
            "d": "M5196.7,-897.479C5207.94,-863.265 5225.6,-800.777 5225.6,-745.791 5225.6,-745.791 5225.6,-745.791 5225.6,-654.051 5225.6,-594.747 5272.15,-540.469 5307.26,-507.961"
        },
        {
            "source": "D11-1100",
            "target": "L16-1429",
            "d": "M5175.72,-897.134C5164.71,-873.421 5147.52,-837.682 5130.6,-807.661 5060.87,-683.97 5048.04,-648.714 4958.6,-538.441 4949.98,-527.815 4939.71,-517.078 4929.99,-507.599"
        },
        {
            "source": "D11-1100",
            "target": "K16-1016",
            "d": "M5298.5,-917.785C5391.19,-910.837 5513.66,-895.365 5548.6,-861.401 5644.35,-768.315 5624.31,-589.11 5610.3,-512.611"
        },
        {
            "source": "D11-1100",
            "target": "P17-1035",
            "d": "M5187.6,-897.109C5187.6,-862.489 5187.6,-799.559 5187.6,-745.791 5187.6,-745.791 5187.6,-745.791 5187.6,-654.051 5187.6,-606.571 5210.88,-479.984 5246.6,-448.701 5258.74,-438.066 5452.63,-412.599 5571.36,-397.867"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "W12-4906",
            "target": "P13-2062",
            "d": "M6139.6,-807.206C6139.6,-799.239 6139.6,-790.348 6139.6,-781.86"
        },
        {
            "source": "kunchukuttan-etal-2012-experiences",
            "target": "P13-4030",
            "d": "M3722.6,-807.206C3722.6,-799.239 3722.6,-790.348 3722.6,-781.86"
        },
        {
            "source": "kunchukuttan-etal-2012-experiences",
            "target": "W14-5148",
            "d": "M3649.22,-815.571C3626.85,-806.286 3604.72,-792.376 3591.6,-771.661 3578.81,-751.486 3582.45,-739.983 3591.6,-717.921 3596.33,-706.509 3604.31,-696.108 3612.99,-687.227"
        },
        {
            "source": "kunchukuttan-etal-2012-experiences",
            "target": "L16-1485",
            "d": "M3824.66,-830.151C3918.97,-824.623 4049.25,-810.055 4081.6,-771.661 4157.63,-681.404 4009.65,-560.045 3931.54,-505.382"
        },
        {
            "source": "C12-3013",
            "target": "W14-5136",
            "d": "M10616.6,-807.343C10616.6,-776.846 10616.6,-726.294 10616.6,-692.103"
        },
        {
            "source": "C12-3030",
            "target": "L18-1728",
            "d": "M4697.07,-813.736C4623.02,-779.369 4482.6,-707.426 4482.6,-656.051 4482.6,-656.051 4482.6,-656.051 4482.6,-474.571 4482.6,-410.629 4425.24,-357.515 4381.93,-326.529"
        },
        {
            "source": "C12-3030",
            "target": "2018.gwc-1.31",
            "d": "M4725.72,-808.105C4692.87,-754.12 4617.56,-623.096 4582.6,-502.441 4565.92,-444.878 4560.79,-375.05 4559.23,-333.441"
        },
        {
            "source": "C12-3033",
            "target": "W14-0126",
            "d": "M5377.45,-807.343C5366.72,-776.627 5348.88,-725.567 5336.93,-691.368"
        },
        {
            "source": "C12-2008",
            "target": "P13-1041",
            "d": "M4893.41,-818.327C4869.54,-814.785 4844.18,-811.057 4820.6,-807.661 4694.31,-789.475 4547.7,-769.21 4458.57,-756.986"
        },
        {
            "source": "C12-2008",
            "target": "L16-1429",
            "d": "M4992.67,-807.331C4988.09,-754.642 4973.92,-632.837 4935.6,-538.441 4931.66,-528.752 4926.16,-518.942 4920.5,-510.076"
        },
        {
            "source": "C12-2008",
            "target": "L16-1485",
            "d": "M4942.74,-809.969C4818.37,-754.38 4492.21,-614.063 4206.6,-538.441 4109.12,-512.632 4081.16,-523.718 3982.6,-502.441 3974.22,-500.632 3965.48,-498.54 3956.84,-496.345"
        },
        {
            "source": "C12-2008",
            "target": "C16-1047",
            "d": "M4997.58,-807.572C5004.8,-744.724 5023.23,-584.283 5031.48,-512.377"
        },
        {
            "source": "C12-2008",
            "target": "C16-1287",
            "d": "M4980.37,-807.624C4950.15,-753.906 4875.5,-627.945 4792.6,-538.441 4782.18,-527.196 4769.65,-516.262 4757.76,-506.792"
        },
        {
            "source": "C12-2008",
            "target": "N18-1053",
            "d": "M5005.22,-807.531C5039.15,-725.012 5144.52,-473.007 5175.6,-448.701 5240.2,-398.17 5304.56,-476.103 5356.6,-412.701 5375.16,-390.079 5365.56,-356.734 5353.43,-331.661"
        },
        {
            "source": "C12-2008",
            "target": "W19-0413",
            "d": "M5018.21,-808.047C5027.62,-797.251 5038.19,-784.269 5046.6,-771.661 5069.01,-738.05 5181.57,-473.324 5213.6,-448.701 5272.41,-403.479 5319.34,-459.874 5376.6,-412.701 5410.16,-385.049 5406.75,-365.538 5415.6,-322.96 5420.46,-299.576 5429.52,-288.626 5415.6,-269.22 5404.22,-253.363 5365.36,-239.136 5326.36,-228.335"
        },
        {
            "source": "C12-1113",
            "target": "S13-2082",
            "d": "M5890.94,-808.623C5879.17,-799.064 5865.58,-788.018 5853.09,-777.879"
        },
        {
            "source": "C12-1113",
            "target": "W14-2619",
            "d": "M5956.33,-808.655C5967.54,-798.581 5978.61,-785.939 5984.6,-771.661 5993.84,-749.636 5999.05,-736.934 5984.6,-717.921 5980.01,-711.887 5903.32,-691.463 5841.14,-675.714"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "W13-3611",
            "target": "W14-1708",
            "d": "M10884.4,-717.941C10879,-709.283 10872.8,-699.493 10867.1,-690.296"
        },
        {
            "source": "W13-3611",
            "target": "W15-5902",
            "d": "M10920,-718.43C10927,-707.756 10934,-694.829 10937.6,-681.921 10944,-658.898 10943.8,-651.241 10937.6,-628.181 10935,-618.652 10930.6,-609.082 10925.7,-600.412"
        },
        {
            "source": "P13-4030",
            "target": "L18-1548",
            "d": "M3735.13,-717.789C3739.94,-706.914 3745.12,-694.013 3748.6,-681.921 3763.01,-631.882 3767.6,-618.382 3767.6,-566.311 3767.6,-566.311 3767.6,-566.311 3767.6,-474.571 3767.6,-227.43 3469.23,-394.257 3232.6,-322.96 3226.66,-321.172 3220.49,-319.274 3214.33,-317.35"
        },
        {
            "source": "P13-2062",
            "target": "P14-2007",
            "d": "M6112.37,-718.648C6101.95,-709.02 6089.92,-697.911 6078.92,-687.749"
        },
        {
            "source": "P13-2062",
            "target": "K16-1016",
            "d": "M6056.7,-727.406C6000.57,-715.243 5932.29,-698.041 5907.6,-681.921 5881.98,-665.201 5890.1,-645.074 5864.6,-628.181 5815.33,-595.545 5788.66,-620.15 5736.6,-592.181 5696.44,-570.607 5657.83,-535.212 5632.45,-509.26"
        },
        {
            "source": "P13-2062",
            "target": "P18-1219",
            "d": "M6166.37,-718.614C6197.47,-686.496 6244.6,-627.71 6244.6,-566.311 6244.6,-566.311 6244.6,-566.311 6244.6,-474.571 6244.6,-424.524 6229.97,-368.264 6218.87,-332.763"
        },
        {
            "source": "P13-2062",
            "target": "2020.aacl-main.86",
            "d": "M6051.44,-728.942C6014.76,-719.712 5973.22,-705.026 5940.6,-681.921 5916.3,-664.713 5919.61,-651.097 5900.6,-628.181 5852.05,-569.661 5812.27,-573.258 5784.6,-502.441 5775.9,-480.195 5774.46,-470.326 5784.6,-448.701 5822.66,-367.534 6043.06,-211.829 6138.53,-147.031"
        },
        {
            "source": "P13-2096",
            "target": "W14-5148",
            "d": "M3468.34,-723.22C3504.5,-710.108 3551.45,-693.084 3589.04,-679.456"
        },
        {
            "source": "P13-2096",
            "target": "W15-5908",
            "d": "M3433.64,-718.179C3455.56,-693.54 3490.84,-656.029 3525.6,-628.181 3541.08,-615.777 3559.19,-603.814 3575.66,-593.768"
        },
        {
            "source": "P13-2096",
            "target": "L16-1485",
            "d": "M3455.64,-720.842C3474.13,-710.21 3495.31,-696.667 3512.6,-681.921 3536.52,-661.518 3531.76,-644.552 3558.6,-628.181 3617.87,-592.03 3646.81,-619.572 3710.6,-592.181 3762.05,-570.087 3815.04,-533.264 3849.46,-507.117"
        },
        {
            "source": "P13-2149",
            "target": "P14-2007",
            "d": "M6289.47,-726.035C6240.42,-712.017 6171.8,-692.405 6119.67,-677.507"
        },
        {
            "source": "P13-2149",
            "target": "P15-2124",
            "d": "M6382,-719.485C6421.72,-687.734 6491.3,-632.106 6534.78,-597.342"
        },
        {
            "source": "P13-2149",
            "target": "K16-1016",
            "d": "M6316.3,-720.096C6276.85,-694.483 6210.38,-653.887 6148.6,-628.181 5992.47,-563.224 5799.86,-517.147 5690.14,-493.88"
        },
        {
            "source": "P13-1041",
            "target": "W14-5148",
            "d": "M4303.29,-734.895C4169.9,-718.707 3887.92,-684.488 3743.29,-666.936"
        },
        {
            "source": "P13-1041",
            "target": "W14-2619",
            "d": "M4454.63,-741.43C4642.54,-735.36 5134.91,-717.154 5544.6,-681.921 5585.02,-678.444 5629.54,-673.358 5667.68,-668.613"
        },
        {
            "source": "P13-1041",
            "target": "D15-1300",
            "d": "M4311.39,-729.574C4197.68,-705.279 3955.46,-655.733 3748.6,-628.181 3556.74,-602.628 3506.45,-617.798 3314.6,-592.181 3296.61,-589.779 3277.43,-586.628 3259.22,-583.354"
        },
        {
            "source": "P13-1041",
            "target": "L16-1485",
            "d": "M4357.1,-718.682C4321.52,-674.732 4241.49,-584.193 4152.6,-538.441 4083.93,-503.097 4057.66,-520.594 3982.6,-502.441 3974.48,-500.477 3965.99,-498.32 3957.58,-496.115"
        },
        {
            "source": "P13-1041",
            "target": "K16-1016",
            "d": "M4442.32,-730.022C4567.83,-703.675 4850.26,-644.124 5087.6,-592.181 5240.07,-558.812 5417.63,-518.668 5519.4,-495.536"
        },
        {
            "source": "N13-1088",
            "target": "W14-2623",
            "d": "M5485.88,-717.941C5484.4,-709.802 5482.73,-700.662 5481.14,-691.956"
        },
        {
            "source": "N13-1088",
            "target": "W14-0126",
            "d": "M5451.26,-722.998C5428.03,-710.722 5398.43,-695.075 5373.68,-681.996"
        },
        {
            "source": "N13-1088",
            "target": "P14-2007",
            "d": "M5551.58,-733.185C5579.33,-728.416 5612.61,-722.781 5642.6,-717.921 5748.55,-700.753 5870.4,-682.16 5951.92,-669.887"
        },
        {
            "source": "N13-1088",
            "target": "W16-1904",
            "d": "M5523.42,-721.181C5557.28,-697.866 5611.62,-660.461 5658.6,-628.181 5720.56,-585.604 5792.35,-536.373 5836.88,-505.851"
        },
        {
            "source": "N13-1088",
            "target": "K16-1016",
            "d": "M5515.97,-719.829C5526.11,-709.073 5537.13,-695.711 5544.6,-681.921 5574.43,-626.851 5590.15,-554.96 5597.39,-512.481"
        },
        {
            "source": "I13-2006",
            "target": "W15-5945",
            "d": "M2480.76,-717.814C2477.25,-706.831 2473.39,-693.845 2470.6,-681.921 2464.39,-655.475 2459.49,-625.159 2456.22,-602.208"
        },
        {
            "source": "I13-1076",
            "target": "W15-5920",
            "d": "M3039.25,-718.075C3025.77,-694.338 3004.72,-658.312 2984.6,-628.181 2978.34,-618.806 2971.22,-608.903 2964.48,-599.835"
        },
        {
            "source": "I13-1076",
            "target": "D15-1300",
            "d": "M3070.93,-718.362C3092.08,-687.348 3127.9,-634.804 3151.4,-600.331"
        },
        {
            "source": "I13-1076",
            "target": "K16-1016",
            "d": "M3138.72,-733.694C3187.99,-728.208 3251.21,-721.776 3307.6,-717.921 3763.26,-686.765 3880.72,-732.814 4334.6,-681.921 4673.16,-643.958 4750.45,-587.463 5087.6,-538.441 5245.41,-515.494 5286.5,-523.335 5444.6,-502.441 5466.58,-499.535 5490.21,-495.928 5512.32,-492.339"
        },
        {
            "source": "I13-1076",
            "target": "2016.gwc-1.54",
            "d": "M2967.05,-734.453C2894.97,-724.979 2799.74,-707.967 2771.6,-681.921 2758.96,-670.222 2732.21,-568.206 2718.35,-512.508"
        },
        {
            "source": "I13-1076",
            "target": "P17-1035",
            "d": "M3139.96,-734.561C3189.09,-729.42 3251.76,-723.043 3307.6,-717.921 3503.43,-699.957 3553.09,-703.13 3748.6,-681.921 4220.79,-630.696 4336.62,-599.883 4807.6,-538.441 4935.07,-521.811 4970.85,-537.285 5094.6,-502.441 5150.46,-486.711 5157.83,-464.769 5213.6,-448.701 5332.03,-414.574 5473.62,-399.047 5565.69,-392.134"
        },
        {
            "source": "I13-1076",
            "target": "D17-1058",
            "d": "M3053.89,-717.832C3054.59,-655.265 3056.37,-495.977 3057.19,-423.608"
        },
        {
            "source": "I13-1076",
            "target": "P18-1089",
            "d": "M3000.25,-722.694C2949.26,-699.298 2875.43,-656.2 2842.6,-592.181 2796.63,-502.54 2870.11,-386.963 2913.45,-330.765"
        },
        {
            "source": "I13-1076",
            "target": "2020.lrec-1.613",
            "d": "M2970.56,-732.47C2934.1,-723.835 2892.99,-708.64 2863.6,-681.921 2765.28,-592.539 2815.6,-519.706 2815.6,-386.831 2815.6,-386.831 2815.6,-386.831 2815.6,-295.09 2815.6,-245.59 2805.21,-189.198 2797.32,-153.519"
        },
        {
            "source": "I13-1131",
            "target": "L16-1485",
            "d": "M3976.92,-717.682C3959.06,-669.466 3920.88,-566.417 3900.71,-511.969"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "W14-5105",
            "target": "N19-1387",
            "d": "M2060.83,-629.383C2050.71,-618.904 2040.15,-605.933 2033.6,-592.181 2011.19,-545.176 2014.6,-528.642 2014.6,-476.571 2014.6,-476.571 2014.6,-476.571 2014.6,-384.831 2014.6,-298.33 1914.94,-250.795 1839.5,-227.204"
        },
        {
            "source": "W14-5115",
            "target": "2016.gwc-1.46",
            "d": "M8398.95,-628.622C8367.27,-597.213 8313.33,-543.725 8278.59,-509.285"
        },
        {
            "source": "W14-5115",
            "target": "W19-7509",
            "d": "M8469.02,-630.067C8516.67,-600.73 8585.6,-546.253 8585.6,-476.571 8585.6,-476.571 8585.6,-476.571 8585.6,-384.831 8585.6,-332.759 8604.18,-305.261 8566.6,-269.22 8538.48,-242.258 8310.83,-223.24 8169.66,-213.909"
        },
        {
            "source": "W14-3308",
            "target": "W15-5916",
            "d": "M1025,-631.91C1006.45,-620.987 983.817,-607.66 963.809,-595.878"
        },
        {
            "source": "W14-2623",
            "target": "K16-1016",
            "d": "M5489.76,-628.595C5504.29,-604.726 5527.25,-568.334 5549.6,-538.441 5556.8,-528.803 5565.07,-518.737 5572.9,-509.593"
        },
        {
            "source": "W14-1708",
            "target": "W15-5902",
            "d": "M10861.8,-628.673C10867.4,-619.928 10873.7,-609.984 10879.7,-600.645"
        },
        {
            "source": "W14-0130",
            "target": "kunchukuttan-etal-2014-shata",
            "d": "M8658.34,-675.719C8623.73,-685.145 8580.81,-695.233 8541.6,-699.921 8434.1,-712.771 853.562,-716.643 746.597,-699.921 723.093,-696.246 698.146,-689.3 675.922,-681.95"
        },
        {
            "source": "W14-0130",
            "target": "W15-5910",
            "d": "M8806.61,-637.134C8872.08,-623.294 8965.18,-603.61 9036.62,-588.506"
        },
        {
            "source": "W14-0130",
            "target": "2016.gwc-1.22",
            "d": "M8726.6,-627.863C8726.6,-597.366 8726.6,-546.814 8726.6,-512.622"
        },
        {
            "source": "W14-0130",
            "target": "L18-1548",
            "d": "M8640.47,-639.13C8611.95,-634.796 8579.99,-630.585 8550.6,-628.181 8088.96,-590.423 6929.61,-604.655 6466.6,-592.181 6330.39,-588.511 4135.44,-567.282 4015.6,-502.441 3989.87,-488.52 4000.11,-467.374 3977.6,-448.701 3959.16,-433.411 3804.82,-364.826 3781.6,-358.96 3544.52,-299.075 3472.84,-368.495 3232.6,-322.96 3225.87,-321.686 3218.93,-320.025 3212.06,-318.159"
        },
        {
            "source": "W14-0130",
            "target": "2021.naacl-main.322",
            "d": "M8831.46,-649.599C8988.5,-641.93 9270.04,-623.911 9303.6,-592.181 9341.44,-556.407 9322.6,-528.642 9322.6,-476.571 9322.6,-476.571 9322.6,-476.571 9322.6,-205.35 9322.6,-156.623 9322.6,-100.371 9322.6,-64.4848"
        },
        {
            "source": "W14-0145",
            "target": "W19-7509",
            "d": "M8056.66,-628.104C8056.83,-552.314 8057.32,-331.12 8057.52,-243.602"
        },
        {
            "source": "W14-0147",
            "target": "W19-7512",
            "d": "M11123.6,-627.888C11123.6,-593.269 11123.6,-530.339 11123.6,-476.571 11123.6,-476.571 11123.6,-476.571 11123.6,-384.831 11123.6,-336.103 11123.6,-279.851 11123.6,-243.965"
        },
        {
            "source": "P14-2007",
            "target": "W16-1904",
            "d": "M6030.8,-628.234C6016.76,-603.443 5993.31,-565.822 5966.6,-538.441 5954.81,-526.356 5940.39,-515.038 5926.71,-505.471"
        },
        {
            "source": "P14-2007",
            "target": "K16-1016",
            "d": "M5982.6,-634.52C5947.17,-622.975 5902.07,-607.628 5862.6,-592.181 5824.07,-577.101 5724.64,-532.256 5660.51,-503.055"
        },
        {
            "source": "P14-2007",
            "target": "P17-1035",
            "d": "M5998.45,-631.287C5944.86,-603.931 5854.84,-555.091 5784.6,-502.441 5750.95,-477.223 5716.53,-443.766 5692.87,-419.335"
        },
        {
            "source": "P14-2007",
            "target": "P18-1219",
            "d": "M6077.89,-629.843C6111.99,-602.957 6163.32,-556.19 6187.6,-502.441 6212.46,-447.393 6212.8,-375.719 6210.15,-333.238"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W14-5126",
            "d": "M729.144,-655.051C731.456,-655.051 733.768,-655.051 736.079,-655.051"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W14-3308",
            "d": "M666.248,-678.671C690.731,-687.172 719.615,-695.703 746.597,-699.921 845.837,-715.435 874.827,-722.942 972.597,-699.921 987.51,-696.41 1002.77,-690.034 1016.4,-683.169"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W15-5916",
            "d": "M678.963,-633.369C724.737,-620.287 784.017,-603.345 831.54,-589.763"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "N15-3017",
            "d": "M614.258,-628.2C616.696,-619.974 619.437,-610.723 622.042,-601.932"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "N15-1125",
            "d": "M693.1,-635.975C707.909,-633.166 723.17,-630.448 737.597,-628.181 873.312,-606.857 909.796,-618.669 1044.6,-592.181 1052.43,-590.642 1060.57,-588.78 1068.63,-586.768"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "L16-1098",
            "d": "M505.007,-639.941C462.001,-630.827 412.684,-616.028 372.597,-592.181 337.728,-571.437 306.946,-536.367 287.065,-510.28"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W18-1207",
            "d": "M559.295,-630.043C515.954,-604.972 454.947,-561.107 430.597,-502.441 405.587,-442.184 428.636,-415.262 461.597,-358.96 467.607,-348.696 475.478,-338.603 483.38,-329.642"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "L18-1413",
            "d": "M501.334,-641.24C442.871,-631.926 369.635,-616.569 307.597,-592.181 236.006,-564.037 197.342,-569.469 159.597,-502.441 129.312,-448.66 153.789,-375.062 173.365,-332.145"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "N19-1387",
            "d": "M673.527,-632.435C697.55,-622.603 723.814,-609.278 744.597,-592.181 767.188,-573.597 757.3,-553.128 782.597,-538.441 854.231,-496.851 1088.5,-553.659 1153.6,-502.441 1239.83,-434.592 1149.09,-337.981 1234.6,-269.22 1250.79,-256.197 1508.32,-230.374 1655.48,-216.566"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "W15-5920",
            "target": "P18-1089",
            "d": "M2938.89,-538.202C2939.43,-490.309 2940.57,-388.316 2941.19,-333.59"
        },
        {
            "source": "W15-5944",
            "target": "I17-2048",
            "d": "M1548.93,-544.554C1444.62,-511.137 1234.12,-443.701 1125.26,-408.828"
        },
        {
            "source": "W15-3912",
            "target": "Q18-1022",
            "d": "M1782.7,-546.769C1748.7,-534.488 1709.4,-517.624 1699.6,-502.441 1666.03,-450.451 1690.21,-375.629 1709.69,-332.121"
        },
        {
            "source": "W15-2905",
            "target": "W16-5001",
            "d": "M7285.72,-543.406C7314.49,-530.912 7351.22,-514.956 7381.59,-501.769"
        },
        {
            "source": "W15-2905",
            "target": "K16-1015",
            "d": "M7237.6,-537.986C7237.6,-530.019 7237.6,-521.128 7237.6,-512.64"
        },
        {
            "source": "W15-2905",
            "target": "D16-1104",
            "d": "M7156.82,-560.331C7050.63,-553.9 6858.8,-538.131 6698.6,-502.441 6692.55,-501.092 6686.3,-499.444 6680.11,-497.639"
        },
        {
            "source": "W15-2905",
            "target": "L18-1424",
            "d": "M7318.85,-561.115C7425.09,-553.74 7591.37,-529.353 7534.6,-448.701 7494.39,-391.575 7303.41,-340.177 7192.3,-314.451"
        },
        {
            "source": "P15-2124",
            "target": "W15-2905",
            "d": "M6671.19,-565.311C6829.14,-565.311 6987.1,-565.311 7145.05,-565.311"
        },
        {
            "source": "P15-2124",
            "target": "W16-5001",
            "d": "M6670.79,-562.29C6817.7,-557.937 7104.67,-544.36 7344.6,-502.441 7352.71,-501.024 7361.13,-499.225 7369.45,-497.237"
        },
        {
            "source": "P15-2124",
            "target": "W16-2111",
            "d": "M6533.13,-540.798C6513.5,-529.39 6489.8,-515.622 6469.4,-503.764"
        },
        {
            "source": "P15-2124",
            "target": "U16-1013",
            "d": "M6625.28,-542.509C6655.7,-529.677 6694.2,-513.438 6725.38,-500.287"
        },
        {
            "source": "P15-2124",
            "target": "P16-1104",
            "d": "M6493.29,-550.019C6404.84,-534.276 6262.94,-509.023 6170.74,-492.614"
        },
        {
            "source": "P15-2124",
            "target": "K16-1015",
            "d": "M6666.84,-557.164C6776.96,-547.961 6965.43,-529.821 7125.6,-502.441 7135.81,-500.695 7146.48,-498.594 7157.02,-496.353"
        },
        {
            "source": "P15-2124",
            "target": "K16-1016",
            "d": "M6478.68,-558.804C6330.32,-549.76 6034.53,-529.975 5784.6,-502.441 5755.65,-499.252 5724.22,-495.05 5695.75,-490.962"
        },
        {
            "source": "P15-2124",
            "target": "D16-1104",
            "d": "M6587.74,-538.46C6592.47,-529.816 6597.82,-520.043 6602.84,-510.858"
        },
        {
            "source": "P15-2124",
            "target": "W17-7518",
            "d": "M6560.19,-538.554C6544.55,-509.093 6519.12,-463.07 6505.6,-448.701 6492.91,-435.228 6476.78,-423.149 6461.58,-413.317"
        },
        {
            "source": "P15-2124",
            "target": "P17-1035",
            "d": "M6493.3,-549.989C6439.69,-539.387 6368.02,-523.2 6306.6,-502.441 6251.62,-483.859 6243.52,-464.221 6187.6,-448.701 6042.39,-408.396 5867.46,-394.341 5760.69,-389.444"
        },
        {
            "source": "P15-2124",
            "target": "L18-1424",
            "d": "M6668.78,-559.359C6810.52,-551.043 7063.61,-532.201 7092.6,-502.441 7136.14,-457.731 7129.2,-378.697 7120.33,-332.813"
        },
        {
            "source": "P15-2124",
            "target": "W19-1309",
            "d": "M6665.9,-556.263C6740.79,-547.716 6838.13,-531.338 6864.6,-502.441 6930.03,-431.011 6915.5,-305.217 6903.19,-243.526"
        },
        {
            "source": "P15-2124",
            "target": "2020.aacl-main.31",
            "d": "M6489.29,-551.75C6430.85,-541.297 6360.01,-524.56 6339.6,-502.441 6268.17,-425.038 6362.2,-352.404 6297.6,-269.22 6264.16,-226.168 6125.71,-172.505 6038.5,-141.939"
        },
        {
            "source": "N15-3017",
            "target": "W15-3912",
            "d": "M694.053,-586.918C723.212,-596.062 758.782,-605.639 791.597,-610.181 892.304,-624.118 1606.33,-626.998 1706.6,-610.181 1731.11,-606.07 1757.18,-597.883 1779.49,-589.615"
        },
        {
            "source": "N15-3017",
            "target": "K16-1027",
            "d": "M713.702,-548.552C735.96,-544.712 760.147,-540.982 782.597,-538.441 1073.62,-505.496 1148.25,-520.183 1440.6,-502.441 1528.95,-497.078 1628.58,-490.423 1704.75,-485.205"
        },
        {
            "source": "N15-3017",
            "target": "D16-1196",
            "d": "M688.463,-542.622C719.871,-530.429 759.27,-515.133 792.242,-502.332"
        },
        {
            "source": "N15-3017",
            "target": "W17-4102",
            "d": "M535.602,-555.882C500.831,-547.624 464.975,-531.933 443.597,-502.441 429.58,-483.102 431.273,-469.16 443.597,-448.701 453.947,-431.521 470.84,-418.864 488.612,-409.642"
        },
        {
            "source": "N15-3017",
            "target": "Q18-1022",
            "d": "M714.953,-548.978C736.886,-545.204 760.598,-541.401 782.597,-538.441 946.781,-516.348 991.932,-538.617 1153.6,-502.441 1347.9,-458.96 1566.61,-368.488 1668.92,-323.571"
        },
        {
            "source": "N15-3017",
            "target": "P18-2064",
            "d": "M661.148,-539.345C673.127,-528.419 686.951,-515.201 698.597,-502.441 705.77,-494.582 788.983,-387.148 832.474,-330.901"
        },
        {
            "source": "N15-3017",
            "target": "2021.emnlp-main.675",
            "d": "M648.706,-538.671C654.849,-527.851 661.381,-514.894 665.597,-502.441 682.295,-453.119 684.597,-438.902 684.597,-386.831 684.597,-386.831 684.597,-386.831 684.597,-205.35 684.597,-153.279 682.603,-138.956 665.597,-89.7401 662.359,-80.3686 657.772,-70.7415 653.013,-61.9564"
        },
        {
            "source": "N15-1125",
            "target": "W15-5946",
            "d": "M1221.25,-565.311C1236.12,-565.311 1250.98,-565.311 1265.85,-565.311"
        },
        {
            "source": "N15-1125",
            "target": "I17-2048",
            "d": "M1149.79,-538.329C1159.41,-513.758 1168.89,-476.595 1153.6,-448.701 1145.45,-433.834 1132.04,-422.024 1117.75,-412.878"
        },
        {
            "source": "N15-1132",
            "target": "L16-1485",
            "d": "M8756.14,-560.502C8590.08,-554.717 8259.33,-543.823 7978.6,-538.441 7538.33,-530 4453.56,-548.266 4015.6,-502.441 3998.07,-500.607 3979.38,-497.501 3961.86,-494.064"
        },
        {
            "source": "N15-1132",
            "target": "L18-1049",
            "d": "M8863.09,-538.202C8863.98,-490.309 8865.89,-388.316 8866.91,-333.59"
        },
        {
            "source": "N15-1132",
            "target": "2018.gwc-1.34",
            "d": "M8881.39,-538.689C8917.13,-489.958 8994.86,-383.983 9034.42,-330.05"
        },
        {
            "source": "D15-1300",
            "target": "D17-1058",
            "d": "M3157.58,-538.504C3137.16,-507.527 3102.85,-455.471 3080.21,-421.134"
        },
        {
            "source": "D15-1300",
            "target": "W18-3705",
            "d": "M3189.46,-538.689C3217.47,-490.405 3278.07,-385.925 3309.61,-331.549"
        },
        {
            "source": "D15-1300",
            "target": "P18-1089",
            "d": "M3181.58,-538.236C3191.31,-495.619 3203.37,-410.628 3161.6,-358.96 3152.64,-347.878 3084.43,-329.745 3026.3,-315.967"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "W16-6331",
            "target": "K18-1012",
            "d": "M4116.04,-448.383C4116.56,-417.886 4117.41,-367.334 4117.99,-333.142"
        },
        {
            "source": "W16-6331",
            "target": "2020.findings-emnlp.206",
            "d": "M4083.91,-450.331C4045.96,-422.789 3979.38,-379.064 3914.6,-358.96 3818.79,-329.228 3532.65,-396.671 3464.6,-322.96 3421.76,-276.559 3450.82,-197.257 3473.33,-151.98"
        },
        {
            "source": "W16-6336",
            "target": "L18-1489",
            "d": "M7653.07,-448.933C7635,-422.827 7603.76,-382.886 7567.6,-358.96 7539.3,-340.242 7504.84,-326.719 7473.31,-317.185"
        },
        {
            "source": "W16-4811",
            "target": "W17-4102",
            "d": "M1007.44,-456.724C996.913,-453.79 985.972,-450.971 975.597,-448.701 866.188,-424.758 738.506,-407.258 653.964,-397.098"
        },
        {
            "source": "W16-4811",
            "target": "I17-2048",
            "d": "M1062.2,-448.246C1061.47,-440.279 1060.66,-431.388 1059.89,-422.9"
        },
        {
            "source": "W16-4206",
            "target": "W16-6325",
            "d": "M11308.4,-475.571C11311,-475.571 11313.6,-475.571 11316.2,-475.571"
        },
        {
            "source": "W16-1904",
            "target": "P18-1219",
            "d": "M5921.34,-451.912C5981.33,-419.357 6091.74,-359.429 6156.19,-324.448"
        },
        {
            "source": "W16-0415",
            "target": "W16-5001",
            "d": "M7050.95,-496.694C7077.2,-505.977 7109.57,-515.81 7139.6,-520.441 7225.69,-533.719 7250.29,-538.059 7335.6,-520.441 7352.98,-516.85 7371.04,-510.211 7387.13,-503.121"
        },
        {
            "source": "P16-1104",
            "target": "P17-1035",
            "d": "M6010,-456.885C5998.2,-454.06 5986.07,-451.236 5974.6,-448.701 5897.57,-431.672 5809.35,-414.382 5746.42,-402.425"
        },
        {
            "source": "N16-4006",
            "target": "W16-4811",
            "d": "M621.3,-495.891C703.362,-517.911 846.562,-546.614 966.597,-520.441 982.949,-516.875 999.837,-510.324 1014.89,-503.309"
        },
        {
            "source": "N16-4006",
            "target": "D16-1196",
            "d": "M656.426,-475.571C684.325,-475.571 712.225,-475.571 740.125,-475.571"
        },
        {
            "source": "N16-4006",
            "target": "W17-4102",
            "d": "M556.398,-448.246C556.942,-440.279 557.55,-431.388 558.131,-422.9"
        },
        {
            "source": "N16-4006",
            "target": "I17-2048",
            "d": "M637.774,-460.033C730.257,-443.869 878.839,-417.899 971.922,-401.63"
        },
        {
            "source": "N16-4006",
            "target": "2021.emnlp-main.675",
            "d": "M496.392,-453.362C435.476,-426.969 348.597,-375.571 348.597,-297.09 348.597,-297.09 348.597,-297.09 348.597,-205.35 348.597,-105.456 476.717,-59.6395 560.547,-40.3531"
        },
        {
            "source": "L16-1098",
            "target": "L18-1413",
            "d": "M253.128,-448.763C240.704,-418.138 219.921,-366.908 205.99,-332.568"
        },
        {
            "source": "L16-1429",
            "target": "C16-1047",
            "d": "M4967.24,-475.571C4969.85,-475.571 4972.46,-475.571 4975.07,-475.571"
        },
        {
            "source": "L16-1429",
            "target": "W19-0413",
            "d": "M4945.15,-456.18C4970.91,-445.452 5002.23,-430.519 5027.6,-412.701 5099.78,-362.005 5169.13,-284.588 5205.75,-240.722"
        },
        {
            "source": "K16-1015",
            "target": "W16-5001",
            "d": "M7335.44,-475.571C7337.9,-475.571 7340.36,-475.571 7342.82,-475.571"
        },
        {
            "source": "K16-1016",
            "target": "P17-1035",
            "d": "M5620.27,-448.72C5626.31,-439.896 5633.15,-429.895 5639.54,-420.545"
        },
        {
            "source": "K16-1027",
            "target": "Q18-1022",
            "d": "M1813.91,-448.763C1796.33,-417.875 1766.83,-366.027 1747.29,-331.688"
        },
        {
            "source": "D16-1104",
            "target": "U16-1013",
            "d": "M6690.35,-475.571C6692.8,-475.571 6695.26,-475.571 6697.72,-475.571"
        },
        {
            "source": "D16-1104",
            "target": "W17-2338",
            "d": "M6611.29,-448.72C6607.97,-440.405 6604.23,-431.044 6600.69,-422.168"
        },
        {
            "source": "D16-1104",
            "target": "I17-2006",
            "d": "M6660.25,-453.107C6681.13,-441.514 6707.14,-427.067 6729.55,-414.623"
        },
        {
            "source": "D16-1104",
            "target": "L18-1424",
            "d": "M6671.44,-456.828C6680.42,-453.916 6689.74,-451.078 6698.6,-448.701 6776.13,-427.878 6799.6,-438.568 6875.6,-412.701 6943.22,-389.684 7016.4,-351.579 7063.05,-325.45"
        },
        {
            "source": "D16-1104",
            "target": "C18-1155",
            "d": "M6670.5,-456.648C6679.75,-453.687 6689.4,-450.877 6698.6,-448.701 6809.55,-422.434 6839.88,-429.882 6952.6,-412.701 7197.45,-375.378 7259.89,-372.302 7502.6,-322.96 7511.32,-321.187 7520.42,-319.192 7529.45,-317.118"
        },
        {
            "source": "D16-1104",
            "target": "2018.gwc-1.31",
            "d": "M6565.93,-459.814C6546.93,-455.405 6525.47,-451.108 6505.6,-448.701 6414.13,-437.619 4928.95,-455.94 4847.6,-412.701 4821.77,-398.972 4833.77,-375.441 4809.6,-358.96 4769.53,-331.634 4718.56,-316.353 4673.03,-307.818"
        },
        {
            "source": "D16-1196",
            "target": "W16-4811",
            "d": "M966.828,-475.571C969.298,-475.571 971.769,-475.571 974.239,-475.571"
        },
        {
            "source": "D16-1196",
            "target": "W17-4102",
            "d": "M790.604,-454.551C743.436,-440.664 680.707,-422.194 632.674,-408.052"
        },
        {
            "source": "D16-1196",
            "target": "I17-2048",
            "d": "M909.845,-451.861C937.023,-439.818 970.514,-424.977 998.637,-412.515"
        },
        {
            "source": "D16-1196",
            "target": "P18-2064",
            "d": "M858.597,-448.383C858.597,-417.886 858.597,-367.334 858.597,-333.142"
        },
        {
            "source": "C16-1047",
            "target": "W17-5229",
            "d": "M5072.35,-457.236C5099.88,-444.246 5137.96,-426.285 5168.76,-411.756"
        },
        {
            "source": "C16-1047",
            "target": "D17-1057",
            "d": "M4998.56,-457.324C4991.06,-454.194 4983.15,-451.15 4975.6,-448.701 4905.85,-426.086 4884.6,-434.513 4814.6,-412.701 4809.31,-411.052 4803.84,-409.154 4798.43,-407.152"
        },
        {
            "source": "C16-1047",
            "target": "N18-1053",
            "d": "M5073.34,-457.631C5081.85,-454.288 5090.93,-451.081 5099.6,-448.701 5150.54,-434.707 5300.18,-452.787 5334.6,-412.701 5353.1,-391.147 5350.54,-358.099 5344.54,-332.848"
        },
        {
            "source": "C16-1047",
            "target": "W19-0413",
            "d": "M5073.38,-457.789C5081.89,-454.437 5090.96,-451.187 5099.6,-448.701 5187.23,-423.477 5243.04,-482.612 5301.6,-412.701 5316.93,-394.39 5312.68,-380.117 5301.6,-358.96 5288.55,-334.062 5264.42,-346.841 5249.6,-322.96 5234.87,-299.239 5231.06,-267.608 5230.69,-243.428"
        },
        {
            "source": "C16-1047",
            "target": "2020.lrec-1.621",
            "d": "M5036.45,-448.607C5039.28,-394.785 5051.41,-268.639 5103.6,-179.48 5110.15,-168.285 5119.4,-157.962 5128.96,-149.086"
        },
        {
            "source": "C16-1287",
            "target": "D17-1057",
            "d": "M4725.91,-448.72C4729.29,-440.256 4733.09,-430.71 4736.69,-421.693"
        },
        {
            "source": "C16-1287",
            "target": "N18-1053",
            "d": "M4780.76,-456.86C4792.33,-453.962 4804.29,-451.119 4815.6,-448.701 4909.06,-428.714 4936.42,-441.341 5027.6,-412.701 5081.04,-395.914 5089.51,-379.575 5141.6,-358.96 5182.36,-342.828 5229.16,-327.614 5266.24,-316.31"
        },
        {
            "source": "C16-1287",
            "target": "2020.lrec-1.201",
            "d": "M4685.02,-450.001C4674.56,-439.713 4664.12,-426.85 4658.6,-412.701 4649.91,-390.452 4652.36,-382.017 4658.6,-358.96 4663.43,-341.079 4674.76,-340.842 4679.6,-322.96 4685.83,-299.904 4685.22,-292.434 4679.6,-269.22 4669.15,-226.071 4644.17,-181.442 4625.27,-151.878"
        },
        {
            "source": "C16-1287",
            "target": "2020.icon-main.62",
            "d": "M4703.1,-448.708C4698.52,-437.854 4693.89,-424.931 4691.6,-412.701 4687.2,-389.224 4685.32,-382.006 4691.6,-358.96 4713.8,-277.437 4771.31,-195.366 4805.95,-151.066"
        },
        {
            "source": "2016.gwc-1.23",
            "target": "2018.gwc-1.37",
            "d": "M8457.8,-452.272C8409.68,-420.524 8321.32,-362.226 8267.67,-326.829"
        },
        {
            "source": "2016.gwc-1.23",
            "target": "2018.gwc-1.49",
            "d": "M8485.34,-448.763C8477.96,-418.269 8465.62,-367.347 8457.3,-333.011"
        },
        {
            "source": "2016.gwc-1.46",
            "target": "2018.gwc-1.37",
            "d": "M8242.2,-448.383C8238.25,-417.886 8231.7,-367.334 8227.27,-333.142"
        },
        {
            "source": "2016.gwc-1.46",
            "target": "W19-7509",
            "d": "M8219.73,-448.854C8191.56,-419.955 8146.31,-370.802 8114.6,-322.96 8097.65,-297.386 8082.5,-266.043 8072.09,-242.447"
        },
        {
            "source": "2016.gwc-1.57",
            "target": "L18-1728",
            "d": "M4355.06,-448.383C4350.93,-417.886 4344.1,-367.334 4339.47,-333.142"
        },
        {
            "source": "2016.gwc-1.57",
            "target": "2018.gwc-1.31",
            "d": "M4386.83,-449.518C4422.31,-418.029 4483.28,-363.924 4522.23,-329.365"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "W17-7518",
            "target": "W18-0522",
            "d": "M6413.6,-358.506C6413.6,-350.539 6413.6,-341.648 6413.6,-333.159"
        },
        {
            "source": "W17-7531",
            "target": "W17-7531",
            "d": "M7991.06,-402.24C8012.88,-401.484 8029.59,-396.014 8029.59,-385.831 8029.59,-377.318 8017.91,-372.099 8001.34,-370.174"
        },
        {
            "source": "W17-7531",
            "target": "W19-7509",
            "d": "M7933.37,-359.402C7958.89,-328.167 8002.25,-275.096 8030.4,-240.639"
        },
        {
            "source": "W17-4102",
            "target": "I17-2048",
            "d": "M650.691,-385.831C753.245,-385.831 855.799,-385.831 958.353,-385.831"
        },
        {
            "source": "W17-4102",
            "target": "W18-1207",
            "d": "M547.634,-358.98C543.382,-350.501 538.585,-340.936 534.056,-331.905"
        },
        {
            "source": "E17-1109",
            "target": "W17-5229",
            "d": "M5018.3,-385.831C5058.9,-385.831 5099.49,-385.831 5140.09,-385.831"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "Y18-3012",
            "target": "2021.wat-1.29",
            "d": "M11267.6,-268.982C11267.6,-221.088 11267.6,-119.096 11267.6,-64.3697"
        },
        {
            "source": "W18-3705",
            "target": "2020.bea-1.8",
            "d": "M3329.6,-268.903C3329.6,-238.405 3329.6,-187.854 3329.6,-153.662"
        },
        {
            "source": "P18-2011",
            "target": "W19-2404",
            "d": "M11564.1,-270.182C11547.6,-259.742 11528.2,-247.527 11511,-236.657"
        },
        {
            "source": "P18-2011",
            "target": "N19-2017",
            "d": "M11622.5,-269.24C11628.9,-260.416 11636.2,-250.415 11643,-241.065"
        },
        {
            "source": "P18-2064",
            "target": "2020.lrec-1.613",
            "d": "M933.667,-281.812C1073.99,-257.51 1385.81,-205.949 1650.6,-179.48 2014.04,-143.15 2445.07,-127.043 2657.23,-120.885"
        },
        {
            "source": "P18-1089",
            "target": "2020.lrec-1.613",
            "d": "M2919.68,-269.662C2892.83,-238.516 2847.26,-185.658 2817.55,-151.193"
        },
        {
            "source": "P18-1219",
            "target": "2020.icon-main.23",
            "d": "M6233.16,-270.413C6267.03,-238.934 6325.69,-184.427 6363.02,-149.737"
        },
        {
            "source": "P18-1219",
            "target": "2020.aacl-main.86",
            "d": "M6203.06,-268.903C6198.93,-238.405 6192.1,-187.854 6187.47,-153.662"
        },
        {
            "source": "L18-1278",
            "target": "K18-1012",
            "d": "M3795.59,-319.155C3810.29,-327.8 3828.12,-336.584 3845.6,-340.96 3910.27,-357.15 3929.95,-352.561 3995.6,-340.96 4016.18,-337.323 4037.85,-330.509 4057.17,-323.269"
        },
        {
            "source": "L18-1278",
            "target": "2020.aacl-main.90",
            "d": "M3787.9,-271.155C3822.74,-239.442 3884.26,-183.453 3922.59,-148.559"
        },
        {
            "source": "L18-1440",
            "target": "K18-1012",
            "d": "M3626.14,-319.574C3647.14,-328.112 3672.07,-336.705 3695.6,-340.96 3826.8,-364.688 3864.3,-364.161 3995.6,-340.96 4016.18,-337.323 4037.85,-330.509 4057.17,-323.269"
        },
        {
            "source": "L18-1440",
            "target": "2020.coling-main.249",
            "d": "M3594.51,-269.662C3617.58,-238.647 3656.66,-186.103 3682.3,-151.63"
        },
        {
            "source": "L18-1442",
            "target": "2020.lrec-1.621",
            "d": "M5500.86,-273.942C5476.95,-263.093 5448.5,-248.874 5424.6,-233.22 5393.81,-213.058 5393.71,-197.463 5361.6,-179.48 5330.05,-161.815 5292.71,-148.279 5259.51,-138.438"
        },
        {
            "source": "L18-1489",
            "target": "C18-1155",
            "d": "M7493.32,-296.09C7495.95,-296.09 7498.58,-296.09 7501.2,-296.09"
        },
        {
            "source": "L18-1489",
            "target": "2020.findings-emnlp.386",
            "d": "M7400.94,-269.283C7424.25,-238.174 7463.49,-185.807 7489.22,-151.476"
        },
        {
            "source": "L18-1489",
            "target": "2021.findings-acl.256",
            "d": "M7375.87,-269.228C7368.2,-227.659 7359.39,-145.047 7395.6,-89.7401 7404.73,-75.7854 7418.08,-64.7959 7432.55,-56.2049"
        },
        {
            "source": "L18-1548",
            "target": "W19-5426",
            "d": "M3126.53,-270.65C3116.55,-260.927 3104.94,-249.608 3094.32,-239.253"
        },
        {
            "source": "L18-1548",
            "target": "N19-1387",
            "d": "M3096.14,-279.022C3080.86,-275.181 3064.19,-271.529 3048.6,-269.22 2820.91,-235.516 2129.45,-216.119 1862.93,-209.726"
        },
        {
            "source": "L18-1548",
            "target": "2021.mtsummit-research.2",
            "d": "M3158.28,-269.045C3160.9,-258.052 3163.72,-245.076 3165.6,-233.22 3174.94,-174.343 3180.17,-105.184 3182.68,-64.0251"
        },
        {
            "source": "L18-1548",
            "target": "2021.emnlp-main.675",
            "d": "M3094.58,-279.906C3040.92,-265.14 2967.36,-243.481 2957.6,-233.22 2912.13,-185.427 2971.33,-130.677 2919.6,-89.7401 2875.3,-54.6803 1103.08,-33.0754 708.797,-28.6971"
        },
        {
            "source": "L18-1548",
            "target": "2021.calcs-1.5",
            "d": "M3165.53,-269.467C3171.35,-258.43 3178.04,-245.308 3183.6,-233.22 3212.32,-170.721 3190.03,-135.254 3241.6,-89.7401 3260.76,-72.8285 3314.67,-57.9717 3365.81,-47.0581"
        },
        {
            "source": "L18-1559",
            "target": "C18-1237",
            "d": "M12030.9,-296.09C12033.2,-296.09 12035.6,-296.09 12037.9,-296.09"
        },
        {
            "source": "K18-1012",
            "target": "2020.findings-emnlp.206",
            "d": "M4042.03,-277.72C3965.58,-260.349 3858.07,-235.838 3848.6,-233.22 3742.41,-203.875 3620.75,-162.698 3550.67,-138.227"
        },
        {
            "source": "K18-1012",
            "target": "2020.coling-main.249",
            "d": "M4066.54,-272.612C3990.57,-239.807 3849.6,-178.932 3768.78,-144.03"
        },
        {
            "source": "K18-1012",
            "target": "2020.aacl-main.90",
            "d": "M4095.39,-269.662C4066.51,-238.028 4017.19,-183.996 3985.79,-149.587"
        },
        {
            "source": "K18-1012",
            "target": "2021.ltedi-1.29",
            "d": "M4118.6,-268.982C4118.6,-221.088 4118.6,-119.096 4118.6,-64.3697"
        },
        {
            "source": "D18-1382",
            "target": "W19-0413",
            "d": "M5691.68,-277.172C5677.65,-274.375 5663.23,-271.62 5649.6,-269.22 5541.2,-250.145 5513.46,-249.405 5404.6,-233.22 5383.84,-230.134 5361.68,-226.817 5340.4,-223.622"
        },
        {
            "source": "D18-1382",
            "target": "N19-1034",
            "d": "M5714.16,-273.176C5677.45,-260.083 5630.88,-243.475 5593.71,-230.217"
        },
        {
            "source": "D18-1382",
            "target": "D19-1566",
            "d": "M5769.12,-269.24C5767.08,-261.102 5764.78,-251.962 5762.6,-243.255"
        },
        {
            "source": "D18-1382",
            "target": "2020.coling-main.393",
            "d": "M5691.29,-277.365C5677.38,-274.559 5663.1,-271.751 5649.6,-269.22 5602.63,-260.414 5468.43,-269.729 5437.6,-233.22 5422.19,-214.972 5428.63,-201.616 5437.6,-179.48 5442.03,-168.54 5449.43,-158.467 5457.55,-149.766"
        },
        {
            "source": "D18-1382",
            "target": "2020.acl-main.401",
            "d": "M5840.07,-273.601C5858.34,-264.193 5876.09,-251.102 5886.6,-233.22 5898.7,-212.627 5899.97,-199.268 5886.6,-179.48 5872.7,-158.919 5850.72,-145.19 5827.67,-136.023"
        },
        {
            "source": "D18-1382",
            "target": "2020.aacl-main.31",
            "d": "M5841.22,-273.658C5863.61,-263.959 5887.53,-250.674 5905.6,-233.22 5928.57,-211.028 5944.46,-178.129 5954.09,-153.095"
        },
        {
            "source": "C18-1042",
            "target": "K18-1012",
            "d": "M3995.62,-296.09C3998.28,-296.09 4000.95,-296.09 4003.61,-296.09"
        },
        {
            "source": "C18-1155",
            "target": "2020.findings-emnlp.386",
            "d": "M7594.93,-269.283C7578.64,-238.526 7551.34,-186.988 7533.15,-152.647"
        },
        {
            "source": "C18-1155",
            "target": "2021.findings-acl.256",
            "d": "M7621.41,-269.237C7639.82,-228.04 7667.99,-146.379 7633.6,-89.7401 7625.09,-75.7298 7612.27,-64.7464 7598.14,-56.1847"
        },
        {
            "source": "2018.gwc-1.47",
            "target": "2020.nuse-1.11",
            "d": "M12340.6,-268.903C12340.6,-238.405 12340.6,-187.854 12340.6,-153.662"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "N19-1034",
            "target": "2020.coling-main.393",
            "d": "M5520.17,-179.5C5517.17,-171.273 5513.8,-162.023 5510.59,-153.231"
        },
        {
            "source": "N19-1034",
            "target": "2020.acl-main.401",
            "d": "M5577.96,-184.446C5605.9,-172.371 5641.31,-157.064 5671.2,-144.146"
        },
        {
            "source": "N19-1091",
            "target": "2020.lrec-1.514",
            "d": "M12581.6,-179.025C12581.6,-171.059 12581.6,-162.168 12581.6,-153.679"
        },
        {
            "source": "D19-1566",
            "target": "2020.coling-main.393",
            "d": "M5689.33,-183.323C5653.03,-170.881 5607.54,-155.293 5569.99,-142.421"
        },
        {
            "source": "D19-1566",
            "target": "2020.acl-main.401",
            "d": "M5747.41,-179.5C5745.46,-171.362 5743.27,-162.222 5741.19,-153.515"
        },
        {
            "source": "D19-1566",
            "target": "2020.aacl-main.31",
            "d": "M5809.3,-182.298C5837.75,-170.523 5872.49,-156.144 5901.99,-143.936"
        },
        {
            "source": "2019.gwc-1.51",
            "target": "2020.lrec-1.378",
            "d": "M12899.2,-180.91C12886.8,-170.916 12872.3,-159.236 12859.2,-148.65"
        },
        {
            "source": "2019.gwc-1.51",
            "target": "2020.coling-main.119",
            "d": "M12960.2,-180.91C12972.7,-171 12987.2,-159.432 13000.3,-148.917"
        },
        {
            "source": "2020",
            "target": "2021",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        },
        {
            "source": "2020.sltu-1.49",
            "target": "2021.emnlp-main.675",
            "d": "M593.216,-90.2321C598.646,-81.4107 604.826,-71.3683 610.618,-61.9585"
        },
        {
            "source": "2020.lrec-1.378",
            "target": "2020.coling-main.119",
            "d": "M12918.5,-116.61C12921,-116.61 12923.5,-116.61 12925.9,-116.61"
        },
        {
            "source": "2020.iwslt-1.22",
            "target": "2021.eacl-main.299",
            "d": "M13254.6,-89.2852C13254.6,-81.3185 13254.6,-72.4275 13254.6,-63.9391"
        },
        {
            "source": "2020.findings-emnlp.206",
            "target": "2021.calcs-1.5",
            "d": "M3487.29,-89.7598C3485.62,-81.6214 3483.75,-72.4816 3481.96,-63.7749"
        },
        {
            "source": "2020.findings-emnlp.386",
            "target": "2021.findings-acl.256",
            "d": "M7514.6,-89.2852C7514.6,-81.3185 7514.6,-72.4275 7514.6,-63.9391"
        },
        {
            "source": "2020.acl-main.401",
            "target": "2020.aacl-main.31",
            "d": "M5838.18,-116.61C5840.63,-116.61 5843.08,-116.61 5845.53,-116.61"
        },
        {
            "source": "2020.acl-main.402",
            "target": "2021.naacl-main.456",
            "d": "M13453.6,-89.2852C13453.6,-81.3185 13453.6,-72.4275 13453.6,-63.9391"
        },
        {
            "source": "2020.aacl-main.86",
            "target": "2020.icon-main.23",
            "d": "M6272.46,-116.61C6274.94,-116.61 6277.41,-116.61 6279.88,-116.61"
        },
        {
            "source": "2021.mtsummit-research.2",
            "target": "2021.mtsummit-research.2",
            "d": "M3266.48,-43.259C3288.81,-42.4307 3305.84,-36.9677 3305.84,-26.8701 3305.84,-18.3502 3293.71,-13.1297 3276.51,-11.2088"
        }
    ],
    [
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "P06-2100",
            "name": "smriti2006Morphological",
            "x": "3700.6",
            "y": "-1287.03"
        },
        {
            "id": "P06-2100",
            "name": "60",
            "x": "3700.6",
            "y": "-1272.03"
        },
        {
            "id": "I08-1067",
            "name": "ananthakrishnan2008Simple",
            "x": "2231.6",
            "y": "-1197.29"
        },
        {
            "id": "I08-1067",
            "name": "61",
            "x": "2231.6",
            "y": "-1182.29"
        },
        {
            "id": "C10-2040",
            "name": "harshada2010Verbs",
            "x": "6984.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-2040",
            "name": "13",
            "x": "6984.6",
            "y": "-1002.81"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "sankaran-etal-2008-common",
            "name": "baskaran2008A",
            "x": "7388.6",
            "y": "-1197.29"
        },
        {
            "id": "sankaran-etal-2008-common",
            "name": "29",
            "x": "7388.6",
            "y": "-1182.29"
        },
        {
            "id": "I08-7013",
            "name": "sankaran2008Designing",
            "x": "7193.6",
            "y": "-1197.29"
        },
        {
            "id": "I08-7013",
            "name": "8",
            "x": "7193.6",
            "y": "-1182.29"
        },
        {
            "id": "P09-1090",
            "name": "ananthakrishnan2009Case",
            "x": "2161.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-1090",
            "name": "35",
            "x": "2161.6",
            "y": "-1092.55"
        },
        {
            "id": "W12-5906",
            "name": "anoop2012Partially",
            "x": "919.597",
            "y": "-838.331"
        },
        {
            "id": "W12-5906",
            "name": "3",
            "x": "919.597",
            "y": "-823.331"
        },
        {
            "id": "W14-5105",
            "name": "rajen2014Supertag",
            "x": "2088.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5105",
            "name": "1",
            "x": "2088.6",
            "y": "-643.851"
        },
        {
            "id": "W14-5148",
            "name": "sudha2014Merging",
            "x": "3653.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5148",
            "name": "2",
            "x": "3653.6",
            "y": "-643.851"
        },
        {
            "id": "W14-3308",
            "name": "piyush2014The",
            "x": "1062.6",
            "y": "-658.851"
        },
        {
            "id": "W14-3308",
            "name": "9",
            "x": "1062.6",
            "y": "-643.851"
        },
        {
            "id": "kunchukuttan-etal-2014-shata",
            "name": "anoop2014Shata-Anuvadak:",
            "x": "606.597",
            "y": "-658.851"
        },
        {
            "id": "kunchukuttan-etal-2014-shata",
            "name": "15",
            "x": "606.597",
            "y": "-643.851"
        },
        {
            "id": "W15-5950",
            "name": "pratik2015Investigating",
            "x": "3426.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5950",
            "name": "0",
            "x": "3426.6",
            "y": "-554.111"
        },
        {
            "id": "W16-6303",
            "name": "debajyoty2016Can",
            "x": "2128.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6303",
            "name": "3",
            "x": "2128.6",
            "y": "-464.371"
        },
        {
            "id": "W16-4622",
            "name": "sukanta2016{IITP}",
            "x": "2320.6",
            "y": "-479.371"
        },
        {
            "id": "W16-4622",
            "name": "3",
            "x": "2320.6",
            "y": "-464.371"
        },
        {
            "id": "L16-1485",
            "name": "sudha2016Synset",
            "x": "3887.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1485",
            "name": "1",
            "x": "3887.6",
            "y": "-464.371"
        },
        {
            "id": "I17-2048",
            "name": "anoop2017Utilizing",
            "x": "1056.6",
            "y": "-389.631"
        },
        {
            "id": "I17-2048",
            "name": "1",
            "x": "1056.6",
            "y": "-374.631"
        },
        {
            "id": "L18-1548",
            "name": "anoop2018The",
            "x": "3151.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1548",
            "name": "???",
            "x": "3151.6",
            "y": "-284.89"
        },
        {
            "id": "N19-1387",
            "name": "rudra2019Addressing",
            "x": "1756.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1387",
            "name": "6",
            "x": "1756.6",
            "y": "-195.15"
        },
        {
            "id": "C08-2007",
            "name": "debasri2008{H}indi",
            "x": "9459.6",
            "y": "-1197.29"
        },
        {
            "id": "C08-2007",
            "name": "19",
            "x": "9459.6",
            "y": "-1182.29"
        },
        {
            "id": "L16-1369",
            "name": "dhirendra2016Multiword",
            "x": "9459.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1369",
            "name": "0",
            "x": "9459.6",
            "y": "-464.371"
        },
        {
            "id": "C08-1068",
            "name": "abbas2008{H}indi",
            "x": "9772.6",
            "y": "-1197.29"
        },
        {
            "id": "C08-1068",
            "name": "30",
            "x": "9772.6",
            "y": "-1182.29"
        },
        {
            "id": "W09-3536",
            "name": "abbas2009A",
            "x": "9718.6",
            "y": "-1107.55"
        },
        {
            "id": "W09-3536",
            "name": "20",
            "x": "9718.6",
            "y": "-1092.55"
        },
        {
            "id": "C10-2091",
            "name": "abbas2010Finite-state",
            "x": "9584.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-2091",
            "name": "2",
            "x": "9584.6",
            "y": "-1002.81"
        },
        {
            "id": "2010.jeptalnrecital-court.7",
            "name": "muhammad2010Weak",
            "x": "9836.6",
            "y": "-1017.81"
        },
        {
            "id": "2010.jeptalnrecital-court.7",
            "name": "0",
            "x": "9836.6",
            "y": "-1002.81"
        },
        {
            "id": "W13-4706",
            "name": "abbas2013{U}rdu",
            "x": "9836.6",
            "y": "-748.591"
        },
        {
            "id": "W13-4706",
            "name": "3",
            "x": "9836.6",
            "y": "-733.591"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "W15-5914",
            "name": "sreelekha2015Solving",
            "x": "2139.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5914",
            "name": "3",
            "x": "2139.6",
            "y": "-554.111"
        },
        {
            "id": "D09-1048",
            "name": "mitesh2009Projecting",
            "x": "4075.6",
            "y": "-1107.55"
        },
        {
            "id": "D09-1048",
            "name": "14",
            "x": "4075.6",
            "y": "-1092.55"
        },
        {
            "id": "C10-1063",
            "name": "mitesh2010Value",
            "x": "4022.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-1063",
            "name": "7",
            "x": "4022.6",
            "y": "-1002.81"
        },
        {
            "id": "P11-1057",
            "name": "mitesh2011Together",
            "x": "4056.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1057",
            "name": "9",
            "x": "4056.6",
            "y": "-913.071"
        },
        {
            "id": "I11-1078",
            "name": "mitesh2011It",
            "x": "3884.6",
            "y": "-928.071"
        },
        {
            "id": "I11-1078",
            "name": "4",
            "x": "3884.6",
            "y": "-913.071"
        },
        {
            "id": "2016.gwc-1.57",
            "name": "meghna2016Mapping",
            "x": "4358.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.57",
            "name": "???",
            "x": "4358.6",
            "y": "-464.371"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "W10-4011",
            "name": "vishal2010More",
            "x": "1392.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-4011",
            "name": "0",
            "x": "1392.6",
            "y": "-1002.81"
        },
        {
            "id": "W10-3604",
            "name": "mugdha2010A",
            "x": "10270.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-3604",
            "name": "12",
            "x": "10270.6",
            "y": "-1002.81"
        },
        {
            "id": "W12-5020",
            "name": "swapnil2012Error",
            "x": "10072.6",
            "y": "-838.331"
        },
        {
            "id": "W12-5020",
            "name": "1",
            "x": "10072.6",
            "y": "-823.331"
        },
        {
            "id": "C12-2023",
            "name": "raj2012Morphological",
            "x": "10270.6",
            "y": "-838.331"
        },
        {
            "id": "C12-2023",
            "name": "2",
            "x": "10270.6",
            "y": "-823.331"
        },
        {
            "id": "W14-5103",
            "name": "raj2014Tackling",
            "x": "10396.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5103",
            "name": "0",
            "x": "10396.6",
            "y": "-643.851"
        },
        {
            "id": "W10-3607",
            "name": "pratikkumar2010Hybrid",
            "x": "10504.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-3607",
            "name": "20",
            "x": "10504.6",
            "y": "-1002.81"
        },
        {
            "id": "W11-3001",
            "name": "kartik2011Hybrid",
            "x": "10504.6",
            "y": "-928.071"
        },
        {
            "id": "W11-3001",
            "name": "24",
            "x": "10504.6",
            "y": "-913.071"
        },
        {
            "id": "P10-1137",
            "name": "manoj2010Multilingual",
            "x": "1162.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1137",
            "name": "8",
            "x": "1162.6",
            "y": "-1002.81"
        },
        {
            "id": "N15-1125",
            "name": "raj2015Leveraging",
            "x": "1137.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1125",
            "name": "10",
            "x": "1137.6",
            "y": "-554.111"
        },
        {
            "id": "P10-1155",
            "name": "mitesh2010All",
            "x": "2690.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1155",
            "name": "19",
            "x": "2690.6",
            "y": "-1002.81"
        },
        {
            "id": "P13-2096",
            "name": "sudha2013Neighbors",
            "x": "3411.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2096",
            "name": "5",
            "x": "3411.6",
            "y": "-733.591"
        },
        {
            "id": "W14-0124",
            "name": "brijesh2014Graph",
            "x": "2561.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0124",
            "name": "0",
            "x": "2561.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5908",
            "name": "sudha2015Using",
            "x": "3624.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5908",
            "name": "1",
            "x": "3624.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5945",
            "name": "diptesh2015Using",
            "x": "2451.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5945",
            "name": "1",
            "x": "2451.6",
            "y": "-554.111"
        },
        {
            "id": "L16-1349",
            "name": "diptesh2016That{'}ll",
            "x": "2520.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1349",
            "name": "1",
            "x": "2520.6",
            "y": "-464.371"
        },
        {
            "id": "2016.gwc-1.54",
            "name": "raksha2016High,",
            "x": "2709.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.54",
            "name": "???",
            "x": "2709.6",
            "y": "-464.371"
        },
        {
            "id": "N10-1065",
            "name": "mitesh2010Everybody",
            "x": "1892.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1065",
            "name": "17",
            "x": "1892.6",
            "y": "-1002.81"
        },
        {
            "id": "khapra-etal-2014-transliteration",
            "name": "mitesh2014When",
            "x": "1830.6",
            "y": "-658.851"
        },
        {
            "id": "khapra-etal-2014-transliteration",
            "name": "4",
            "x": "1830.6",
            "y": "-643.851"
        },
        {
            "id": "K16-1027",
            "name": "anoop2016Substring-based",
            "x": "1828.6",
            "y": "-479.371"
        },
        {
            "id": "K16-1027",
            "name": "0",
            "x": "1828.6",
            "y": "-464.371"
        },
        {
            "id": "Q18-1022",
            "name": "anoop2018Leveraging",
            "x": "1727.6",
            "y": "-299.89"
        },
        {
            "id": "Q18-1022",
            "name": "1",
            "x": "1727.6",
            "y": "-284.89"
        },
        {
            "id": "bhattacharyya-2010-indowordnet",
            "name": "pushpak2010{I}ndo{W}ord{N}et",
            "x": "4839.6",
            "y": "-1017.81"
        },
        {
            "id": "bhattacharyya-2010-indowordnet",
            "name": "???",
            "x": "4839.6",
            "y": "-1002.81"
        },
        {
            "id": "W12-5209",
            "name": "brijesh2012Domain",
            "x": "4330.6",
            "y": "-838.331"
        },
        {
            "id": "W12-5209",
            "name": "8",
            "x": "4330.6",
            "y": "-823.331"
        },
        {
            "id": "C12-2008",
            "name": "balamurali2012Cross-Lingual",
            "x": "4994.6",
            "y": "-838.331"
        },
        {
            "id": "C12-2008",
            "name": "38",
            "x": "4994.6",
            "y": "-823.331"
        },
        {
            "id": "L18-1728",
            "name": "diptesh2018{I}ndian",
            "x": "4334.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1728",
            "name": "0",
            "x": "4334.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.31",
            "name": "kevin2018Semi-automatic",
            "x": "4558.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.31",
            "name": "???",
            "x": "4558.6",
            "y": "-284.89"
        },
        {
            "id": "W19-7509",
            "name": "malhar2019Introduction",
            "x": "8057.6",
            "y": "-210.15"
        },
        {
            "id": "W19-7509",
            "name": "???",
            "x": "8057.6",
            "y": "-195.15"
        },
        {
            "id": "W15-5905",
            "name": "sachin2015Noun",
            "x": "7891.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5905",
            "name": "0",
            "x": "7891.6",
            "y": "-554.111"
        },
        {
            "id": "ar-etal-2012-cost",
            "name": "balamurali2012Cost",
            "x": "4525.6",
            "y": "-838.331"
        },
        {
            "id": "ar-etal-2012-cost",
            "name": "4",
            "x": "4525.6",
            "y": "-823.331"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "P11-4022",
            "name": "aditya2011{C}-Feel-It:",
            "x": "5851.6",
            "y": "-928.071"
        },
        {
            "id": "P11-4022",
            "name": "25",
            "x": "5851.6",
            "y": "-913.071"
        },
        {
            "id": "C12-1113",
            "name": "subhabrata2012Sentiment",
            "x": "5921.6",
            "y": "-838.331"
        },
        {
            "id": "C12-1113",
            "name": "51",
            "x": "5921.6",
            "y": "-823.331"
        },
        {
            "id": "S13-2082",
            "name": "karan2013{IITB}-Sentiment-Analysts:",
            "x": "5813.6",
            "y": "-748.591"
        },
        {
            "id": "S13-2082",
            "name": "13",
            "x": "5813.6",
            "y": "-733.591"
        },
        {
            "id": "N13-1088",
            "name": "salil2013More",
            "x": "5490.6",
            "y": "-748.591"
        },
        {
            "id": "N13-1088",
            "name": "12",
            "x": "5490.6",
            "y": "-733.591"
        },
        {
            "id": "D11-1100",
            "name": "balamurali2011Harnessing",
            "x": "5187.6",
            "y": "-928.071"
        },
        {
            "id": "D11-1100",
            "name": "26",
            "x": "5187.6",
            "y": "-913.071"
        },
        {
            "id": "P13-1041",
            "name": "kashyap2013The",
            "x": "4376.6",
            "y": "-748.591"
        },
        {
            "id": "P13-1041",
            "name": "20",
            "x": "4376.6",
            "y": "-733.591"
        },
        {
            "id": "W14-2619",
            "name": "nikhilkumar2014Dive",
            "x": "5761.6",
            "y": "-658.851"
        },
        {
            "id": "W14-2619",
            "name": "4",
            "x": "5761.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5920",
            "name": "raksha2015Domain",
            "x": "2938.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5920",
            "name": "2",
            "x": "2938.6",
            "y": "-554.111"
        },
        {
            "id": "W16-6315",
            "name": "raksha2016Meaning",
            "x": "5345.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6315",
            "name": "0",
            "x": "5345.6",
            "y": "-464.371"
        },
        {
            "id": "L16-1429",
            "name": "md2016Aspect",
            "x": "4895.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1429",
            "name": "15",
            "x": "4895.6",
            "y": "-464.371"
        },
        {
            "id": "K16-1016",
            "name": "abhijit2016Leveraging",
            "x": "5602.6",
            "y": "-479.371"
        },
        {
            "id": "K16-1016",
            "name": "9",
            "x": "5602.6",
            "y": "-464.371"
        },
        {
            "id": "P17-1035",
            "name": "abhijit2017Learning",
            "x": "5662.6",
            "y": "-389.631"
        },
        {
            "id": "P17-1035",
            "name": "14",
            "x": "5662.6",
            "y": "-374.631"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "W12-4906",
            "name": "abhijit2012A",
            "x": "6139.6",
            "y": "-838.331"
        },
        {
            "id": "W12-4906",
            "name": "9",
            "x": "6139.6",
            "y": "-823.331"
        },
        {
            "id": "P13-2062",
            "name": "abhijit2013Automatically",
            "x": "6139.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2062",
            "name": "14",
            "x": "6139.6",
            "y": "-733.591"
        },
        {
            "id": "kunchukuttan-etal-2012-experiences",
            "name": "anoop2012Experiences",
            "x": "3722.6",
            "y": "-838.331"
        },
        {
            "id": "kunchukuttan-etal-2012-experiences",
            "name": "9",
            "x": "3722.6",
            "y": "-823.331"
        },
        {
            "id": "P13-4030",
            "name": "anoop2013{T}rans{D}oop:",
            "x": "3722.6",
            "y": "-748.591"
        },
        {
            "id": "P13-4030",
            "name": "2",
            "x": "3722.6",
            "y": "-733.591"
        },
        {
            "id": "C12-3013",
            "name": "shilpa2012Automated",
            "x": "10616.6",
            "y": "-838.331"
        },
        {
            "id": "C12-3013",
            "name": "5",
            "x": "10616.6",
            "y": "-823.331"
        },
        {
            "id": "W14-5136",
            "name": "shilpa2014{A}uto{P}ar{S}e:",
            "x": "10616.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5136",
            "name": "1",
            "x": "10616.6",
            "y": "-643.851"
        },
        {
            "id": "C12-3030",
            "name": "salil2012Eating",
            "x": "4741.6",
            "y": "-838.331"
        },
        {
            "id": "C12-3030",
            "name": "1",
            "x": "4741.6",
            "y": "-823.331"
        },
        {
            "id": "C12-3033",
            "name": "diptesh2012Discrimination-Net",
            "x": "5386.6",
            "y": "-838.331"
        },
        {
            "id": "C12-3033",
            "name": "1",
            "x": "5386.6",
            "y": "-823.331"
        },
        {
            "id": "W14-0126",
            "name": "diptesh2014Do",
            "x": "5324.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0126",
            "name": "0",
            "x": "5324.6",
            "y": "-643.851"
        },
        {
            "id": "C16-1047",
            "name": "md2016A",
            "x": "5035.6",
            "y": "-479.371"
        },
        {
            "id": "C16-1047",
            "name": "20",
            "x": "5035.6",
            "y": "-464.371"
        },
        {
            "id": "C16-1287",
            "name": "prerana2016Borrow",
            "x": "4715.6",
            "y": "-479.371"
        },
        {
            "id": "C16-1287",
            "name": "5",
            "x": "4715.6",
            "y": "-464.371"
        },
        {
            "id": "N18-1053",
            "name": "md2018Solving",
            "x": "5332.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1053",
            "name": "4",
            "x": "5332.6",
            "y": "-284.89"
        },
        {
            "id": "W19-0413",
            "name": "md2019Language-Agnostic",
            "x": "5232.6",
            "y": "-210.15"
        },
        {
            "id": "W19-0413",
            "name": "0",
            "x": "5232.6",
            "y": "-195.15"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "W13-3611",
            "name": "anoop2013{IITB}",
            "x": "10900.6",
            "y": "-748.591"
        },
        {
            "id": "W13-3611",
            "name": "5",
            "x": "10900.6",
            "y": "-733.591"
        },
        {
            "id": "W14-1708",
            "name": "anoop2014Tuning",
            "x": "10845.6",
            "y": "-658.851"
        },
        {
            "id": "W14-1708",
            "name": "6",
            "x": "10845.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5902",
            "name": "anoop2015Addressing",
            "x": "10901.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5902",
            "name": "0",
            "x": "10901.6",
            "y": "-554.111"
        },
        {
            "id": "P14-2007",
            "name": "aditya2014Measuring",
            "x": "6044.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2007",
            "name": "13",
            "x": "6044.6",
            "y": "-643.851"
        },
        {
            "id": "P18-1219",
            "name": "sandeep2018Eyes",
            "x": "6206.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1219",
            "name": "2",
            "x": "6206.6",
            "y": "-284.89"
        },
        {
            "id": "2020.aacl-main.86",
            "name": "sandeep2020Happy",
            "x": "6182.6",
            "y": "-120.41"
        },
        {
            "id": "2020.aacl-main.86",
            "name": "0",
            "x": "6182.6",
            "y": "-105.41"
        },
        {
            "id": "P13-2149",
            "name": "ankit2013Detecting",
            "x": "6351.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2149",
            "name": "10",
            "x": "6351.6",
            "y": "-733.591"
        },
        {
            "id": "P15-2124",
            "name": "aditya2015Harnessing",
            "x": "6573.6",
            "y": "-569.111"
        },
        {
            "id": "P15-2124",
            "name": "101",
            "x": "6573.6",
            "y": "-554.111"
        },
        {
            "id": "D15-1300",
            "name": "raksha2015Adjective",
            "x": "3174.6",
            "y": "-569.111"
        },
        {
            "id": "D15-1300",
            "name": "8",
            "x": "3174.6",
            "y": "-554.111"
        },
        {
            "id": "W14-2623",
            "name": "abhijit2014A",
            "x": "5474.6",
            "y": "-658.851"
        },
        {
            "id": "W14-2623",
            "name": "8",
            "x": "5474.6",
            "y": "-643.851"
        },
        {
            "id": "W16-1904",
            "name": "joe2016Leveraging",
            "x": "5879.6",
            "y": "-479.371"
        },
        {
            "id": "W16-1904",
            "name": "4",
            "x": "5879.6",
            "y": "-464.371"
        },
        {
            "id": "I13-2006",
            "name": "aditya2013Making",
            "x": "2489.6",
            "y": "-748.591"
        },
        {
            "id": "I13-2006",
            "name": "1",
            "x": "2489.6",
            "y": "-733.591"
        },
        {
            "id": "I13-1076",
            "name": "raksha2013Detecting",
            "x": "3053.6",
            "y": "-748.591"
        },
        {
            "id": "I13-1076",
            "name": "13",
            "x": "3053.6",
            "y": "-733.591"
        },
        {
            "id": "D17-1058",
            "name": "raksha2017Sentiment",
            "x": "3057.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1058",
            "name": "4",
            "x": "3057.6",
            "y": "-374.631"
        },
        {
            "id": "P18-1089",
            "name": "raksha2018Identifying",
            "x": "2941.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1089",
            "name": "5",
            "x": "2941.6",
            "y": "-284.89"
        },
        {
            "id": "2020.lrec-1.613",
            "name": "akash2020Recommendation",
            "x": "2788.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.613",
            "name": "0",
            "x": "2788.6",
            "y": "-105.41"
        },
        {
            "id": "I13-1131",
            "name": "arjun2013Structure",
            "x": "3986.6",
            "y": "-748.591"
        },
        {
            "id": "I13-1131",
            "name": "2",
            "x": "3986.6",
            "y": "-733.591"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "W14-5115",
            "name": "hanumant2014Introduction",
            "x": "8424.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5115",
            "name": "1",
            "x": "8424.6",
            "y": "-643.851"
        },
        {
            "id": "2016.gwc-1.46",
            "name": "hanumant2016Sam{\\=a}sa-Kart{\\=a}:",
            "x": "8245.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.46",
            "name": "???",
            "x": "8245.6",
            "y": "-464.371"
        },
        {
            "id": "W14-5126",
            "name": "diptesh2014{P}a{CM}an",
            "x": "859.597",
            "y": "-658.851"
        },
        {
            "id": "W14-5126",
            "name": "0",
            "x": "859.597",
            "y": "-643.851"
        },
        {
            "id": "W15-5916",
            "name": "diptesh2015{T}rans{C}hat:",
            "x": "913.597",
            "y": "-569.111"
        },
        {
            "id": "W15-5916",
            "name": "1",
            "x": "913.597",
            "y": "-554.111"
        },
        {
            "id": "W14-0130",
            "name": "pushpak2014Facilitating",
            "x": "8726.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0130",
            "name": "9",
            "x": "8726.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5910",
            "name": "hanumant2015{I}ndo{W}ord{N}et",
            "x": "9141.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5910",
            "name": "0",
            "x": "9141.6",
            "y": "-554.111"
        },
        {
            "id": "2016.gwc-1.22",
            "name": "diptesh2016Sophisticated",
            "x": "8726.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.22",
            "name": "???",
            "x": "8726.6",
            "y": "-464.371"
        },
        {
            "id": "2021.naacl-main.322",
            "name": "kumar2021How",
            "x": "9322.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.naacl-main.322",
            "name": "???",
            "x": "9322.6",
            "y": "-15.6701"
        },
        {
            "id": "W14-0145",
            "name": "sudha2014Semi-Automatic",
            "x": "8056.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0145",
            "name": "1",
            "x": "8056.6",
            "y": "-643.851"
        },
        {
            "id": "W14-0147",
            "name": "devendra2014{I}ndo{W}ordnet",
            "x": "11123.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0147",
            "name": "3",
            "x": "11123.6",
            "y": "-643.851"
        },
        {
            "id": "W19-7512",
            "name": "diptesh2019An",
            "x": "11123.6",
            "y": "-210.15"
        },
        {
            "id": "W19-7512",
            "name": "???",
            "x": "11123.6",
            "y": "-195.15"
        },
        {
            "id": "N15-3017",
            "name": "anoop2015Brahmi-Net:",
            "x": "632.597",
            "y": "-569.111"
        },
        {
            "id": "N15-3017",
            "name": "11",
            "x": "632.597",
            "y": "-554.111"
        },
        {
            "id": "L16-1098",
            "name": "sreelekha2016Lexical",
            "x": "263.597",
            "y": "-479.371"
        },
        {
            "id": "L16-1098",
            "name": "4",
            "x": "263.597",
            "y": "-464.371"
        },
        {
            "id": "W18-1207",
            "name": "tamali2018Meaningless",
            "x": "516.597",
            "y": "-299.89"
        },
        {
            "id": "W18-1207",
            "name": "2",
            "x": "516.597",
            "y": "-284.89"
        },
        {
            "id": "L18-1413",
            "name": "sreelekha2018Morphology",
            "x": "191.597",
            "y": "-299.89"
        },
        {
            "id": "L18-1413",
            "name": "0",
            "x": "191.597",
            "y": "-284.89"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "W15-5944",
            "name": "rohit2015Augmenting",
            "x": "1610.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5944",
            "name": "2",
            "x": "1610.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5946",
            "name": "deepak2015Triangulation",
            "x": "1386.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5946",
            "name": "0",
            "x": "1386.6",
            "y": "-554.111"
        },
        {
            "id": "W15-3912",
            "name": "anoop2015Data",
            "x": "1836.6",
            "y": "-569.111"
        },
        {
            "id": "W15-3912",
            "name": "7",
            "x": "1836.6",
            "y": "-554.111"
        },
        {
            "id": "W15-2905",
            "name": "anupam2015Your",
            "x": "7237.6",
            "y": "-569.111"
        },
        {
            "id": "W15-2905",
            "name": "41",
            "x": "7237.6",
            "y": "-554.111"
        },
        {
            "id": "W16-5001",
            "name": "aditya2016{`}Who",
            "x": "7439.6",
            "y": "-479.371"
        },
        {
            "id": "W16-5001",
            "name": "4",
            "x": "7439.6",
            "y": "-464.371"
        },
        {
            "id": "K16-1015",
            "name": "aditya2016Harnessing",
            "x": "7237.6",
            "y": "-479.371"
        },
        {
            "id": "K16-1015",
            "name": "18",
            "x": "7237.6",
            "y": "-464.371"
        },
        {
            "id": "D16-1104",
            "name": "aditya2016Are",
            "x": "6621.6",
            "y": "-479.371"
        },
        {
            "id": "D16-1104",
            "name": "27",
            "x": "6621.6",
            "y": "-464.371"
        },
        {
            "id": "L18-1424",
            "name": "aditya2018Sarcasm",
            "x": "7111.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1424",
            "name": "1",
            "x": "7111.6",
            "y": "-284.89"
        },
        {
            "id": "W16-2111",
            "name": "aditya2016How",
            "x": "6422.6",
            "y": "-479.371"
        },
        {
            "id": "W16-2111",
            "name": "???",
            "x": "6422.6",
            "y": "-464.371"
        },
        {
            "id": "U16-1013",
            "name": "aditya2016How",
            "x": "6781.6",
            "y": "-479.371"
        },
        {
            "id": "U16-1013",
            "name": "???",
            "x": "6781.6",
            "y": "-464.371"
        },
        {
            "id": "P16-1104",
            "name": "abhijit2016Harnessing",
            "x": "6080.6",
            "y": "-479.371"
        },
        {
            "id": "P16-1104",
            "name": "13",
            "x": "6080.6",
            "y": "-464.371"
        },
        {
            "id": "W17-7518",
            "name": "diptesh2017Is",
            "x": "6413.6",
            "y": "-389.631"
        },
        {
            "id": "W17-7518",
            "name": "0",
            "x": "6413.6",
            "y": "-374.631"
        },
        {
            "id": "W19-1309",
            "name": "abhijeet2019{``}When",
            "x": "6894.6",
            "y": "-210.15"
        },
        {
            "id": "W19-1309",
            "name": "1",
            "x": "6894.6",
            "y": "-195.15"
        },
        {
            "id": "2020.aacl-main.31",
            "name": "dushyant2020All-in-One:",
            "x": "5965.6",
            "y": "-120.41"
        },
        {
            "id": "2020.aacl-main.31",
            "name": "???",
            "x": "5965.6",
            "y": "-105.41"
        },
        {
            "id": "D16-1196",
            "name": "anoop2016Orthographic",
            "x": "858.597",
            "y": "-479.371"
        },
        {
            "id": "D16-1196",
            "name": "8",
            "x": "858.597",
            "y": "-464.371"
        },
        {
            "id": "W17-4102",
            "name": "anoop2017Learning",
            "x": "560.597",
            "y": "-389.631"
        },
        {
            "id": "W17-4102",
            "name": "7",
            "x": "560.597",
            "y": "-374.631"
        },
        {
            "id": "P18-2064",
            "name": "rudra2018Judicious",
            "x": "858.597",
            "y": "-299.89"
        },
        {
            "id": "P18-2064",
            "name": "3",
            "x": "858.597",
            "y": "-284.89"
        },
        {
            "id": "2021.emnlp-main.675",
            "name": "tejas2021Role",
            "x": "631.597",
            "y": "-30.6701"
        },
        {
            "id": "2021.emnlp-main.675",
            "name": "???",
            "x": "631.597",
            "y": "-15.6701"
        },
        {
            "id": "N15-1132",
            "name": "sudha2015Unsupervised",
            "x": "8862.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1132",
            "name": "20",
            "x": "8862.6",
            "y": "-554.111"
        },
        {
            "id": "L18-1049",
            "name": "sabyasachi2018Sentence",
            "x": "8867.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1049",
            "name": "1",
            "x": "8867.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.34",
            "name": "kevin2018An",
            "x": "9058.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.34",
            "name": "???",
            "x": "9058.6",
            "y": "-284.89"
        },
        {
            "id": "W18-3705",
            "name": "sandeep2018Thank",
            "x": "3329.6",
            "y": "-299.89"
        },
        {
            "id": "W18-3705",
            "name": "0",
            "x": "3329.6",
            "y": "-284.89"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "W16-6325",
            "name": "shweta2016A",
            "x": "11391.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6325",
            "name": "7",
            "x": "11391.6",
            "y": "-464.371"
        },
        {
            "id": "W16-6331",
            "name": "deepak2016Opinion",
            "x": "4115.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6331",
            "name": "2",
            "x": "4115.6",
            "y": "-464.371"
        },
        {
            "id": "K18-1012",
            "name": "deepak2018Uncovering",
            "x": "4118.6",
            "y": "-299.89"
        },
        {
            "id": "K18-1012",
            "name": "4",
            "x": "4118.6",
            "y": "-284.89"
        },
        {
            "id": "2020.findings-emnlp.206",
            "name": "deepak2020A",
            "x": "3492.6",
            "y": "-120.41"
        },
        {
            "id": "2020.findings-emnlp.206",
            "name": "???",
            "x": "3492.6",
            "y": "-105.41"
        },
        {
            "id": "W16-6336",
            "name": "girishkumar2016On",
            "x": "7669.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6336",
            "name": "0",
            "x": "7669.6",
            "y": "-464.371"
        },
        {
            "id": "L18-1489",
            "name": "girishkumar2018Towards",
            "x": "7381.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1489",
            "name": "0",
            "x": "7381.6",
            "y": "-284.89"
        },
        {
            "id": "W16-4811",
            "name": "anoop2016Faster",
            "x": "1064.6",
            "y": "-479.371"
        },
        {
            "id": "W16-4811",
            "name": "1",
            "x": "1064.6",
            "y": "-464.371"
        },
        {
            "id": "W16-4206",
            "name": "shweta2016Deep",
            "x": "11229.6",
            "y": "-479.371"
        },
        {
            "id": "W16-4206",
            "name": "14",
            "x": "11229.6",
            "y": "-464.371"
        },
        {
            "id": "W16-0415",
            "name": "aditya2016Political",
            "x": "6997.6",
            "y": "-479.371"
        },
        {
            "id": "W16-0415",
            "name": "9",
            "x": "6997.6",
            "y": "-464.371"
        },
        {
            "id": "N16-4006",
            "name": "pushpak2016Statistical",
            "x": "554.597",
            "y": "-479.371"
        },
        {
            "id": "N16-4006",
            "name": "4",
            "x": "554.597",
            "y": "-464.371"
        },
        {
            "id": "W17-2338",
            "name": "kevin2017Adapting",
            "x": "6586.6",
            "y": "-389.631"
        },
        {
            "id": "W17-2338",
            "name": "5",
            "x": "6586.6",
            "y": "-374.631"
        },
        {
            "id": "I17-2006",
            "name": "kevin2017Towards",
            "x": "6779.6",
            "y": "-389.631"
        },
        {
            "id": "I17-2006",
            "name": "1",
            "x": "6779.6",
            "y": "-374.631"
        },
        {
            "id": "C18-1155",
            "name": "girishkumar2018Treat",
            "x": "7608.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1155",
            "name": "0",
            "x": "7608.6",
            "y": "-284.89"
        },
        {
            "id": "W17-5229",
            "name": "md2017{IITP}",
            "x": "5221.6",
            "y": "-389.631"
        },
        {
            "id": "W17-5229",
            "name": "3",
            "x": "5221.6",
            "y": "-374.631"
        },
        {
            "id": "D17-1057",
            "name": "md2017A",
            "x": "4750.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1057",
            "name": "11",
            "x": "4750.6",
            "y": "-374.631"
        },
        {
            "id": "2020.lrec-1.621",
            "name": "mamta2020Multi-domain",
            "x": "5171.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.621",
            "name": "???",
            "x": "5171.6",
            "y": "-105.41"
        },
        {
            "id": "2020.lrec-1.201",
            "name": "soumitra2020{CEASE},",
            "x": "4601.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.201",
            "name": "???",
            "x": "4601.6",
            "y": "-105.41"
        },
        {
            "id": "2020.icon-main.62",
            "name": "soumitra2020Annotated",
            "x": "4833.6",
            "y": "-120.41"
        },
        {
            "id": "2020.icon-main.62",
            "name": "???",
            "x": "4833.6",
            "y": "-105.41"
        },
        {
            "id": "2016.gwc-1.23",
            "name": "diptesh2016A",
            "x": "8491.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.23",
            "name": "???",
            "x": "8491.6",
            "y": "-464.371"
        },
        {
            "id": "2018.gwc-1.37",
            "name": "hanumant2018{H}indi",
            "x": "8222.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.37",
            "name": "???",
            "x": "8222.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.49",
            "name": "diptesh2018Synthesizing",
            "x": "8448.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.49",
            "name": "???",
            "x": "8448.6",
            "y": "-284.89"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "W18-0522",
            "name": "nikhil2018The",
            "x": "6413.6",
            "y": "-299.89"
        },
        {
            "id": "W18-0522",
            "name": "1",
            "x": "6413.6",
            "y": "-284.89"
        },
        {
            "id": "W17-7531",
            "name": "hanumant2017{H}indi",
            "x": "7912.6",
            "y": "-389.631"
        },
        {
            "id": "W17-7531",
            "name": "0",
            "x": "7912.6",
            "y": "-374.631"
        },
        {
            "id": "E17-1109",
            "name": "shweta2017Entity",
            "x": "4937.6",
            "y": "-389.631"
        },
        {
            "id": "E17-1109",
            "name": "7",
            "x": "4937.6",
            "y": "-374.631"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "Y18-3012",
            "name": "sukanta2018{IITP}-{MT}",
            "x": "11267.6",
            "y": "-299.89"
        },
        {
            "id": "Y18-3012",
            "name": "1",
            "x": "11267.6",
            "y": "-284.89"
        },
        {
            "id": "2021.wat-1.29",
            "name": "ramakrishna2021{IITP}-{MT}",
            "x": "11267.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.wat-1.29",
            "name": "???",
            "x": "11267.6",
            "y": "-15.6701"
        },
        {
            "id": "2020.bea-1.8",
            "name": "sandeep2020Can",
            "x": "3329.6",
            "y": "-120.41"
        },
        {
            "id": "2020.bea-1.8",
            "name": "???",
            "x": "3329.6",
            "y": "-105.41"
        },
        {
            "id": "P18-2011",
            "name": "sangameshwar2018Identification",
            "x": "11603.6",
            "y": "-299.89"
        },
        {
            "id": "P18-2011",
            "name": "0",
            "x": "11603.6",
            "y": "-284.89"
        },
        {
            "id": "W19-2404",
            "name": "girish2019Extraction",
            "x": "11464.6",
            "y": "-210.15"
        },
        {
            "id": "W19-2404",
            "name": "???",
            "x": "11464.6",
            "y": "-195.15"
        },
        {
            "id": "N19-2017",
            "name": "girish2019Extraction",
            "x": "11667.6",
            "y": "-210.15"
        },
        {
            "id": "N19-2017",
            "name": "0",
            "x": "11667.6",
            "y": "-195.15"
        },
        {
            "id": "2020.icon-main.23",
            "name": "sandeep2020Cognitively",
            "x": "6397.6",
            "y": "-120.41"
        },
        {
            "id": "2020.icon-main.23",
            "name": "???",
            "x": "6397.6",
            "y": "-105.41"
        },
        {
            "id": "L18-1278",
            "name": "deepak2018A",
            "x": "3761.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1278",
            "name": "5",
            "x": "3761.6",
            "y": "-284.89"
        },
        {
            "id": "2020.aacl-main.90",
            "name": "deepak2020A",
            "x": "3956.6",
            "y": "-120.41"
        },
        {
            "id": "2020.aacl-main.90",
            "name": "???",
            "x": "3956.6",
            "y": "-105.41"
        },
        {
            "id": "L18-1440",
            "name": "deepak2018{MMQA}:",
            "x": "3575.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1440",
            "name": "6",
            "x": "3575.6",
            "y": "-284.89"
        },
        {
            "id": "2020.coling-main.249",
            "name": "deepak2020Reinforced",
            "x": "3707.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.249",
            "name": "0",
            "x": "3707.6",
            "y": "-105.41"
        },
        {
            "id": "L18-1442",
            "name": "shweta2018Medical",
            "x": "5551.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1442",
            "name": "5",
            "x": "5551.6",
            "y": "-284.89"
        },
        {
            "id": "2020.findings-emnlp.386",
            "name": "girishkumar2020Looking",
            "x": "7514.6",
            "y": "-120.41"
        },
        {
            "id": "2020.findings-emnlp.386",
            "name": "???",
            "x": "7514.6",
            "y": "-105.41"
        },
        {
            "id": "2021.findings-acl.256",
            "name": "girishkumar2021{F}rame{N}et-assisted",
            "x": "7514.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.findings-acl.256",
            "name": "???",
            "x": "7514.6",
            "y": "-15.6701"
        },
        {
            "id": "W19-5426",
            "name": "jyotsana2019Utilizing",
            "x": "3061.6",
            "y": "-210.15"
        },
        {
            "id": "W19-5426",
            "name": "0",
            "x": "3061.6",
            "y": "-195.15"
        },
        {
            "id": "2021.mtsummit-research.2",
            "name": "kamal2021Investigating",
            "x": "3184.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.mtsummit-research.2",
            "name": "???",
            "x": "3184.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.calcs-1.5",
            "name": "ramakrishna2021{IITP}-{MT}",
            "x": "3474.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.calcs-1.5",
            "name": "???",
            "x": "3474.6",
            "y": "-15.6701"
        },
        {
            "id": "L18-1559",
            "name": "tirthankar2018{TAP}-{DLND}",
            "x": "11895.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1559",
            "name": "2",
            "x": "11895.6",
            "y": "-284.89"
        },
        {
            "id": "C18-1237",
            "name": "tirthankar2018Novelty",
            "x": "12146.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1237",
            "name": "1",
            "x": "12146.6",
            "y": "-284.89"
        },
        {
            "id": "2021.ltedi-1.29",
            "name": "pankaj2021{CFILT}",
            "x": "4118.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.ltedi-1.29",
            "name": "???",
            "x": "4118.6",
            "y": "-15.6701"
        },
        {
            "id": "D18-1382",
            "name": "deepanway2018Contextual",
            "x": "5775.6",
            "y": "-299.89"
        },
        {
            "id": "D18-1382",
            "name": "7",
            "x": "5775.6",
            "y": "-284.89"
        },
        {
            "id": "N19-1034",
            "name": "md2019Multi-task",
            "x": "5529.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1034",
            "name": "4",
            "x": "5529.6",
            "y": "-195.15"
        },
        {
            "id": "D19-1566",
            "name": "dushyant2019Context-aware",
            "x": "5753.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1566",
            "name": "2",
            "x": "5753.6",
            "y": "-195.15"
        },
        {
            "id": "2020.coling-main.393",
            "name": "mauajama2020{MEISD}:",
            "x": "5497.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.393",
            "name": "???",
            "x": "5497.6",
            "y": "-105.41"
        },
        {
            "id": "2020.acl-main.401",
            "name": "dushyant2020Sentiment",
            "x": "5732.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.401",
            "name": "???",
            "x": "5732.6",
            "y": "-105.41"
        },
        {
            "id": "C18-1042",
            "name": "deepak2018Can",
            "x": "3920.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1042",
            "name": "3",
            "x": "3920.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.47",
            "name": "ritesh2018pyiwn:",
            "x": "12340.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.47",
            "name": "???",
            "x": "12340.6",
            "y": "-284.89"
        },
        {
            "id": "2020.nuse-1.11",
            "name": "swapnil2020Extracting",
            "x": "12340.6",
            "y": "-120.41"
        },
        {
            "id": "2020.nuse-1.11",
            "name": "???",
            "x": "12340.6",
            "y": "-105.41"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "N19-1091",
            "name": "hitesh2019Courteously",
            "x": "12581.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1091",
            "name": "0",
            "x": "12581.6",
            "y": "-195.15"
        },
        {
            "id": "2020.lrec-1.514",
            "name": "mauajama2020Incorporating",
            "x": "12581.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.514",
            "name": "???",
            "x": "12581.6",
            "y": "-105.41"
        },
        {
            "id": "2019.gwc-1.51",
            "name": "diptesh2019Utilizing",
            "x": "12929.6",
            "y": "-210.15"
        },
        {
            "id": "2019.gwc-1.51",
            "name": "???",
            "x": "12929.6",
            "y": "-195.15"
        },
        {
            "id": "2020.lrec-1.378",
            "name": "diptesh2020Challenge",
            "x": "12820.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.378",
            "name": "???",
            "x": "12820.6",
            "y": "-105.41"
        },
        {
            "id": "2020.coling-main.119",
            "name": "diptesh2020Harnessing",
            "x": "13039.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.119",
            "name": "???",
            "x": "13039.6",
            "y": "-105.41"
        },
        {
            "id": "2021",
            "name": "2021",
            "x": "28.5975",
            "y": "-23.1701"
        },
        {
            "id": "2020.sltu-1.49",
            "name": "saurav2020{``}A",
            "x": "577.597",
            "y": "-120.41"
        },
        {
            "id": "2020.sltu-1.49",
            "name": "???",
            "x": "577.597",
            "y": "-105.41"
        },
        {
            "id": "2020.iwslt-1.22",
            "name": "nikhil2020Generating",
            "x": "13254.6",
            "y": "-120.41"
        },
        {
            "id": "2020.iwslt-1.22",
            "name": "???",
            "x": "13254.6",
            "y": "-105.41"
        },
        {
            "id": "2021.eacl-main.299",
            "name": "nikhil2021Disfluency",
            "x": "13254.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.eacl-main.299",
            "name": "???",
            "x": "13254.6",
            "y": "-15.6701"
        },
        {
            "id": "2020.acl-main.402",
            "name": "tulika2020Towards",
            "x": "13453.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.402",
            "name": "???",
            "x": "13453.6",
            "y": "-105.41"
        },
        {
            "id": "2021.naacl-main.456",
            "name": "tulika2021Towards",
            "x": "13453.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.naacl-main.456",
            "name": "???",
            "x": "13453.6",
            "y": "-15.6701"
        }
    ],
    [
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "2356.89,-1198.47 2346.7,-1201.37 2356.47,-1205.46 2356.89,-1198.47",
        "6888.83,-1026.32 6898.51,-1022.02 6888.26,-1019.34 6888.83,-1026.32",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "7305.71,-1196.99 7315.71,-1193.49 7305.71,-1189.99 7305.71,-1196.99",
        "2191.05,-1136.05 2182.07,-1130.42 2185.57,-1140.42 2191.05,-1136.05",
        "923.375,-871.346 919.546,-861.467 916.379,-871.578 923.375,-871.346",
        "2069.92,-692.026 2071.96,-681.631 2063.88,-688.482 2069.92,-692.026",
        "3612.58,-688.726 3618.2,-679.74 3608.2,-683.265 3612.58,-688.726",
        "1057.01,-692.547 1055.89,-682.011 1050.2,-690.945 1057.01,-692.547",
        "641.079,-686.696 631.835,-681.518 635.833,-691.33 641.079,-686.696",
        "3357.98,-594.084 3365.85,-586.986 3355.27,-587.629 3357.98,-594.084",
        "2206.51,-492.526 2195.92,-492.32 2204.07,-499.087 2206.51,-492.526",
        "2330.49,-511.914 2325.39,-502.63 2323.59,-513.07 2330.49,-511.914",
        "3896.23,-512.312 3891.55,-502.808 3889.28,-513.157 3896.23,-512.312",
        "1150.85,-391.047 1140.57,-393.598 1150.2,-398.016 1150.85,-391.047",
        "3178.61,-328.404 3170.03,-322.19 3172.87,-332.399 3178.61,-328.404",
        "1661.95,-224.193 1671.28,-219.182 1660.86,-217.278 1661.95,-224.193",
        "9463.1,-512.733 9459.6,-502.733 9456.1,-512.733 9463.1,-512.733",
        "9742.24,-1136.5 9734.02,-1129.81 9736.28,-1140.16 9742.24,-1136.5",
        "9600.69,-1048.81 9593.67,-1040.88 9594.21,-1051.46 9600.69,-1048.81",
        "9827.18,-1051.44 9827.28,-1040.85 9820.6,-1049.07 9827.18,-1051.44",
        "9878.88,-773.56 9868.97,-769.797 9874.37,-778.915 9878.88,-773.56",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "9631.39,-1041.47 9621.11,-1038.92 9627.56,-1047.32 9631.39,-1041.47",
        "9797.73,-1048.27 9803.67,-1039.5 9793.55,-1042.66 9797.73,-1048.27",
        "9819.77,-781.955 9821.59,-771.519 9813.66,-778.541 9819.77,-781.955",
        "2169.08,-597.876 2160.49,-591.678 2163.34,-601.882 2169.08,-597.876",
        "2198.22,-496.321 2187.69,-495.175 2195.2,-502.639 2198.22,-496.321",
        "4046.08,-1047.12 4037.92,-1040.37 4040.09,-1050.74 4046.08,-1047.12",
        "4094.05,-955.063 4084.79,-949.916 4088.82,-959.715 4094.05,-955.063",
        "3896.35,-960.054 3890.27,-951.377 3889.61,-961.951 3896.35,-960.054",
        "4348.9,-513.218 4349.07,-502.624 4342.33,-510.8 4348.9,-513.218",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "10109.6,-864.008 10099.8,-859.94 10104.9,-869.221 10109.6,-864.008",
        "10274.1,-871.537 10270.6,-861.537 10267.1,-871.537 10274.1,-871.537",
        "10402.3,-691.81 10398.4,-681.953 10395.3,-692.081 10402.3,-691.81",
        "10508.1,-961.275 10504.6,-951.275 10501.1,-961.275 10508.1,-961.275",
        "1308.3,-1017.51 1318.3,-1014.01 1308.3,-1010.51 1308.3,-1017.51",
        "1149.06,-601.381 1143.59,-592.312 1142.21,-602.817 1149.06,-601.381",
        "3379.68,-779.901 3384.83,-770.64 3375.03,-774.669 3379.68,-779.901",
        "2576.39,-690.344 2569.98,-681.911 2569.73,-692.503 2576.39,-690.344",
        "3560.2,-590.139 3568.86,-584.037 3558.29,-583.406 3560.2,-590.139",
        "2431.34,-602.038 2433.74,-591.717 2425.43,-598.292 2431.34,-602.038",
        "2570.51,-502.642 2560.27,-499.917 2566.58,-508.432 2570.51,-502.642",
        "2713.1,-512.733 2709.6,-502.733 2706.1,-512.733 2713.1,-512.733",
        "1840.25,-691.166 1835.09,-681.913 1833.35,-692.364 1840.25,-691.166",
        "1888.55,-502.763 1878.29,-500.15 1884.68,-508.596 1888.55,-502.763",
        "1809.21,-314.666 1798.62,-314.904 1807.05,-321.323 1809.21,-314.666",
        "4397.64,-855.2 4387.04,-855.212 4395.33,-861.81 4397.64,-855.2",
        "4968.34,-870.916 4972.28,-861.081 4963.07,-866.311 4968.34,-870.916",
        "4315.72,-332.886 4318,-322.54 4309.76,-329.205 4315.72,-332.886",
        "4573.07,-331.436 4566.77,-322.912 4566.38,-333.499 4573.07,-331.436",
        "7968.77,-229.874 7977.46,-223.812 7966.89,-223.132 7968.77,-229.874",
        "7895.1,-602.473 7891.6,-592.473 7888.1,-602.473 7895.1,-602.473",
        "4046.18,-961.829 4046.53,-951.24 4039.66,-959.3 4046.18,-961.829",
        "3929.03,-949.375 3918.72,-946.963 3925.28,-955.282 3929.03,-949.375",
        "4460.69,-862.143 4468.96,-855.515 4458.37,-855.542 4460.69,-862.143",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "5897.65,-871.156 5901.15,-861.156 5892.18,-866.79 5897.65,-871.156",
        "5804.88,-782.39 5804.18,-771.818 5798.14,-780.519 5804.88,-782.39",
        "3950.58,-496.8 3940.01,-496.082 3947.83,-503.235 3950.58,-496.8",
        "3456.14,-772.788 3445.89,-770.096 3452.23,-778.59 3456.14,-772.788",
        "5520.16,-776.871 5511.53,-770.727 5514.45,-780.913 5520.16,-776.871",
        "3561.2,-590.836 3569.51,-584.272 3558.92,-584.217 3561.2,-590.836",
        "4601.31,-852.018 4590.78,-853.155 4599.72,-858.835 4601.31,-852.018",
        "5057.07,-860.087 5046.52,-859.134 5054.17,-866.459 5057.07,-860.087",
        "5813.25,-852.008 5822.76,-847.346 5812.42,-845.057 5813.25,-852.008",
        "5778.2,-780.439 5783.8,-771.445 5773.81,-774.984 5778.2,-780.439",
        "4445.95,-762.043 4435.38,-762.767 4444.1,-768.792 4445.95,-762.043",
        "5694.38,-683.33 5701.89,-675.852 5691.36,-677.017 5694.38,-683.33",
        "2932.1,-602.902 2931,-592.364 2925.29,-601.285 2932.1,-602.902",
        "5309.93,-510.266 5315.01,-500.966 5305.24,-505.071 5309.93,-510.266",
        "4932.35,-505.008 4922.71,-500.618 4927.51,-510.063 4932.35,-505.008",
        "5613.69,-511.686 5608.38,-502.518 5606.81,-512.997 5613.69,-511.686",
        "5572.05,-401.309 5581.55,-396.607 5571.19,-394.362 5572.05,-401.309",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "6143.1,-781.795 6139.6,-771.795 6136.1,-781.795 6143.1,-781.795",
        "3726.1,-781.795 3722.6,-771.795 3719.1,-781.795 3726.1,-781.795",
        "3615.59,-689.58 3620.39,-680.135 3610.75,-684.526 3615.59,-689.58",
        "3933.51,-502.489 3923.29,-499.678 3929.52,-508.245 3933.51,-502.489",
        "10620.1,-692.057 10616.6,-682.057 10613.1,-692.057 10620.1,-692.057",
        "4383.69,-323.483 4373.48,-320.63 4379.68,-329.223 4383.69,-323.483",
        "4562.73,-333.125 4558.91,-323.244 4555.73,-333.351 4562.73,-333.125",
        "5340.18,-690.055 5333.58,-681.769 5333.58,-692.364 5340.18,-690.055",
        "4458.91,-753.499 4448.53,-755.609 4457.96,-760.434 4458.91,-753.499",
        "4923.29,-507.948 4914.83,-501.565 4917.46,-511.828 4923.29,-507.948",
        "3957.72,-492.955 3947.16,-493.832 3955.96,-499.731 3957.72,-492.955",
        "5034.96,-512.767 5032.63,-502.433 5028.01,-511.968 5034.96,-512.767",
        "4759.7,-503.866 4749.66,-500.484 4755.4,-509.389 4759.7,-503.866",
        "5356.4,-329.785 5348.71,-322.497 5350.18,-332.99 5356.4,-329.785",
        "5327.03,-224.889 5316.46,-225.664 5325.21,-231.648 5327.03,-224.889",
        "5855.05,-774.961 5845.08,-771.372 5850.64,-780.394 5855.05,-774.961",
        "5841.85,-672.284 5831.3,-673.23 5840.14,-679.071 5841.85,-672.284",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "10870,-688.33 10861.7,-681.716 10864,-692.048 10870,-688.33",
        "10928.6,-598.506 10920.5,-591.743 10922.6,-602.116 10928.6,-598.506",
        "3215.27,-313.976 3204.68,-314.319 3213.17,-320.655 3215.27,-313.976",
        "6081.22,-685.108 6071.5,-680.894 6076.47,-690.25 6081.22,-685.108",
        "5634.79,-506.646 5625.33,-501.873 5629.75,-511.503 5634.79,-506.646",
        "6222.09,-331.336 6215.7,-322.882 6215.42,-333.473 6222.09,-331.336",
        "6140.5,-149.921 6146.82,-141.417 6136.58,-144.124 6140.5,-149.921",
        "3590.67,-682.588 3598.88,-675.889 3588.29,-676.008 3590.67,-682.588",
        "3577.85,-596.541 3584.62,-588.394 3574.24,-590.539 3577.85,-596.541",
        "3852,-509.58 3857.8,-500.714 3847.74,-504.028 3852,-509.58",
        "6120.39,-674.071 6109.81,-674.688 6118.46,-680.802 6120.39,-674.071",
        "6537.22,-599.871 6542.85,-590.892 6532.85,-594.403 6537.22,-599.871",
        "5690.61,-490.404 5680.11,-491.768 5689.17,-497.254 5690.61,-490.404",
        "3743.47,-663.432 3733.12,-665.702 3742.63,-670.381 3743.47,-663.432",
        "5668.26,-672.068 5677.74,-667.351 5667.39,-665.123 5668.26,-672.068",
        "3259.68,-579.881 3249.21,-581.527 3258.42,-586.767 3259.68,-579.881",
        "3958.33,-492.693 3947.77,-493.514 3956.54,-499.459 3958.33,-492.693",
        "5520.33,-498.913 5529.31,-493.283 5518.78,-492.087 5520.33,-498.913",
        "5484.57,-691.23 5479.33,-682.02 5477.68,-692.485 5484.57,-691.23",
        "5375.28,-678.883 5364.81,-677.304 5372.01,-685.072 5375.28,-678.883",
        "5952.79,-673.296 5962.16,-668.348 5951.75,-666.374 5952.79,-673.296",
        "5838.87,-508.731 5845.13,-500.19 5834.91,-502.957 5838.87,-508.731",
        "5600.85,-513.034 5599.02,-502.6 5593.94,-511.901 5600.85,-513.034",
        "2459.68,-601.645 2454.84,-592.221 2452.74,-602.608 2459.68,-601.645",
        "2967.24,-597.679 2958.44,-591.78 2961.64,-601.88 2967.24,-597.679",
        "3154.36,-602.214 3157.1,-591.98 3148.57,-598.271 3154.36,-602.214",
        "5513.06,-495.765 5522.36,-490.694 5511.93,-488.857 5513.06,-495.765",
        "2721.71,-511.545 2715.91,-502.679 2714.92,-513.227 2721.71,-511.545",
        "5566.12,-395.613 5575.83,-391.393 5565.61,-388.631 5566.12,-395.613",
        "3060.69,-423.214 3057.3,-413.175 3053.69,-423.135 3060.69,-423.214",
        "2916.45,-332.618 2919.87,-322.591 2910.94,-328.296 2916.45,-332.618",
        "2800.69,-152.564 2795.06,-143.584 2793.86,-154.111 2800.69,-152.564",
        "3903.92,-510.553 3897.16,-502.392 3897.36,-512.985 3903.92,-510.553",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "1840.08,-223.723 1829.5,-224.17 1838.05,-230.422 1840.08,-223.723",
        "8281.05,-506.795 8271.49,-502.24 8276.12,-511.766 8281.05,-506.795",
        "8169.86,-210.415 8159.65,-213.254 8169.4,-217.4 8169.86,-210.415",
        "965.514,-592.82 955.121,-590.762 961.962,-598.852 965.514,-592.82",
        "5575.73,-511.677 5579.63,-501.829 5570.44,-507.092 5575.73,-511.677",
        "10882.8,-602.248 10885.2,-591.936 10876.9,-598.482 10882.8,-602.248",
        "676.842,-678.567 666.248,-678.671 674.595,-685.196 676.842,-678.567",
        "9037.49,-591.899 9046.55,-586.406 9036.04,-585.05 9037.49,-591.899",
        "8730.1,-512.577 8726.6,-502.577 8723.1,-512.577 8730.1,-512.577",
        "3212.67,-314.693 3202.1,-315.299 3210.74,-321.421 3212.67,-314.693",
        "9326.1,-64.0324 9322.6,-54.0324 9319.1,-64.0325 9326.1,-64.0324",
        "8061.02,-243.377 8057.54,-233.369 8054.02,-243.361 8061.02,-243.377",
        "11127.1,-243.513 11123.6,-233.513 11120.1,-243.513 11127.1,-243.513",
        "5928.55,-502.49 5918.32,-499.749 5924.61,-508.274 5928.55,-502.49",
        "5661.86,-499.827 5651.31,-498.866 5658.96,-506.197 5661.86,-499.827",
        "5695.33,-416.849 5685.88,-412.061 5690.28,-421.698 5695.33,-416.849",
        "6213.63,-332.819 6209.42,-323.095 6206.65,-333.319 6213.63,-332.819",
        "736.215,-658.551 746.215,-655.051 736.215,-651.551 736.215,-658.551",
        "1018.31,-686.116 1025.54,-678.368 1015.06,-679.918 1018.31,-686.116",
        "832.533,-593.119 841.186,-587.006 830.609,-586.389 832.533,-593.119",
        "625.417,-602.862 624.902,-592.28 618.705,-600.873 625.417,-602.862",
        "1069.58,-590.137 1078.39,-584.252 1067.83,-583.358 1069.58,-590.137",
        "289.787,-508.077 281.001,-502.156 284.177,-512.264 289.787,-508.077",
        "486.125,-331.826 490.286,-322.083 480.957,-327.105 486.125,-331.826",
        "176.599,-333.494 177.697,-322.956 170.267,-330.509 176.599,-333.494",
        "1655.81,-220.05 1665.44,-215.634 1655.16,-213.081 1655.81,-220.05",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "2944.69,-333.344 2941.3,-323.305 2937.69,-333.265 2944.69,-333.344",
        "1126.28,-405.48 1115.69,-405.762 1124.15,-412.146 1126.28,-405.48",
        "1712.97,-333.356 1714,-322.811 1706.62,-330.414 1712.97,-333.356",
        "7383.01,-504.966 7390.79,-497.772 7380.22,-498.545 7383.01,-504.966",
        "7241.1,-512.575 7237.6,-502.575 7234.1,-512.575 7241.1,-512.575",
        "6681.04,-494.264 6670.46,-494.692 6679,-500.959 6681.04,-494.264",
        "7192.99,-311.019 7182.46,-312.192 7191.43,-317.841 7192.99,-311.019",
        "7145.46,-568.811 7155.46,-565.311 7145.46,-561.811 7145.46,-568.811",
        "7370.3,-500.634 7379.16,-494.829 7368.61,-493.84 7370.3,-500.634",
        "6470.98,-500.636 6460.58,-498.639 6467.46,-506.689 6470.98,-500.636",
        "6726.95,-503.423 6734.8,-496.311 6724.23,-496.973 6726.95,-503.423",
        "6171.33,-489.163 6160.87,-490.857 6170.1,-496.055 6171.33,-489.163",
        "7158.02,-499.716 7167.05,-494.168 7156.53,-492.876 7158.02,-499.716",
        "5696.21,-487.492 5685.81,-489.523 5695.21,-494.42 5696.21,-487.492",
        "6605.99,-512.387 6607.72,-501.934 6599.85,-509.027 6605.99,-512.387",
        "6463.1,-410.141 6452.77,-407.794 6459.39,-416.071 6463.1,-410.141",
        "5760.71,-385.942 5750.57,-388.998 5760.41,-392.935 5760.71,-385.942",
        "7123.75,-332.087 7118.31,-323.001 7116.9,-333.501 7123.75,-332.087",
        "6906.56,-242.558 6901.09,-233.486 6899.71,-243.99 6906.56,-242.558",
        "6039.51,-138.585 6028.92,-138.601 6037.21,-145.196 6039.51,-138.585",
        "1781.03,-592.776 1789.13,-585.952 1778.54,-586.232 1781.03,-592.776",
        "1705.13,-488.688 1714.87,-484.512 1704.65,-481.704 1705.13,-488.688",
        "793.75,-505.501 801.805,-498.619 791.216,-498.976 793.75,-505.501",
        "490.474,-412.63 497.956,-405.128 487.429,-406.327 490.474,-412.63",
        "1670.42,-326.736 1678.16,-319.501 1667.6,-320.33 1670.42,-326.736",
        "835.494,-332.716 838.841,-322.664 829.956,-328.435 835.494,-332.716",
        "656.005,-60.1374 648.039,-53.1521 649.91,-63.5804 656.005,-60.1374",
        "1266.15,-568.811 1276.15,-565.311 1266.15,-561.811 1266.15,-568.811",
        "1119.16,-409.645 1108.78,-407.524 1115.58,-415.655 1119.16,-409.645",
        "3962.18,-490.56 3951.69,-492.01 3960.8,-497.421 3962.18,-490.56",
        "8870.42,-333.369 8867.11,-323.305 8863.42,-333.238 8870.42,-333.369",
        "9037.34,-331.991 9040.43,-321.857 9031.69,-327.851 9037.34,-331.991",
        "3082.87,-418.804 3074.44,-412.381 3077.02,-422.656 3082.87,-418.804",
        "3312.79,-333.041 3314.78,-322.635 3306.74,-329.529 3312.79,-333.041",
        "3026.86,-312.504 3016.33,-313.622 3025.26,-319.318 3026.86,-312.504",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "4121.49,-333.154 4118.16,-323.097 4114.49,-333.036 4121.49,-333.154",
        "3476.56,-153.355 3478.01,-142.859 3470.33,-150.161 3476.56,-153.355",
        "7474.2,-313.801 7463.62,-314.353 7472.24,-320.519 7474.2,-313.801",
        "654.216,-393.603 643.872,-395.895 653.388,-400.554 654.216,-393.603",
        "1063.36,-422.475 1058.97,-412.835 1056.39,-423.111 1063.36,-422.475",
        "11316.3,-479.071 11326.3,-475.571 11316.3,-472.071 11316.3,-479.071",
        "6158.24,-327.32 6165.36,-319.474 6154.9,-321.168 6158.24,-327.32",
        "7388.72,-506.247 7396.36,-498.914 7385.81,-499.878 7388.72,-506.247",
        "5747.02,-398.976 5736.54,-400.553 5745.72,-405.854 5747.02,-398.976",
        "1016.52,-506.407 1024.01,-498.904 1013.48,-500.105 1016.52,-506.407",
        "740.404,-479.071 750.404,-475.571 740.404,-472.071 740.404,-479.071",
        "561.629,-423.05 558.819,-412.835 554.645,-422.572 561.629,-423.05",
        "972.623,-405.061 981.871,-399.891 971.418,-398.165 972.623,-405.061",
        "561.538,-43.7184 570.539,-38.1308 560.018,-36.8854 561.538,-43.7184",
        "209.082,-330.879 202.08,-322.929 202.596,-333.511 209.082,-330.879",
        "4975.13,-479.071 4985.13,-475.571 4975.13,-472.071 4975.13,-479.071",
        "5208.51,-242.869 5212.2,-232.936 5203.12,-238.404 5208.51,-242.869",
        "7342.94,-479.071 7352.94,-475.571 7342.94,-472.071 7342.94,-479.071",
        "5642.5,-422.424 5645.25,-412.194 5636.72,-418.473 5642.5,-422.424",
        "1750.29,-329.889 1742.3,-322.929 1744.21,-333.351 1750.29,-329.889",
        "6697.81,-479.071 6707.81,-475.571 6697.81,-472.071 6697.81,-479.071",
        "6603.91,-420.791 6596.96,-412.799 6597.41,-423.384 6603.91,-420.791",
        "6731.54,-417.521 6738.58,-409.606 6728.14,-411.401 6731.54,-417.521",
        "7065.02,-328.354 7072.01,-320.392 7061.58,-322.257 7065.02,-328.354",
        "7530.26,-320.522 7539.21,-314.843 7528.67,-313.705 7530.26,-320.522",
        "4673.56,-304.358 4663.1,-306.046 4672.33,-311.249 4673.56,-304.358",
        "974.396,-479.071 984.396,-475.571 974.396,-472.071 974.396,-479.071",
        "633.547,-404.661 622.966,-405.194 631.57,-411.376 633.547,-404.661",
        "1000.23,-415.639 1007.95,-408.387 997.391,-409.239 1000.23,-415.639",
        "862.098,-333.097 858.597,-323.097 855.098,-333.097 862.098,-333.097",
        "5170.51,-414.798 5178.06,-407.366 5167.53,-408.467 5170.51,-414.798",
        "4799.4,-403.775 4788.81,-403.465 4796.89,-410.312 4799.4,-403.775",
        "5347.85,-331.68 5341.92,-322.905 5341.09,-333.468 5347.85,-331.68",
        "5234.19,-243.246 5230.73,-233.233 5227.19,-243.221 5234.19,-243.246",
        "5131.48,-151.532 5136.66,-142.289 5126.85,-146.285 5131.48,-151.532",
        "4740.03,-422.779 4740.48,-412.194 4733.52,-420.185 4740.03,-422.779",
        "5267.38,-319.623 5275.94,-313.376 5265.35,-312.923 5267.38,-319.623",
        "4628.02,-149.676 4619.63,-143.205 4622.15,-153.495 4628.02,-149.676",
        "4808.88,-152.995 4812.34,-142.982 4803.39,-148.652 4808.88,-152.995",
        "8269.32,-323.724 8259.05,-321.139 8265.47,-329.567 8269.32,-323.724",
        "8460.61,-331.823 8454.86,-322.929 8453.81,-333.472 8460.61,-331.823",
        "8230.72,-332.564 8225.97,-323.097 8223.78,-333.463 8230.72,-332.564",
        "8075.26,-240.951 8068.07,-233.17 8068.84,-243.737 8075.26,-240.951",
        "4342.92,-332.537 4338.11,-323.097 4335.99,-333.475 4342.92,-332.537",
        "4525.03,-331.557 4530.19,-322.302 4520.38,-326.321 4525.03,-331.557",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "6417.1,-333.094 6413.6,-323.094 6410.1,-333.094 6417.1,-333.094",
        "8001.29,-366.661 7991.06,-369.421 8000.78,-373.643 8001.29,-366.661",
        "8033.24,-242.69 8036.86,-232.732 8027.82,-238.261 8033.24,-242.69",
        "958.598,-389.331 968.598,-385.831 958.598,-382.331 958.598,-389.331",
        "537.08,-330.126 529.468,-322.756 530.822,-333.264 537.08,-330.126",
        "5140.13,-389.331 5150.13,-385.831 5140.13,-382.331 5140.13,-389.331",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "11271.1,-64.0848 11267.6,-54.0848 11264.1,-64.0848 11271.1,-64.0848",
        "3333.1,-153.616 3329.6,-143.616 3326.1,-153.616 3333.1,-153.616",
        "11512.6,-233.49 11502.2,-231.112 11508.8,-239.41 11512.6,-233.49",
        "11646,-242.855 11649.1,-232.714 11640.4,-238.73 11646,-242.855",
        "2657.58,-124.377 2667.47,-120.591 2657.38,-117.38 2657.58,-124.377",
        "2819.91,-148.568 2810.73,-143.28 2814.6,-153.139 2819.91,-148.568",
        "6365.71,-152.022 6370.65,-142.651 6360.94,-146.894 6365.71,-152.022",
        "6190.92,-153.057 6186.11,-143.616 6183.99,-153.995 6190.92,-153.057",
        "4058.67,-326.441 4066.74,-319.577 4056.15,-319.91 4058.67,-326.441",
        "3925.37,-150.769 3930.4,-141.449 3920.65,-145.592 3925.37,-150.769",
        "4058.67,-326.441 4066.74,-319.577 4056.15,-319.91 4058.67,-326.441",
        "3685.35,-153.392 3688.51,-143.28 3679.73,-149.215 3685.35,-153.392",
        "5260.39,-135.049 5249.81,-135.632 5258.44,-141.773 5260.39,-135.049",
        "7501.34,-299.591 7511.34,-296.09 7501.34,-292.591 7501.34,-299.591",
        "7492.25,-153.262 7495.45,-143.161 7486.65,-149.064 7492.25,-153.262",
        "7434.62,-59.0591 7441.69,-51.1606 7431.24,-52.9315 7434.62,-59.0591",
        "3096.76,-236.744 3087.16,-232.269 3091.88,-241.756 3096.76,-236.744",
        "1862.94,-206.226 1852.86,-209.486 1862.77,-213.224 1862.94,-206.226",
        "3186.18,-64.1274 3183.28,-53.9385 3179.19,-63.7149 3186.18,-64.1274",
        "708.674,-25.1956 698.636,-28.5847 708.597,-32.1952 708.674,-25.1956",
        "3366.58,-50.4723 3375.65,-44.9976 3365.15,-43.6208 3366.58,-50.4723",
        "12038.1,-299.591 12048.1,-296.09 12038.1,-292.591 12038.1,-299.591",
        "3551.72,-134.889 3541.13,-134.888 3549.41,-141.496 3551.72,-134.889",
        "3769.8,-140.657 3759.23,-139.906 3767.02,-147.083 3769.8,-140.657",
        "3988.31,-147.161 3978.98,-142.135 3983.14,-151.881 3988.31,-147.161",
        "4122.1,-64.0848 4118.6,-54.0848 4115.1,-64.0848 4122.1,-64.0848",
        "5340.78,-220.14 5330.37,-222.115 5339.74,-227.062 5340.78,-220.14",
        "5594.58,-226.81 5583.98,-226.747 5592.22,-233.403 5594.58,-226.81",
        "5765.94,-242.168 5760.11,-233.319 5759.15,-243.87 5765.94,-242.168",
        "5460.22,-152.038 5464.81,-142.485 5455.27,-147.095 5460.22,-152.038",
        "5828.7,-132.67 5818.1,-132.488 5826.27,-139.236 5828.7,-132.67",
        "5957.42,-154.178 5957.59,-143.585 5950.85,-151.757 5957.42,-154.178",
        "4003.88,-299.591 4013.88,-296.09 4003.88,-292.591 4003.88,-299.591",
        "7536.06,-150.647 7528.28,-143.448 7529.87,-153.924 7536.06,-150.647",
        "7599.63,-53.0103 7589.2,-51.1613 7596.2,-59.1124 7599.63,-53.0103",
        "12344.1,-153.616 12340.6,-143.616 12337.1,-153.616 12344.1,-153.616",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "5513.78,-151.774 5507.07,-143.579 5507.21,-154.173 5513.78,-151.774",
        "5672.9,-147.224 5680.69,-140.043 5670.13,-140.798 5672.9,-147.224",
        "12585.1,-153.614 12581.6,-143.614 12578.1,-153.614 12585.1,-153.614",
        "5570.72,-138.971 5560.12,-139.04 5568.45,-145.593 5570.72,-138.971",
        "5744.54,-152.49 5738.81,-143.579 5737.74,-154.119 5744.54,-152.49",
        "5903.47,-147.112 5911.37,-140.054 5900.79,-140.644 5903.47,-147.112",
        "12861.2,-145.768 12851.2,-142.224 12856.8,-151.221 12861.2,-145.768",
        "13002.7,-151.501 13008.4,-142.529 12998.4,-146.029 13002.7,-151.501",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701",
        "613.753,-63.5419 616.014,-53.1911 607.791,-59.8729 613.753,-63.5419",
        "12926,-120.11 12936,-116.61 12926,-113.11 12926,-120.11",
        "13258.1,-63.874 13254.6,-53.874 13251.1,-63.8741 13258.1,-63.874",
        "3485.36,-62.9314 3479.92,-53.8389 3478.51,-64.3383 3485.36,-62.9314",
        "7518.1,-63.874 7514.6,-53.874 7511.1,-63.8741 7518.1,-63.874",
        "5845.6,-120.11 5855.6,-116.61 5845.6,-113.11 5845.6,-120.11",
        "13457.1,-63.874 13453.6,-53.874 13450.1,-63.8741 13457.1,-63.874",
        "6280.06,-120.11 6290.06,-116.61 6280.06,-113.11 6280.06,-120.11",
        "3276.7,-7.71366 3266.48,-10.4811 3276.2,-14.6953 3276.7,-7.71366"
    ]
]